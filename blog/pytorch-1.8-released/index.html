<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:

  Support for doing python to python functional transformations via torch.fx;
  Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and
  Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes here.


" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials" />
<meta property="og:description" content="We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:

  Support for doing python to python functional transformations via torch.fx;
  Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and
  Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes here.


" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials" />
<meta name="twitter:description" content="We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:

  Support for doing python to python functional transformations via torch.fx;
  Added or stabilized APIs to support FFTs (torch.fft), Linear Algebra functions (torch.linalg), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and
  Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes here.


" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2024</span>
            <p>September 18-19 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">March 04, 2021</p>
            <h1>
                <a class="blog-title">PyTorch 1.8 Release, including Compiler and Distributed Training updates, and New Mobile Tutorials</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Team PyTorch
                      
                    </p>
                    <p>We are excited to announce the availability of PyTorch 1.8. This release is composed of more than 3,000 commits since 1.7. It includes major updates and new features for compilation, code optimization, frontend APIs for scientific computing, and AMD ROCm support through binaries that are available via pytorch.org. It also provides improved features for large-scale training for pipeline and model parallelism, and gradient compression. A few of the highlights include:</p>
<ol>
  <li>Support for doing python to python functional transformations via <code class="language-plaintext highlighter-rouge">torch.fx</code>;</li>
  <li>Added or stabilized APIs to support FFTs (<code class="language-plaintext highlighter-rouge">torch.fft</code>), Linear Algebra functions (<code class="language-plaintext highlighter-rouge">torch.linalg</code>), added support for autograd for complex tensors and updates to improve performance for calculating hessians and jacobians; and</li>
  <li>Significant updates and improvements to distributed training including: Improved NCCL reliability; Pipeline parallelism support; RPC profiling; and support for communication hooks adding gradient compression.
See the full release notes <a href="https://github.com/pytorch/pytorch/releases">here</a>.</li>
</ol>

<p>Along with 1.8, we are also releasing major updates to PyTorch libraries including <a href="https://github.com/pytorch/csprng">TorchCSPRNG</a>, <a href="https://github.com/pytorch/vision">TorchVision</a>, <a href="https://github.com/pytorch/text">TorchText</a> and <a href="https://github.com/pytorch/audio">TorchAudio</a>. For more on the library releases, see the post <a href="http://pytorch.org/blog/pytorch-1.8-new-library-releases">here</a>. As previously noted, features in PyTorch releases are classified as Stable, Beta and Prototype. You can learn more about the definitions in the post <a href="https://pytorch.org/blog/pytorch-feature-classification-changes/">here</a>.</p>

<h1 id="new-and-updated-apis">New and Updated APIs</h1>
<p>The PyTorch 1.8 release brings a host of new and updated API surfaces ranging from additional APIs for NumPy compatibility, also support for ways to improve and scale your code for performance at both inference and training time. Here is a brief summary of the major features coming in this release:</p>

<h3 id="stable-torchfft-support-for-high-performance-numpy-style-ffts">[Stable] <code class="language-plaintext highlighter-rouge">Torch.fft</code> support for high performance NumPy style FFTs</h3>
<p>As part of PyTorch’s goal to support scientific computing, we have invested in improving our FFT support and with PyTorch 1.8, we are releasing the <code class="language-plaintext highlighter-rouge">torch.fft</code> module. This module implements the same functions as NumPy’s <code class="language-plaintext highlighter-rouge">np.fft</code> module, but with support for hardware acceleration and autograd.</p>
<ul>
  <li>See this <a href="https://pytorch.org/blog/the-torch.fft-module-accelerated-fast-fourier-transforms-with-autograd-in-pyTorch/">blog post</a> for more details</li>
  <li><a href="https://pytorch.org/docs/1.8.0/fft.html">Documentation</a></li>
</ul>

<h3 id="beta-support-for-numpy-style-linear-algebra-functions-via-torchlinalg">[Beta] Support for NumPy style linear algebra functions via <code class="language-plaintext highlighter-rouge">torch.linalg</code></h3>
<p>The <code class="language-plaintext highlighter-rouge">torch.linalg</code> module, modeled after NumPy’s <a href="https://numpy.org/doc/stable/reference/routines.linalg.html?highlight=linalg#module-numpy.linalg">np.linalg</a> module, brings NumPy-style support for common linear algebra operations including Cholesky decompositions, determinants, eigenvalues and many others.</p>
<ul>
  <li><a href="https://pytorch.org/docs/1.8.0/linalg.html">Documentation</a></li>
</ul>

<h2 id="beta-python-code-transformations-with-fx">[Beta] Python code Transformations with FX</h2>
<p>FX allows you to write transformations of the form <code class="language-plaintext highlighter-rouge">transform(input_module : nn.Module)</code> -&gt; <code class="language-plaintext highlighter-rouge">nn.Module</code>, where you can feed in a <code class="language-plaintext highlighter-rouge">Module</code> instance and get a transformed <code class="language-plaintext highlighter-rouge">Module</code> instance out of it.</p>

<p>This kind of functionality is applicable in many scenarios. For example, the FX-based Graph Mode Quantization product is releasing as a prototype contemporaneously with FX. Graph Mode Quantization automates the process of quantizing a neural net and does so by leveraging FX’s program capture, analysis and transformation facilities. We are also developing many other transformation products with FX and we are excited to share this powerful toolkit with the community.</p>

<p>Because FX transforms consume and produce nn.Module instances, they can be used within many existing PyTorch workflows. This includes workflows that, for example, train in Python then deploy via TorchScript.</p>

<p>You can read more about FX in the official <a href="https://pytorch.org/docs/master/fx.html">documentation</a>. You can also find several examples of program transformations implemented using <code class="language-plaintext highlighter-rouge">torch.fx</code> <a href="https://github.com/pytorch/examples/tree/master/fx">here</a>. We are constantly improving FX and invite you to share any feedback you have about the toolkit on the <a href="https://discuss.pytorch.org/">forums</a> or <a href="https://github.com/pytorch/pytorch/issues">issue tracker</a>.</p>

<p>We’d like to acknowledge <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a> tracing, <a href="https://mxnet.apache.org/versions/1.7.0/">Apache MXNet</a> hybridize, and more recently <a href="https://github.com/google/jax">JAX</a> as influences for program acquisition via tracing. We’d also like to acknowledge <a href="https://caffe2.ai/">Caffe2</a>, <a href="https://github.com/google/jax">JAX</a>, and <a href="https://www.tensorflow.org/">TensorFlow</a> as inspiration for the value of simple, directed dataflow graph program representations and transformations over those representations.</p>

<h1 id="distributed-training">Distributed Training</h1>
<p>The PyTorch 1.8 release added a number of new features as well as improvements to reliability and usability. Concretely, support for: <a href="https://pytorch.org/docs/stable/distributed.html?highlight=init_process_group#torch.distributed.init_process_group">Stable level async error/timeout handling</a> was added to improve NCCL reliability; and stable support for <a href="https://pytorch.org/docs/stable/rpc.html">RPC based profiling</a>. Additionally, we have added support for pipeline parallelism as well as gradient compression through the use of communication hooks in DDP. Details are below:</p>

<h3 id="beta-pipeline-parallelism">[Beta] Pipeline Parallelism</h3>
<p>As machine learning models continue to grow in size, traditional Distributed DataParallel (DDP) training no longer scales as these models don’t fit on a single GPU device. The new pipeline parallelism feature provides an easy to use PyTorch API to leverage pipeline parallelism as part of your training loop.</p>
<ul>
  <li><a href="https://github.com/pytorch/pytorch/issues/44827">RFC</a></li>
  <li><a href="https://pytorch.org/docs/1.8.0/pipeline.html?highlight=pipeline#">Documentation</a></li>
</ul>

<h3 id="beta-ddp-communication-hook">[Beta] DDP Communication Hook</h3>
<p>The DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in DistributedDataParallel. A few built-in communication hooks are provided including PowerSGD, and users can easily apply any of these hooks to optimize communication. Additionally, the communication hook interface can also support user-defined communication strategies for more advanced use cases.</p>
<ul>
  <li><a href="https://github.com/pytorch/pytorch/issues/39272">RFC</a></li>
  <li><a href="https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html?highlight=powersgd">Documentation</a></li>
</ul>

<h3 id="additional-prototype-features-for-distributed-training">Additional Prototype Features for Distributed Training</h3>
<p>In addition to the major stable and beta distributed training features in this release, we also have a number of prototype features available in our nightlies to try out and provide feedback. We have linked in the draft docs below for reference:</p>
<ul>
  <li><strong>(Prototype) ZeroRedundancyOptimizer</strong> - Based on and in partnership with the Microsoft DeepSpeed team, this feature helps reduce per-process memory footprint by sharding optimizer states across all participating processes in the <code class="language-plaintext highlighter-rouge">ProcessGroup</code> gang. Refer to this <a href="https://pytorch.org/docs/master/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer">documentation</a> for more details.</li>
  <li><strong>(Prototype) Process Group NCCL Send/Recv</strong> - The NCCL send/recv API was introduced in v2.7 and this feature adds support for it in NCCL process groups. This feature will provide an option for users to implement collective operations at Python layer instead of C++ layer. Refer to this <a href="https://pytorch.org/docs/master/distributed.html#distributed-communication-package-torch-distributed">documentation</a> and <a href="https://github.com/pytorch/pytorch/blob/master/torch/distributed/distributed_c10d.py#L899">code examples</a> to learn more.</li>
  <li><strong>(Prototype) CUDA-support in RPC using TensorPipe</strong> - This feature should bring consequent speed improvements for users of PyTorch RPC with multiple-GPU machines, as TensorPipe will automatically leverage NVLink when available, and avoid costly copies to and from host memory when exchanging GPU tensors between processes. When not on the same machine, TensorPipe will fall back to copying the tensor to host memory and sending it as a regular CPU tensor. This will also improve the user experience as users will be able to treat GPU tensors like regular CPU tensors in their code. Refer to this <a href="https://pytorch.org/docs/1.8.0/rpc.html">documentation</a> for more details.</li>
  <li><strong>(Prototype) Remote Module</strong> - This feature allows users to operate a module on a remote worker like using a local module, where the RPCs are transparent to the user. In the past, this functionality was implemented in an ad-hoc way and overall this feature will improve the usability of model parallelism on PyTorch. Refer to this <a href="https://pytorch.org/docs/master/rpc.html#remotemodule">documentation</a> for more details.</li>
</ul>

<h1 id="pytorch-mobile">PyTorch Mobile</h1>
<p>Support for PyTorch Mobile is expanding with a new set of tutorials to help new users launch models on-device quicker and give existing users a tool to get more out of our framework. These include:</p>
<ul>
  <li><a href="https://pytorch.org/tutorials/beginner/deeplabv3_on_ios.html">Image segmentation DeepLabV3 on iOS</a></li>
  <li><a href="https://pytorch.org/tutorials/beginner/deeplabv3_on_android.html">Image segmentation DeepLabV3 on Android</a></li>
</ul>

<p>Our new demo apps also include examples of image segmentation, object detection, neural machine translation, question answering, and vision transformers. They are available on both iOS and Android:</p>
<ul>
  <li><a href="https://github.com/pytorch/ios-demo-app">iOS demo app</a></li>
  <li><a href="https://github.com/pytorch/android-demo-app">Android demo app</a></li>
</ul>

<p>In addition to performance improvements on CPU for MobileNetV3 and other models, we also revamped our Android GPU backend prototype for broader models coverage and faster inferencing:</p>
<ul>
  <li><a href="https://pytorch.org/tutorials/prototype/vulkan_workflow.html">Android tutorial</a></li>
</ul>

<p>Lastly, we are launching the PyTorch Mobile Lite Interpreter as a prototype feature in this release. The Lite Interpreter allows users to reduce the runtime binary size. Please try these out and send us your feedback on the <a href="https://discuss.pytorch.org/c/mobile/">PyTorch Forums</a>. All our latest updates can be found on the <a href="https://pytorch.org/mobile/home/">PyTorch Mobile page</a></p>

<h3 id="prototype-pytorch-mobile-lite-interpreter">[Prototype] PyTorch Mobile Lite Interpreter</h3>
<p>PyTorch Lite Interpreter is a streamlined version of the PyTorch runtime that can execute PyTorch programs in resource constrained devices, with reduced binary size footprint. This prototype feature reduces binary sizes by up to 70% compared to the current on-device runtime in the current release.</p>
<ul>
  <li><a href="https://pytorch.org/tutorials/prototype/lite_interpreter.html">iOS/Android Tutorial</a></li>
</ul>

<h1 id="performance-optimization">Performance Optimization</h1>
<p>In 1.8, we are releasing the support for benchmark utils to enable users to better monitor performance. We are also opening up a new automated quantization API. See the details below:</p>

<h3 id="beta-benchmark-utils">(Beta) Benchmark utils</h3>
<p>Benchmark utils allows users to take accurate performance measurements, and provides composable tools to help with both benchmark formulation and post processing. This expected to be helpful for contributors to PyTorch to quickly understand how their contributions are impacting PyTorch performance.</p>

<p>Example:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torch.utils.benchmark</span> <span class="kn">import</span> <span class="n">Timer</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num_threads</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]:</span>
    <span class="n">timer</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span>
        <span class="n">stmt</span><span class="o">=</span><span class="s">"torch.add(x, y, out=out)"</span><span class="p">,</span>
        <span class="n">setup</span><span class="o">=</span><span class="s">"""
            n = 1024
            x = torch.ones((n, n))
            y = torch.ones((n, 1))
            out = torch.empty((n, n))
        """</span><span class="p">,</span>
        <span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">timer</span><span class="p">.</span><span class="n">blocked_autorange</span><span class="p">(</span><span class="n">min_run_time</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">num_threads</span><span class="si">}</span><span class="s"> thread</span><span class="si">{</span><span class="s">'s'</span> <span class="k">if</span> <span class="n">num_threads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s">' '</span><span class="p">:</span><span class="o">&lt;</span><span class="mi">4</span><span class="si">}</span><span class="s">"</span>
        <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">median</span> <span class="o">*</span> <span class="mf">1e6</span><span class="p">:</span><span class="o">&gt;</span><span class="mf">4.0</span><span class="n">f</span><span class="si">}</span><span class="s"> us   "</span> <span class="o">+</span>
        <span class="p">(</span><span class="sa">f</span><span class="s">"(</span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">median</span> <span class="o">/</span> <span class="n">results</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="n">median</span><span class="p">:.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x)"</span> <span class="k">if</span> <span class="n">num_threads</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="s">''</span><span class="p">)</span>
    <span class="p">)</span>

<span class="mi">1</span> <span class="n">thread</span>     <span class="mi">376</span> <span class="n">us</span>   
<span class="mi">2</span> <span class="n">threads</span>    <span class="mi">189</span> <span class="n">us</span>   <span class="p">(</span><span class="mf">2.0</span><span class="n">x</span><span class="p">)</span>
<span class="mi">4</span> <span class="n">threads</span>     <span class="mi">99</span> <span class="n">us</span>   <span class="p">(</span><span class="mf">3.8</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>
<ul>
  <li><a href="https://pytorch.org/docs/1.8.0/benchmark_utils.html?highlight=benchmark#">Documentation</a></li>
  <li><a href="https://pytorch.org/tutorials/recipes/recipes/benchmark.html">Tutorial</a></li>
</ul>

<h3 id="prototype-fx-graph-mode-quantization">(Prototype) FX Graph Mode Quantization</h3>
<p>FX Graph Mode Quantization is the new automated quantization API in PyTorch. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with <code class="language-plaintext highlighter-rouge">torch.fx</code>).</p>
<ul>
  <li><a href="https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization">Documentation</a></li>
  <li>Tutorials:
    <ul>
      <li><a href="https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_dynamic.html">(Prototype) FX Graph Mode Post Training Dynamic Quantization</a></li>
      <li><a href="https://pytorch.org/tutorials/prototype/fx_graph_mode_ptq_static.html">(Prototype) FX Graph Mode Post Training Static Qunatization</a></li>
      <li><a href="https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html">(Prototype) FX Graph Mode Quantization User Guide</a></li>
    </ul>
  </li>
</ul>

<h1 id="hardware-support">Hardware Support</h1>

<h3 id="beta-ability-to-extend-the-pytorch-dispatcher-for-a-new-backend-in-c">[Beta] Ability to Extend the PyTorch Dispatcher for a new backend in C++</h3>
<p>In PyTorch 1.8, you can now create new out-of-tree devices that live outside the <code class="language-plaintext highlighter-rouge">pytorch/pytorch</code> repo. The tutorial linked below shows how to register your device and keep it in sync with native PyTorch devices.</p>
<ul>
  <li><a href="https://pytorch.org/tutorials/advanced/extend_dispatcher.html">Tutorial</a></li>
</ul>

<h3 id="beta-amd-gpu-binaries-now-available">[Beta] AMD GPU Binaries Now Available</h3>
<p>Starting in PyTorch 1.8, we have added support for ROCm wheels providing an easy onboarding to using AMD GPUs. You can simply go to the standard <a href="https://pytorch.org/get-started/locally/">PyTorch installation selector</a> and choose ROCm as an installation option and execute the provided command.</p>

<p>Thanks for reading, and if you are excited about these updates and want to participate in the future of PyTorch, we encourage you to join the <a href="https://discuss.pytorch.org/">discussion forums</a> and <a href="https://github.com/pytorch/pytorch/issues">open GitHub issues</a>.</p>

<p>Cheers!</p>

<p><strong><em>Team PyTorch</em></strong></p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">www.linuxfoundation.org/legal/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2024</a>
          </li>
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

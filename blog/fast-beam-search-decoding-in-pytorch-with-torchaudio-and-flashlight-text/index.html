<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Beam search decoding with industry-leading speed from Flashlight Text (part of the Flashlight ML framework) is now available with official support in TorchAudio, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for any modeling setting that outputs token-level probability distributions over time steps.

" />

  <meta property="og:image" content="https://pytorch.org//assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-6.png" />
  <meta name="twitter:image" content="https://pytorch.org//assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-6.png" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text" />
<meta property="og:description" content="Beam search decoding with industry-leading speed from Flashlight Text (part of the Flashlight ML framework) is now available with official support in TorchAudio, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for any modeling setting that outputs token-level probability distributions over time steps.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text" />
<meta name="twitter:description" content="Beam search decoding with industry-leading speed from Flashlight Text (part of the Flashlight ML framework) is now available with official support in TorchAudio, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for any modeling setting that outputs token-level probability distributions over time steps.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2024</span>
            <p>September 18-19 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">August 29, 2022</p>
            <h1>
                <a class="blog-title">Fast Beam Search Decoding in PyTorch with TorchAudio and Flashlight Text</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Caroline Chen, Jacob Kahn (@jacob_d_kahn)
                      
                    </p>
                    <p>Beam search decoding with industry-leading speed from <a href="https://github.com/flashlight/text">Flashlight Text</a> (part of the <a href="https://arxiv.org/abs/2201.12465">Flashlight</a> ML framework) is now available with official support in <a href="https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder">TorchAudio</a>, bringing high-performance beam search and text utilities for speech and text applications built on top of PyTorch. The current integration supports CTC-style decoding, but it can be used for <em>any modeling setting that outputs token-level probability distributions over time steps</em>.</p>

<h2 id="a-brief-beam-search-refresher">A brief beam search refresher</h2>

<p>In speech and language settings, <em>beam search</em> is an efficient, greedy algorithm that can convert sequences of <em>continuous values</em> (i.e. probabilities or scores) into <em>graphs</em> or <em>sequences</em> (i.e. tokens, word-pieces, words) using <em>optional constraints</em> on valid sequences (i.e. a lexicon), <em>optional external scoring</em> (i.e. an LM which scores valid sequences), and other <em>score adjustments</em> for particular sequences.</p>

<p>In the example that follows, we’ll consider — a token set of {ϵ, a, b}, where ϵ is a special token that we can imagine denotes a space between words or a pause in speech. Graphics here and below are taken from Awni Hannun’s excellent <a href="https://distill.pub/2017/ctc/">distill.pub writeup</a> on CTC and beam search.</p>

<p align="center">
  <img src="/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-1.jpeg" width="70%" />
</p>

<p>With a greedy-like approach, beam search considers the next viable token given an existing sequence of tokens — in the example above, a, b, b is a valid sequence, but a, b, a is not. We <em>rank</em> each possible next token at each step of the beam search according to a scoring function. Scoring functions (s) typically looks something like:</p>

<p align="center">
  <img src="/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-2.jpeg" width="80%" />
</p>

<p>Where <strong>ŷ</strong> is a potential path/sequence of tokens, <strong>x</strong> is the input <em><strong>(P(ŷ|x)</strong></em> represents the model’s predictions over time), and 𝛼 is a weight on the language model probability <em><strong>(P(y)</strong></em> the probability of the sequence under the language model). Some scoring functions add <em><strong>𝜷</strong></em> which adjusts a score based on the length of the predicted sequence <strong>|ŷ|</strong>. This particular scoring function is used in <a href="https://arxiv.org/pdf/1911.08460.pdf">FAIR’s prior work</a> on end-to-end ASR, and there are many variations on scoring functions which can vary across application areas.</p>

<p>Given a particular sequence, to assess the next viable token in that sequence (perhaps constrained by a set of allowed words or sequences, such as a lexicon of words), the beam search algorithm scores the sequence with each candidate token added, and sorts token candidates based on those scores. For efficiency and since the number of paths is exponential in the token set size, the <em><strong>top-k</strong></em> highest-scoring candidates are kept — <em><strong>k</strong></em> represents the <em><strong>beam size</strong></em>.</p>

<p align="center">
  <img src="/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-3.jpeg" width="100%" />
</p>

<p align="center">There are many other nuances with how beam search can progress: similar hypothesis sequences can be “merged”, for instance.
</p>

<p>The scoring function can be further augmented to up/down-weight token insertion or long or short words. Scoring with <em>stronger external language</em> models, while incurring computational cost, can also significantly improve performance; this is frequently referred to as <em>LM fusion</em>. There are many other knobs to tune for decoding — these are documented in <a href="https://pytorch.org/audio/0.12.0/models.decoder.html#ctcdecoder">TorchAudio’s documentation</a> and explored further in <a href="https://pytorch.org/audio/0.12.0/tutorials/asr_inference_with_ctc_decoder_tutorial.html#beam-search-decoder-parameters">TorchAudio’s ASR Inference tutorial</a>. Since decoding is quite efficient, parameters can be easily swept and tuned.</p>

<p>Beam search has been used in ASR extensively over the years in far too many works to cite, and in strong, recent results and systems including <a href="https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf">wav2vec 2.0</a> and <a href="https://developer.nvidia.com/nvidia-nemo">NVIDIA’s NeMo</a>.</p>

<h2 id="why-beam-search">Why beam search?</h2>

<p>Beam search remains a fast competitor to heavier-weight decoding approaches such as <a href="https://arxiv.org/pdf/1211.3711.pdf">RNN-Transducer</a> that Google has invested in putting <a href="https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html">on-device</a> and has shown strong results with on <a href="https://arxiv.org/pdf/2010.10504.pdf">common benchmarks</a>. Autoregressive text models at scale can benefit from beam search as well. Among other things, beam search gives:</p>

<ul>
  <li>A flexible performance/latency tradeoff — by adjusting beam size and the external LM, users can sacrifice latency for accuracy or pay for more accurate results with a small latency cost. Decoding with no external LM can improve results at very little performance cost.</li>
  <li>Portability without retraining — existing neural models can benefit from multiple decoding setups and plug-and-play with external LMs without training or fine-tuning.</li>
  <li>A compelling complexity/accuracy tradeoff — adding beam search to an existing modeling pipeline incurs little additional complexity and can improve performance.</li>
</ul>

<h2 id="performance-benchmarks">Performance Benchmarks</h2>

<p>Today’s most commonly-used beam search decoding libraries today that support external language model integration include Kensho’s <a href="https://github.com/kensho-technologies/pyctcdecode">pyctcdecode</a>, NVIDIA’s <a href="https://github.com/NVIDIA/NeMo/tree/stable/scripts/asr_language_modeling">NeMo toolkit</a>. We benchmark the TorchAudio + Flashlight decoder against them with a <em>wav2vec 2.0</em> base model trained on 100 hours of audio evaluated on <a href="https://www.openslr.org/12">LibriSpeech</a> dev-other with the official <a href="https://github.com/kpu/kenlm/">KenLM</a> 3-gram LM. Benchmarks were run on Intel E5-2698 CPUs on a single thread. All computation was in-memory — KenLM memory mapping was disabled as it wasn’t widely supported.</p>

<p>When benchmarking, we measure the <em>time-to-WER (word error rate)</em> — because of subtle differences in the implementation of decoding algorithms and the complex relationships between parameters and decoding speed, some hyperparameters differed across runs. To fairly assess performance, we first sweep for parameters that achieve a baseline WER, minimizing beam size if possible.</p>

<p align="center">
  <img src="/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-4.jpeg" width="70%" />
</p>

<p align="center">
Decoding performance on Librispeech dev-other of a pretrained wav2vec 2.0 model. TorchAudio + Flashlight decoding outperforms by an order of magnitude at low WERs.
</p>

<p align="center">
  <img src="/assets/images/fast-beam-search-decoding-in-pytorch-with-torchaudio-and-flashlight-text-5.jpeg" width="70%" />
</p>

<p align="center">
Time-to-WER results, deferring to smaller beam size, across decoders. The TorchAudio + Flashlight decoder scales far better with larger beam sizes and at lower WERs.
</p>

<h2 id="torchaudio-api-and-usage">TorchAudio API and Usage</h2>

<p>TorchAudio provides a Python API for CTC beam search decoding, with support for the following:</p>

<ul>
  <li>lexicon and lexicon-free decoding</li>
  <li>KenLM n-gram language model integration</li>
  <li>character and word-piece decoding</li>
  <li>sample pretrained LibriSpeech KenLM models and corresponding lexicon and token files</li>
  <li>various customizable beam search parameters (beam size, pruning threshold, LM weight…)</li>
</ul>

<p>To set up the decoder, use the factory function torchaudio.models.decoder.ctc_decoder</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">torchaudio.models.decoder</span> <span class="kn">import</span> <span class="n">ctc_decoder</span><span class="p">,</span> <span class="n">download_pretrained_files</span>
<span class="n">files</span> <span class="o">=</span> <span class="n">download_pretrained_files</span><span class="p">(</span><span class="s">"librispeech-4-gram"</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">ctc_decoder</span><span class="p">(</span>
   <span class="n">lexicon</span><span class="o">=</span><span class="n">files</span><span class="p">.</span><span class="n">lexicon</span><span class="p">,</span>
   <span class="n">tokens</span><span class="o">=</span><span class="n">files</span><span class="p">.</span><span class="n">tokens</span><span class="p">,</span>
   <span class="n">lm</span><span class="o">=</span><span class="n">files</span><span class="p">.</span><span class="n">lm</span><span class="p">,</span>
   <span class="n">nbest</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
   <span class="p">...</span> <span class="n">additional</span> <span class="n">optional</span> <span class="n">customizable</span> <span class="n">args</span> <span class="p">...</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Given emissions of shape <em>(batch, time, num_tokens)</em>, the decoder will compute and return a List of batch Lists, each consisting of the nbest hypotheses corresponding to the emissions. Each hypothesis can be further broken down into tokens, words (if a lexicon is provided), score, and timesteps components.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emissions</span> <span class="o">=</span> <span class="n">acoustic_model</span><span class="p">(</span><span class="n">waveforms</span><span class="p">)</span>  <span class="c1"># (B, T, N)
</span><span class="n">batch_hypotheses</span> <span class="o">=</span> <span class="n">decoder</span><span class="p">(</span><span class="n">emissions</span><span class="p">)</span>  <span class="c1"># List[List[CTCHypothesis]]
</span>
<span class="c1"># transcript for a lexicon decoder
</span><span class="n">transcripts</span> <span class="o">=</span> <span class="p">[</span><span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">hypo</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">words</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="n">batch_hypotheses</span><span class="p">]</span>

<span class="c1"># transcript for a lexicon free decoder, splitting by sil token
</span><span class="n">batch_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">decoder</span><span class="p">.</span><span class="n">idxs_to_tokens</span><span class="p">(</span><span class="n">hypo</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">hypo</span> <span class="ow">in</span> <span class="n">batch_hypotheses</span><span class="p">]</span>
<span class="n">transcripts</span> <span class="o">=</span> <span class="p">[</span><span class="s">""</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="k">for</span> <span class="n">tokens</span> <span class="ow">in</span> <span class="n">batch_tokens</span><span class="p">]</span>
</code></pre></div></div>

<p>Please refer to the <a href="https://pytorch.org/audio/stable/models.decoder.html#ctcdecoder">documentation</a> for more API details, and the tutorial (<a href="https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html">ASR Inference Decoding</a>) or sample <a href="https://github.com/pytorch/audio/tree/main/examples/asr/librispeech_ctc_decoder">inference script</a> for more usage examples.</p>

<h2 id="upcoming-improvements">Upcoming Improvements</h2>

<p><strong>Full NNLM support</strong> — decoding with large neural language models (e.g. transformers) remains somewhat unexplored at scale. Already supported in Flashlight, we plan to add support in TorchAudio, allowing users to use custom decoder-compatible LMs. Custom word level language models are already available in the nightly TorchAudio build, and is slated to be released in TorchAudio 0.13.</p>

<p><strong>Autoregressive/seq2seq decoding</strong> — Flashlight Text also supports <a href="https://github.com/flashlight/text/blob/main/flashlight/lib/text/decoder/LexiconSeq2SeqDecoder.h">sequence-to-sequence (seq2seq) decoding</a> for autoregressive models, which we hope to add bindings for and add to TorchAudio and TorchText with efficient GPU implementations as well.</p>

<p><strong>Better build support</strong> — to benefit from improvements in Flashlight Text, TorchAudio will directly submodule Flashlight Text to make upstreaming modifications and improvements easier. This is already in effect in the nightly TorchAudio build, and is slated to be released in TorchAudio 0.13.</p>

<h2 id="citation">Citation</h2>

<p>To cite the decoder, please use the following:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">inproceedings</span><span class="p">{</span><span class="n">kahn2022flashlight</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Flashlight</span><span class="p">:</span> <span class="n">Enabling</span> <span class="n">innovation</span> <span class="ow">in</span> <span class="n">tools</span> <span class="k">for</span> <span class="n">machine</span> <span class="n">learning</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Kahn</span><span class="p">,</span> <span class="n">Jacob</span> <span class="n">D</span> <span class="ow">and</span> <span class="n">Pratap</span><span class="p">,</span> <span class="n">Vineel</span> <span class="ow">and</span> <span class="n">Likhomanenko</span><span class="p">,</span> <span class="n">Tatiana</span> <span class="ow">and</span> <span class="n">Xu</span><span class="p">,</span> <span class="n">Qiantong</span> <span class="ow">and</span> <span class="n">Hannun</span><span class="p">,</span> <span class="n">Awni</span> <span class="ow">and</span> <span class="n">Cai</span><span class="p">,</span> <span class="n">Jeff</span> <span class="ow">and</span> <span class="n">Tomasello</span><span class="p">,</span> <span class="n">Paden</span> <span class="ow">and</span> <span class="n">Lee</span><span class="p">,</span> <span class="n">Ann</span> <span class="ow">and</span> <span class="n">Grave</span><span class="p">,</span> <span class="n">Edouard</span> <span class="ow">and</span> <span class="n">Avidov</span><span class="p">,</span> <span class="n">Gilad</span> <span class="ow">and</span> <span class="n">others</span><span class="p">},</span>
  <span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">International</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Machine</span> <span class="n">Learning</span><span class="p">},</span>
  <span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">10557</span><span class="o">--</span><span class="mi">10574</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2022</span><span class="p">},</span>
  <span class="n">organization</span><span class="o">=</span><span class="p">{</span><span class="n">PMLR</span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">inproceedings</span><span class="p">{</span><span class="n">yang2022torchaudio</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Torchaudio</span><span class="p">:</span> <span class="n">Building</span> <span class="n">blocks</span> <span class="k">for</span> <span class="n">audio</span> <span class="ow">and</span> <span class="n">speech</span> <span class="n">processing</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Yang</span><span class="p">,</span> <span class="n">Yao</span><span class="o">-</span><span class="n">Yuan</span> <span class="ow">and</span> <span class="n">Hira</span><span class="p">,</span> <span class="n">Moto</span> <span class="ow">and</span> <span class="n">Ni</span><span class="p">,</span> <span class="n">Zhaoheng</span> <span class="ow">and</span> <span class="n">Astafurov</span><span class="p">,</span> <span class="n">Artyom</span> <span class="ow">and</span> <span class="n">Chen</span><span class="p">,</span> <span class="n">Caroline</span> <span class="ow">and</span> <span class="n">Puhrsch</span><span class="p">,</span> <span class="n">Christian</span> <span class="ow">and</span> <span class="n">Pollack</span><span class="p">,</span> <span class="n">David</span> <span class="ow">and</span> <span class="n">Genzel</span><span class="p">,</span> <span class="n">Dmitriy</span> <span class="ow">and</span> <span class="n">Greenberg</span><span class="p">,</span> <span class="n">Donny</span> <span class="ow">and</span> <span class="n">Yang</span><span class="p">,</span> <span class="n">Edward</span> <span class="n">Z</span> <span class="ow">and</span> <span class="n">others</span><span class="p">},</span>
  <span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">ICASSP</span> <span class="mi">2022</span><span class="o">-</span><span class="mi">2022</span> <span class="n">IEEE</span> <span class="n">International</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Acoustics</span><span class="p">,</span> <span class="n">Speech</span> <span class="ow">and</span> <span class="n">Signal</span> <span class="n">Processing</span> <span class="p">(</span><span class="n">ICASSP</span><span class="p">)},</span>
  <span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">6982</span><span class="o">--</span><span class="mi">6986</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2022</span><span class="p">},</span>
  <span class="n">organization</span><span class="o">=</span><span class="p">{</span><span class="n">IEEE</span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/legal/policies/">www.linuxfoundation.org/legal/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2024</a>
          </li>
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

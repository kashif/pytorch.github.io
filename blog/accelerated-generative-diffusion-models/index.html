<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Accelerated Generative Diffusion Models with PyTorch 2 | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Accelerated Generative Diffusion Models with PyTorch 2" />
<meta property="og:description" content="TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Accelerated Generative Diffusion Models with PyTorch 2" />
<meta name="twitter:description" content="TL;DR: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new torch.compile() compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2024</span>
            <p>September 18-19 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">April 14, 2023</p>
            <h1>
                <a class="blog-title">Accelerated Generative Diffusion Models with PyTorch 2</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Grigory Sizov, Michael Gschwind, Hamid Shojanazeri, Driss Guessous, Daniel Haziza, Christian Puhrsch
                      
                    </p>
                    <p><strong>TL;DR</strong>: PyTorch 2.0 nightly offers out-of-the-box performance improvement for Generative Diffusion models by using the new <code class="language-plaintext highlighter-rouge">torch.compile()</code> compiler and optimized implementations of Multihead Attention integrated with PyTorch 2.</p>

<h2 id="introduction">Introduction</h2>

<p>A large part of the recent progress in Generative AI came from denoising diffusion models, which allow producing high quality images and videos from text prompts. This family includes Imagen, DALLE, Latent Diffusion, and others. However, all models in this family share a common drawback: generation is rather slow, due to the iterative nature of the sampling process by which the images are produced. This makes it important to optimize the code running inside the sampling loop.</p>

<p>We took an open source implementation of a popular text-to-image diffusion model as a starting point and accelerated its generation using two optimizations available in PyTorch 2: compilation and fast attention implementation. Together with a few minor memory processing improvements in the code these optimizations give up to 49% inference speedup relative to the original implementation without <a href="https://github.com/facebookresearch/xformers">xFormers</a>, and 39% inference speedup relative to using the original code with xFormers (excluding the compilation time), depending on the GPU architecture and batch size. Importantly, the speedup comes without a need to install xFormers or any other extra dependencies.</p>

<p>The table below shows the improvement in runtime between the original implementation with xFormers installed and our optimized version with PyTorch-integrated memory efficient attention (originally developed for and released in the <a href="https://github.com/facebookresearch/xformers">xFormers</a> library)  and PyTorch compilation. The compilation time is excluded.</p>

<p><strong>Runtime improvement in % compared to original+xFormers</strong></p>

<p>See the absolute runtime numbers in section “Benchmarking setup and results summary”</p>

<table class="table table-bordered">
<thead>
  <tr>
   <td scope="col"><strong>GPU</strong>
   </td>
   <td scope="col"><strong>Batch size 1</strong>
   </td>
   <td scope="col"><strong>Batch size 2</strong>
   </td>
   <td scope="col"><strong>Batch size 4</strong>
   </td>
  </tr>
</thead>
  <tr>
   <td><strong>P100 (no compilation)</strong>
   </td>
   <td>-3.8
   </td>
   <td>0.44
   </td>
   <td>5.47
   </td>
  </tr>
  <tr>
   <td><strong>T4</strong>
   </td>
   <td>2.12
   </td>
   <td>10.51
   </td>
   <td>14.2
   </td>
  </tr>
  <tr>
   <td><strong>A10</strong>
   </td>
   <td>-2.34
   </td>
   <td>8.99
   </td>
   <td>10.57
   </td>
  </tr>
  <tr>
   <td><strong>V100</strong>
   </td>
   <td>18.63
   </td>
   <td>6.39
   </td>
   <td>10.43
   </td>
  </tr>
  <tr>
   <td><strong>A100</strong>
   </td>
   <td>38.5
   </td>
   <td>20.33
   </td>
   <td>12.17
   </td>
  </tr>
</table>

<p>One can notice the following:</p>

<ul>
  <li>The improvements are significant for powerful GPUs like A100 and V100. For those GPUs the improvement is most pronounced for batch size 1</li>
  <li>For less powerful GPUs we observe smaller speedups (or in two cases slight regressions). The batch size trend is reversed here: improvement is larger for larger batches</li>
</ul>

<p>In the following sections we describe the applied optimizations and provide detailed benchmarking data, comparing the generation time with various optimization features on/off.</p>

<p>Specifically, we benchmark 5 configurations and the plots below compare their absolute performance for different GPUs and batch sizes. For definitions of these configurations see section “Benchmarking setup and results”.</p>

<p><img src="/assets/images/2023-04-11-accelerated-generative-diffusion-models1.png" alt="Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1" style="max-height:800px; width:100%" /></p>

<p><img src="/assets/images/2023-04-11-accelerated-generative-diffusion-models2.png" alt="Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 2" style="max-height:800px; width:100%" /></p>

<p><img src="/assets/images/2023-04-11-accelerated-generative-diffusion-models3.png" alt="Benchmark of denoising diffusion text-to-image generation across GPU architectures, batch size 1" style="max-height:800px; width:100%" /></p>

<h2 id="optimizations">Optimizations</h2>

<p>Here we’ll go into more detail about the optimizations introduced into the model code. These optimizations rely on features of PyTorch 2.0 which has been released recently.</p>

<h3 id="optimized-attention">Optimized Attention</h3>

<p>One part of the code which we optimized is the scaled dot-product attention. Attention is known to be a heavy operation: naive implementation materializes the attention matrix, leading to time and memory complexity quadratic in sequence length. It is common for diffusion models to use attention (<code class="language-plaintext highlighter-rouge">CrossAttention</code>) as part of Transformer blocks in multiple parts of the U-Net. Since the U-Net runs at every sampling step, this becomes a critical point to optimize. Instead of custom attention implementation one can use <code class="language-plaintext highlighter-rouge">torch.nn.MultiheadAttention,</code> which in PyTorch 2 has optimized attention implementation is integrated into it. This optimization schematically boils down to the following pseudocode:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class CrossAttention(nn.Module):
    def __init__(self, ...):
        # Create matrices: Q, K, V, out_proj
        ...
    def forward(self, x, context=None, mask=None):
       # Compute out = SoftMax(Q*K/sqrt(d))V
       # Return out_proj(out)
       …
</code></pre></div></div>

<p>gets replaced with</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class CrossAttention(nn.Module):
    def __init__(self, ...):
        self.mha = nn.MultiheadAttention(...)
    def forward(self, x, context):
	return self.mha(x, context, context)
</code></pre></div></div>

<p>The optimized implementation of attention was available already in PyTorch 1.13 (see <a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/">here</a>) and widely adopted (see e.g. <a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2">HuggingFace transformers library example</a>). In particular, it integrates memory-efficient attention from the <a href="https://github.com/facebookresearch/xformers">xFormers</a> library and flash attention from <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a>. PyTorch 2.0 expands this to additional attention functions such as cross attention and custom kernels for further acceleration, making it applicable to diffusion models.</p>

<p>Flash attention is available on GPUs with compute capability SM 7.5 or SM 8.x - for example, on T4, A10, and A100, which are included in our benchmark (you can check compute capability of each NVIDIA GPU <a href="https://developer.nvidia.com/cuda-gpus#compute">here</a>). However, in our tests on A100 the memory efficient attention performed better than flash attention for the particular case of diffusion models, due to the small number of attention heads and small batch size.  PyTorch understands this and in this case chooses memory efficient attention over flash attention when both are available (see the logic <a href="https://github.com/pytorch/pytorch/blob/d8e795ecd53670682bd3b2e5ff1f378402b147d5/aten/src/ATen/native/transformers/cuda/sdp_utils.h#L33-L71">here</a>). For full control over the attention backends (memory-efficient attention, flash attention, “vanilla math”, or any future ones), power users can enable and disable them manually with the help of the context manager <a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel">torch.backends.cuda.sdp_kernel</a>.</p>

<h3 id="compilation">Compilation</h3>

<p>Compilation is a <a href="https://pytorch.org/get-started/pytorch-2.0/#user-experience">new feature of PyTorch 2.0</a>, enabling significant speedups with a very simple user experience. To invoke the default behavior, simply wrap a PyTorch module or a function into <code class="language-plaintext highlighter-rouge">torch.compile</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = torch.compile(model)
</code></pre></div></div>

<p>PyTorch compiler then turns Python code into a set of instructions which can be executed efficiently without Python overhead. The compilation happens dynamically the first time the code is executed. With the default behavior, under the hood PyTorch utilized <a href="https://pytorch.org/docs/stable/torch.compiler">TorchDynamo</a> to compile the code and <a href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor</a> to further optimize it. See <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">this tutorial</a> for more details.</p>

<p>Although the one-liner above is enough for compilation, certain modifications in the code can squeeze a larger speedup. In particular, one should avoid so-called graph breaks - places in the code which PyTorch can’t compile. As opposed to previous PyTorch compilation approaches (like TorchScript), PyTorch 2 compiler doesn’t break in this case. Instead it falls back on eager execution - so the code runs, but with reduced performance. We introduced a few minor changes to the model code to get rid of graph breaks. This included eliminating functions from libraries not supported by the compiler, such as <code class="language-plaintext highlighter-rouge">inspect.isfunction</code> and <code class="language-plaintext highlighter-rouge">einops.rearrange</code>. See this <a href="https://pytorch.org/docs/stable/torch.compiler_faq.html#identifying-the-cause-of-a-graph-break">doc</a> to learn more about graph breaks and how to eliminate them.</p>

<p>Theoretically, one can apply <code class="language-plaintext highlighter-rouge">torch.compile </code>on the whole diffusion sampling loop. However, in practice it is enough to just compile the U-Net. The reason is that <code class="language-plaintext highlighter-rouge">torch.compile</code> doesn’t yet have a loop analyzer and would recompile the code for each iteration of the sampling loop. Moreover, compiled sampler code is likely to generate graph breaks - so one would need to adjust it if one wants to get a good performance from the compiled version.</p>

<p>Note that compilation <a href="https://github.com/openai/triton/blob/b5d32896b1f89fc44a82f8df3bb010934c53f4f5/README.md?plain=1#L66-L68">requires GPU compute capability &gt;= SM 7.0</a> to run in non-eager mode. This covers all GPUs in our benchmarks -  T4, V100, A10, A100 - except for P100 (see the <a href="https://developer.nvidia.com/cuda-gpus#compute">full list</a>).</p>

<h3 id="other-optimizations">Other optimizations</h3>

<p>In addition, we have improved efficiency of GPU memory operations by eliminating some common pitfalls, e.g. creating a tensor on GPU directly rather than creating it on CPU and later moving to GPU. The places where such optimizations were necessary were determined by line-profiling and looking at CPU/GPU traces and <a href="https://github.com/brendangregg/FlameGraph">Flame Graphs</a>.</p>

<h2 id="benchmarking-setup-and-results-summary">Benchmarking setup and results summary</h2>

<p>We have two versions of code to compare: <em>original</em> and <em>optimized</em>. On top of this, several optimization features (xFormers, PyTorch memory efficient attention, compilation) can be turned on/off. Overall, as mentioned in the introduction, we will be benchmarking 5 configurations:</p>

<ul>
  <li><em>Original code without xFormers</em></li>
  <li><em>Original code with xFormers</em></li>
  <li><em>Optimized code with vanilla math attention backend and no compilation</em></li>
  <li><em>Optimized code with memory-efficient attention backend and no compilation</em></li>
  <li><em>Optimized code with memory-efficient attention backend and compilation</em></li>
</ul>

<p>As the <em>original version</em> we took the version of the code which uses PyTorch 1.12 and a custom implementation of attention. The <em>optimized version</em> uses <code class="language-plaintext highlighter-rouge">nn.MultiheadAttention</code> in <code class="language-plaintext highlighter-rouge">CrossAttention</code> and PyTorch 2.0.0.dev20230111+cu117. It also has a few other minor optimizations in PyTorch-related code.</p>

<p>The table below shows runtime of each version of the code in seconds, and the percentage improvement compared to the _original with xFormers. _The compilation time is excluded.</p>

<p><strong>Runtimes for batch size 1. In parenthesis - relative improvement with respect to the “Original with xFormers” row</strong></p>

<table class="table table-bordered">
<thead>
  <tr>
   <td><strong>Configuration</strong>
   </td>
   <td><strong>P100</strong>
   </td>
   <td><strong>T4</strong>
   </td>
   <td><strong>A10</strong>
   </td>
   <td><strong>V100</strong>
   </td>
   <td><strong>A100</strong>
   </td>
  </tr>
</thead>
  <tr>
   <td><strong>Original without xFormers</strong>
   </td>
   <td>30.4s (-19.3%)
   </td>
   <td>29.8s (-77.3%)
   </td>
   <td>13.0s (-83.9%)
   </td>
   <td>10.9s (-33.1%)
   </td>
   <td>8.0s (-19.3%)
   </td>
  </tr>
  <tr>
   <td><strong>Original with xFormers</strong>
   </td>
   <td><strong>25.5s</strong> (0.0%)
   </td>
   <td>16.8s (0.0%)
   </td>
   <td><strong>7.1s</strong> (0.0%)
   </td>
   <td>8.2s (0.0%)
   </td>
   <td>6.7s (0.0%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with vanilla math attention, no compilation</strong>
   </td>
   <td>27.3s (-7.0%)
   </td>
   <td>19.9s (-18.7%)
   </td>
   <td>13.2s (-87.2%)
   </td>
   <td>7.5s (8.7%)
   </td>
   <td>5.7s (15.1%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention, no compilation</strong>
   </td>
   <td>26.5s (-3.8%)
   </td>
   <td>16.8s (0.2%)
   </td>
   <td><strong>7.1s</strong> (-0.8%)
   </td>
   <td>6.9s (16.0%)
   </td>
   <td>5.3s (20.6%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention and compilation</strong>
   </td>
   <td>-
   </td>
   <td><strong>16.4s </strong>(2.1%)
   </td>
   <td>7.2s (-2.3%)
   </td>
   <td><strong>6.6s</strong> (18.6%)
   </td>
   <td><strong>4.1s</strong> (38.5%)
   </td>
  </tr>
</table>

<p><strong>Runtimes for batch size 2</strong></p>

<table class="table table-bordered">
<thead>
  <tr>
   <td><strong>Configuration</strong>
   </td>
   <td><strong>P100</strong>
   </td>
   <td><strong>T4</strong>
   </td>
   <td><strong>A10</strong>
   </td>
   <td><strong>V100</strong>
   </td>
   <td><strong>A100</strong>
   </td>
  </tr>
</thead>
  <tr>
   <td><strong>Original without xFormers</strong>
   </td>
   <td>58.0s (-21.6%)
   </td>
   <td>57.6s (-84.0%)
   </td>
   <td>24.4s (-95.2%)
   </td>
   <td>18.6s (-63.0%)
   </td>
   <td>12.0s (-50.6%)
   </td>
  </tr>
  <tr>
   <td><strong>Original with xFormers</strong>
   </td>
   <td>47.7s (0.0%)
   </td>
   <td>31.3s (0.0%)
   </td>
   <td>12.5s (0.0%)
   </td>
   <td>11.4s (0.0%)
   </td>
   <td>8.0s (0.0%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with vanilla math attention, no compilation</strong>
   </td>
   <td>49.3s (-3.5%)
   </td>
   <td>37.9s (-21.0%)
   </td>
   <td>17.8s (-42.2%)
   </td>
   <td>12.7s (-10.7%)
   </td>
   <td>7.8s (1.8%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention, no compilation</strong>
   </td>
   <td><strong>47.5s </strong>(0.4%)
   </td>
   <td>31.2s (0.5%)
   </td>
   <td>12.2s (2.6%)
   </td>
   <td>11.5s (-0.7%)
   </td>
   <td>7.0s (12.6%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention and compilation</strong>
   </td>
   <td>-
   </td>
   <td><strong>28.0s</strong> (10.5%)
   </td>
   <td><strong>11.4s</strong> (9.0%)
   </td>
   <td><strong>10.7s </strong>(6.4%)
   </td>
   <td><strong>6.4s</strong> (20.3%)
   </td>
  </tr>
</table>

<p><strong>Runtimes for batch size 4</strong></p>

<table class="table table-bordered">
<thead>
  <tr>
   <td><strong>Configuration</strong>
   </td>
   <td><strong>P100</strong>
   </td>
   <td><strong>T4</strong>
   </td>
   <td><strong>A10</strong>
   </td>
   <td><strong>V100</strong>
   </td>
   <td><strong>A100</strong>
   </td>
  </tr>
</thead>
  <tr>
   <td><strong>Original without xFormers</strong>
   </td>
   <td>117.9s (-20.0%)
   </td>
   <td>112.4s (-81.8%)
   </td>
   <td>47.2s (-101.7%)
   </td>
   <td>35.8s (-71.9%)
   </td>
   <td>22.8s (-78.9%)
   </td>
  </tr>
  <tr>
   <td><strong>Original with xFormers</strong>
   </td>
   <td>98.3s (0.0%)
   </td>
   <td>61.8s (0.0%)
   </td>
   <td>23.4s (0.0%)
   </td>
   <td>20.8s (0.0%)
   </td>
   <td>12.7s (0.0%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with vanilla math attention, no compilation</strong>
   </td>
   <td>101.1s (-2.9%)
   </td>
   <td>73.0s (-18.0%)
   </td>
   <td>28.3s (-21.0%)
   </td>
   <td>23.3s (-11.9%)
   </td>
   <td>14.5s (-13.9%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention, no compilation</strong>
   </td>
   <td><strong>92.9s </strong>(5.5%)
   </td>
   <td>61.1s (1.2%)
   </td>
   <td>23.9s (-1.9%)
   </td>
   <td>20.8s (-0.1%)
   </td>
   <td>12.8s (-0.9%)
   </td>
  </tr>
  <tr>
   <td><strong>Optimized with mem. efficient attention and compilation</strong>
   </td>
   <td>-
   </td>
   <td><strong>53.1s </strong>(14.2%)
   </td>
   <td><strong>20.9s</strong> (10.6%)
   </td>
   <td><strong>18.6s</strong> (10.4%)
   </td>
   <td><strong>11.2s</strong> (12.2%)
   </td>
  </tr>
</table>

<p>To minimize fluctuations and external influence on the performance of the benchmarked code, we ran each version of the code one after another, and then repeated this sequence 10 times: A, B, C, D, E,  A, B, … So the results of a typical run would look like the one in the picture below.. Note that one shouldn’t rely on comparison of absolute run times between different graphs, but comparison of run times_ inside_ one graph is pretty reliable, thanks to our benchmarking setup.</p>

<p><img src="/assets/images/2023-04-11-accelerated-generative-diffusion-models4.png" alt="Denoising diffusion model generation benchmarks" style="max-height:700px" /></p>

<p>Each run of text-to-image generation script produces several batches, the number of which is regulated by the CLI parameter <code class="language-plaintext highlighter-rouge">--n_iter</code>. In the benchmarks we used <code class="language-plaintext highlighter-rouge">n_iter = 2</code>, but introduced an additional “warm-up” iteration, which doesn’t contribute to the run time. This was necessary for the runs with compilation, because compilation happens the first time the code runs, and so the first iteration is much longer than all subsequent. To make comparison fair, we also introduced this additional “warm-up” iteration to all other runs.</p>

<p>The numbers in the table above are for number of iterations 2 (plus a “warm-up one”), prompt ”A photo”, seed 1, PLMS sampler, and autocast turned on.</p>

<p>Benchmarks were done using P100, V100, A100, A10 and T4 GPUs. The T4 benchmarks were done in Google Colab Pro. The A10 benchmarks were done on g5.4xlarge AWS instances with 1 GPU.</p>

<h2 id="conclusions-and-next-steps">Conclusions and next steps</h2>

<p>We have shown that new features of PyTorch 2 - compiler and optimized attention implementation - give performance improvements exceeding or comparable with what previously required installation of an external dependency (xFormers). PyTorch achieved this, in particular, by integrating memory efficient attention from xFormers into its codebase. This is a significant improvement for user experience, given that xFormers, being a state-of-the-art library, in many scenarios requires custom installation process and long builds.</p>

<p>There are a few natural directions in which this work can be continued:</p>

<ul>
  <li>The optimizations we implemented and described here are only benchmarked for text-to-image inference so far. It would be interesting to see how they affect training performance. PyTorch compilation can be directly applied to training; enabling training with PyTorch optimized attention is on the roadmap</li>
  <li>We intentionally minimized changes to the original model code. Further profiling and optimization can probably bring more improvements</li>
  <li>At the moment compilation is applied only to the U-Net model inside the sampler. Since there is a lot happening outside of U-Net (e.g. operations directly in the sampling loop), it would be beneficial to compile the whole sampler. However, this would require analysis of the compilation process to avoid recompilation at every sampling step</li>
  <li>Current code only applies compilation within the PLMS sampler, but it should be trivial to extend it to other samplers</li>
  <li>Besides text-to-image generation, diffusion models are also applied to other tasks - image-to-image and inpainting. It would be interesting to measure how their performance improves from PyTorch 2 optimizations</li>
</ul>

<p>See if you can increase performance of open source diffusion models using the methods we described, and share the results!</p>

<h2 id="resources">Resources</h2>

<ul>
  <li>PyTorch 2.0 overview, which has a lot of information on <code class="language-plaintext highlighter-rouge">torch.compile:</code> <a href="https://pytorch.org/get-started/pytorch-2.0/">https://pytorch.org/get-started/pytorch-2.0/</a></li>
  <li>Tutorial on <code class="language-plaintext highlighter-rouge">torch.compile</code>: <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html</a></li>
  <li>General compilation troubleshooting: <a href="https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html">https://pytorch.org/docs/stable/torch.compiler_troubleshooting.html</a></li>
  <li>Details on graph breaks: <a href="https://pytorch.org/docs/stable/torch.compiler_faq.html#identifying-the-cause-of-a-graph-break">https://pytorch.org/docs/stable/torch.compiler_faq.html#identifying-the-cause-of-a-graph-break</a></li>
  <li>Details on guards: <a href="https://pytorch.org/docs/stable/torch.compiler_guards_overview.html">https://pytorch.org/docs/stable/torch.compiler_guards_overview.html</a></li>
  <li>Video deep dive on TorchDynamo <a href="https://www.youtube.com/watch?v=egZB5Uxki0I">https://www.youtube.com/watch?v=egZB5Uxki0I</a></li>
  <li>Tutorial on optimized attention in PyTorch 1.12: <a href="https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html">https://pytorch.org/tutorials/beginner/bettertransformer_tutorial.html</a></li>
</ul>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>We would like to thank Geeta Chauhan, Natalia Gimelshein, Patrick Labatut, Bert Maher, Mark Saroufim, Michael Voznesensky and Francisco Massa for their valuable advice and early feedback on the text.</p>

<p>Special thanks to Yudong Tao initiating the work on using PyTorch native attention in diffusion models.</p>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2024</a>
          </li>
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

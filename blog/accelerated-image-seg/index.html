<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-T8XT4PS');</script>
  <!-- End Google Tag Manager -->
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
  <title>
    
      Accelerated Image Segmentation using PyTorch | PyTorch
    
  </title>
  <meta name="robots" content="index, follow" />

<meta name="description" content="Using Intel® Extension for PyTorch to Boost Image Processing Performance

" />

  <meta property="og:image" content="https://pytorch.org/assets/images/social-share.jpg" />
  <meta name="twitter:image" content="https://pytorch.org/assets/images/social-share.jpg" />

<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="Accelerated Image Segmentation using PyTorch" />
<meta property="og:description" content="Using Intel® Extension for PyTorch to Boost Image Processing Performance

" />
<meta property="og:site_name" content="PyTorch" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title" content="Accelerated Image Segmentation using PyTorch" />
<meta name="twitter:description" content="Using Intel® Extension for PyTorch to Boost Image Processing Performance

" />

  <link rel="stylesheet" href="/assets/main.css">
  <script src="/assets/vendor/jquery.min.js"></script>
  <script src="/assets/vendor/popper.min.js"></script>
  <script src="/assets/vendor/bootstrap.min.js"></script>
  <script src="/assets/vendor/anchor.min.js"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  </script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
  
    <script
  async
  src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"
></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "UA-117752657-2");
</script>

    <script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '243028289693773');
    fbq('track', 'PageView');
</script>

<noscript>
  <img height="1" width="1"
  src="https://www.facebook.com/tr?id=243028289693773&ev=PageView
  &noscript=1"/>
</noscript>

    <!-- Twitter universal website tag code -->
<img height="1" width="1" style="display:none;" alt="" src="https://analytics.twitter.com/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__analytics.twitter.com_i_adsct-3Fp-5Fid-3DTwitter-26p-5Fuser-5Fid-3D0-26txn-5Fid-3Do2gi1-26events-3D-255B-255B-2522pageview-2522-252Cnull-255D-255D-26tw-5Fsale-5Famount-3D0-26tw-5Forder-5Fquantity-3D0&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=o6i4D0V0088WH2RnzIoqiF-vj45PL-2sTrsxQ0SNO3A&e=)" />
<img height="1" width="1" style="display:none;" alt="" src="//t.co/i/adsct?p_id=Twitter&p_user_id=0&txn_id=o2gi1&events=%5B%5B%22pageview%22%2Cnull%5D%5D&tw_sale_amount=0&tw_order_quantity=0 (https://urldefense.proofpoint.com/v2/url?u=https-3A__linkprotect.cudasvc.com_url-3Fa-3Dhttp-253a-252f-252ft.co-252fi-252fadsct-253fp-5Fid-253dTwitter-2526p-5Fuser-5Fid-253d0-2526txn-5Fid-253do2gi1-2526events-253d-25255B-25255B-252522pageview-252522-25252Cnull-25255D-25255D-2526tw-5Fsale-5Famount-253d0-2526tw-5Forder-5Fquantity-253d0-26c-3DE-2C1-2CC33dLwIhtuEcl5FhdztSnUwsioeej5k-2DWy0RYREBAq51kGji32A2Cw94YU9vQBpY5tPN3AukEw3C-5F-2DlbtndnLoR7-5FA-5FLoH0Rr7zLtP1ykptN-26typo-3D1&d=DwMGaQ&c=5VD0RTtNlTh3ycd41b3MUw&r=GMr8XYCDyeQQZuD3noL91A&m=dAJyokk16UvYy-vMrGn_JwYiGfp_eEgo25B9iGDCG-A&s=Abgc3XBkhESv8XBYtLchdDZyISGsK6v_BB6cLMJGyCw&e=)" />
<!-- End Twitter universal website tag code -->

  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css" />
  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Pythorch Blog Posts" />
</head>


<body class="blog">
    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8XT4PS"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

    <div class="main-background blog-background blog-detail-background"></div>

    <div class="container-fluid header-holder blog-detail-header">
        <div class="container">
            

<div class="header-container">
  <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>

  <div class="main-menu">
  <ul>
    <li class="main-menu-item ">
      <a href="/get-started">Get Started</a>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Ecosystem 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="https://events.linuxfoundation.org/pytorch-conference/">
            <span class="dropdown-title">PyTorch Conference - 2024</span>
            <p>September 18-19 in San Francisco</p>
          </a>
          <a class="nav-dropdown-item" href="/ecosystem">
            <span class="dropdown-title">Tools</span>
            <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item">
      <div id="dropdownMenuButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Edge 
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/edge">
            <span class="dropdown-title">About PyTorch Edge</span>
          </a>
          <a class="nav-dropdown-item" href="/executorch-overview">
            <span class="dropdown-title">ExecuTorch</span>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item active">
      <a href="/blog">Blog</a>
    </li>

    <li class="main-menu-item">
      <a href="https://pytorch.org/tutorials">Tutorials</a>
    </li>

    <li class="main-menu-item">
      <div id="docsDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Docs
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/docs">
            <span class="dropdown-title docs-title">PyTorch</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/audio">
            <span class="dropdown-title docs-title">torchaudio</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/text">
            <span class="dropdown-title docs-title">torchtext</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/vision">
            <span class="dropdown-title docs-title">torchvision</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torcharrow">
            <span class="dropdown-title docs-title">torcharrow</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/data">
            <span class="dropdown-title docs-title">TorchData</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/torchrec">
            <span class="dropdown-title docs-title">TorchRec</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/serve">
            <span class="dropdown-title docs-title">TorchServe</span>
            <p></p>
          </a>
          <a class="nav-dropdown-item" href="/xla/release/1.6/index.html">
            <span class="dropdown-title docs-title">PyTorch on XLA Devices</span>
            <p></p>
          </a>
        </div>
      </div>
    </li>

    

    <li class="main-menu-item ">

      <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
        <a class="resource-option with-down-arrow">
          Resources
        </a>
        <div class="resources-dropdown-menu">
          <a class="nav-dropdown-item" href="/features">
            <span class=dropdown-title>About</span>
            <p>Learn about PyTorch’s features and capabilities</p>
          </a>
          <a class="nav-dropdown-item" href="/foundation">
            <span class=dropdown-title>PyTorch Foundation</span>
            <p>Learn more about the PyTorch Foundation.</p>
          </a>
          <a class="nav-dropdown-item" href="/#community-module">
            <span class=dropdown-title>Community</span>
            <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
          </a>
          <a class="nav-dropdown-item" href="/community-stories">
            <span class=dropdown-title>Community stories</span>
            <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
          </a>
          <a class="nav-dropdown-item" href="/resources">
            <span class=dropdown-title>Developer Resources</span>
            <p>Find resources and get questions answered</p>
          </a>
          <a class="nav-dropdown-item" href="/events">
            <span class=dropdown-title>Events</span>
            <p>Find events, webinars, and podcasts</p>
          </a>
          <a class="nav-dropdown-item" href="https://discuss.pytorch.org" target="_blank">
            <span class=dropdown-title>Forums</span>
            <p>A place to discuss PyTorch code, issues, install, research</p>
          </a>
          <a class="nav-dropdown-item" href="/hub">
            <span class=dropdown-title>Models (Beta)</span>
            <p>Discover, publish, and reuse pre-trained models</p>
          </a>
        </div>
      </div>
    </li>

    <li class="main-menu-item" id="github-main-menu-link">
      <a href="https://github.com/pytorch/pytorch">GitHub</a>
    </li>

    <li class="navSearchWrapper reactNavSearchWrapper" key="search">
      <div class="search-border">
        <div id="search-icon"></div>
        <input
          id="search-input"
          type="text"
          title="Search"
        />
        <div id="close-search">X</div>
      </div>
    </li>
  </ul>
</div>

<script src="/assets/main-menu-dropdown.js"></script>


  <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
</div>

        </div>
    </div>

    <div class="jumbotron jumbotron-fluid blog-detail-jumbotron">
        <div class="container blog-detail-container">
            <p class="featured-post">May 02, 2023</p>
            <h1>
                <a class="blog-title">Accelerated Image Segmentation using PyTorch</a>
            </h1>
        </div>
    </div>

    <div class="main-content-wrapper blog-detail-wrapper">
        <div class="main-content blog-detail-content">
            <div class="container">
                <img src="/assets/images/logo-icon.svg" class="img-fluid author-icon">
                <article class="pytorch-article">
                    <p class="author">
                      by
                      
                        Intel
                      
                    </p>
                    <p><em>Using Intel® Extension for PyTorch to Boost Image Processing Performance</em></p>

<p>PyTorch delivers great CPU performance, and it can be further accelerated with Intel® Extension for PyTorch. I trained an AI image segmentation model using PyTorch 1.13.1 (with ResNet34 + UNet architecture) to identify roads and speed limits from satellite images, all on the 4th Gen Intel® Xeon® Scalable processor.</p>

<p>I will walk you through the steps to work with a satellite image dataset called SpaceNet5 and how I optimized the code to make deep learning workloads feasible on CPUs just by flipping a few key switches.</p>

<p><strong>Before we get started, some housekeeping…</strong></p>

<p>The code accompanying this article is available in the examples folder in the <a href="http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5">Intel Extension for PyTorch repository</a>. I borrowed heavily from the <a href="http://github.com/avanetten/cresi/">City-Scale Road Extraction from Satellite Imagery (CRESI) repository</a>. I adapted it for the 4th Gen Intel Xeon processors with PyTorch optimizations and <a href="http://github.com/intel/intel-extension-for-pytorch">Intel Extension for PyTorch</a> optimizations. In particular, I was able to piece together a workflow using the <a href="http://github.com/avanetten/cresi/tree/main/notebooks">notebooks here</a>.</p>

<p>You can find the accompanying talk I gave <a href="http://www.youtube.com/watch?v=LVZWm5GFvAw">on YouTube</a>.</p>

<p>I also highly recommend these articles for a detailed explanation of how to get started with the SpaceNet5 data:</p>

<ul>
  <li><a href="http://medium.com/the-downlinq/the-spacenet-5-baseline-part-1-imagery-and-label-preparation-598af46d485e">The SpaceNet 5 Baseline — Part 1: Imagery and Label Preparation</a></li>
  <li><a href="http://medium.com/the-downlinq/the-spacenet-5-baseline-part-2-training-a-road-speed-segmentation-model-2bc93de564d7">The SpaceNet 5 Baseline — Part 2: Training a Road Speed Segmentation Model</a></li>
  <li><a href="https://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21">The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery</a></li>
  <li><a href="http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c">SpaceNet 5 Winning Model Release: End of the Road</a></li>
</ul>

<p>I referenced two Hugging Face blogs by Julien Simon; he ran his tests on the AWS instance <code class="language-plaintext highlighter-rouge">r7iz.metal-16xl</code>:</p>

<ul>
  <li><a href="http://huggingface.co/blog/intel-sapphire-rapids">Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1</a></li>
  <li><a href="http://huggingface.co/blog/intel-sapphire-rapids-inference">Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2</a></li>
</ul>

<p>The potential cost savings from using a CPU instance instead of a GPU instance on the major cloud service providers (CSP) can be significant. The latest processors are still being rolled out to the CSPs, so I’m using a 4th Gen Intel Xeon processor that is hosted on the Intel® Developer Cloud (you can sign up for the Beta here: <a href="http://cloud.intel.com/">cloud.intel.com</a>).</p>

<p>On AWS, you can select from the <code class="language-plaintext highlighter-rouge">r7iz.*</code> EC2 instances after you <a href="http://pages.awscloud.com/R7iz-Preview.html">sign up for the preview here</a> (Figure 1). At the time of writing, the new AI-acceleration engine, Intel® Advanced Matrix Extensions (Intel® AMX), is only available on bare metal but it should soon be enabled on the virtual machines.</p>

<p><img src="/assets/images/f1-4th-gen-xeon-aws-instances.png" alt="List of 4th Gen Xeon  instances on AWS EC2" style="max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 1</strong>. List of 4th Gen Xeon  instances on AWS EC2 (image by author)</em></small></p>

<p>On Google Cloud* Platform, you can select from the 4th Gen Xeon Scalable processors C3 VMs (Figure 2).</p>

<p><img src="/assets/images/f2-4th-gen-xeon-googlecloud-instances.png" alt="List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform" style="max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 2</strong>. List of 4th Gen Intel Xeon Scalable processor instances on Google Cloud Platform (image by author)</em></small></p>

<h2 id="hardware-introduction-and-optimizations">Hardware Introduction and Optimizations</h2>

<p>The 4th Gen Intel Xeon processors were released January 2023, and the bare-metal instance I am using has two sockets (each with 56 physical cores), 504 GB of memory, and Intel AMX acceleration. I installed a few key libraries in the backend to take control and monitor the sockets, memory, and cores that I am using on the CPU:</p>

<p><code class="language-plaintext highlighter-rouge">numactl</code> (with <code class="language-plaintext highlighter-rouge">sudo apt-get install numactl</code>)</p>

<p><code class="language-plaintext highlighter-rouge">libjemalloc-dev</code> (with <code class="language-plaintext highlighter-rouge">sudo apt-get install libjemalloc</code>)</p>

<p><code class="language-plaintext highlighter-rouge">intel-openmp</code> (with <code class="language-plaintext highlighter-rouge">conda install intel-openmp</code>)</p>

<p><code class="language-plaintext highlighter-rouge">gperftools</code> (with <code class="language-plaintext highlighter-rouge">conda install gperftools -c conda-forge</code>)</p>

<p>Both PyTorch and Intel Extension for PyTorch have helper scripts so that one does not need to explicitly use <code class="language-plaintext highlighter-rouge">intel-openmp</code> and <code class="language-plaintext highlighter-rouge">numactl</code>, but they do need to be installed in the backend. In case you want to set them up for other work, here is what I used for OpenMP* …</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export OMP_NUM_THREADS=36
export KMP_AFFINITY=granularity=fine,compact,1,0
export KMP_BLOCKTIME=1
</code></pre></div></div>

<p>… where <code class="language-plaintext highlighter-rouge">OMP_NUM_THREADS</code> is the number of threads allocated to the job, <code class="language-plaintext highlighter-rouge">KMP_AFFINITY</code> affects thread affinity settings (including packing threads close to each other, the state of pinning threads), and <code class="language-plaintext highlighter-rouge">KMP_BLOCKTIME</code> sets the time in milliseconds that an idle thread should wait before going to sleep.</p>

<p>Here’s what I used for <code class="language-plaintext highlighter-rouge">numactl</code> …</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>numactl -C 0-35 --membind=0 train.py
</code></pre></div></div>

<p>…where <code class="language-plaintext highlighter-rouge">-C</code> specifies which cores to use and <code class="language-plaintext highlighter-rouge">--membind</code> instructs the program to only use one socket (socket 0 in this case).</p>

<h2 id="spacenet-data">SpaceNet Data</h2>

<p>I am using a satellite image dataset from the <a href="http://spacenet.ai/sn5-challenge/">SpaceNet 5 Challenge</a>. Different cities can be downloaded for free from an AWS S3 bucket:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 ls s3://spacenet-dataset/spacenet/SN5_roads/tarballs/ --human-readable
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-09-03 20:59:32    5.8 GiB SN5_roads_test_public_AOI_7_Moscow.tar.gz
2019-09-24 08:43:02    3.2 GiB SN5_roads_test_public_AOI_8_Mumbai.tar.gz
2019-09-24 08:43:47    4.9 GiB SN5_roads_test_public_AOI_9_San_Juan.tar.gz
2019-09-14 13:13:26   35.0 GiB SN5_roads_train_AOI_7_Moscow.tar.gz
2019-09-14 13:13:34   18.5 GiB SN5_roads_train_AOI_8_Mumbai.tar.gz
</code></pre></div></div>

<p>You can use the following commands to download and unpack a file:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>aws s3 cp s3://spacenet-dataset/spacenet/SN5_roads/tarballs/SN5_roads_train_AOI_7_Moscow.tar.gz .
tar -xvzf ~/spacenet5data/moscow/SN5_roads_train_AOI_7_Moscow.tar.gz
</code></pre></div></div>

<h3 id="dataset-preparation">Dataset Preparation</h3>

<p>I used the Moscow satellite image dataset, which consists of 1,352 images of 1,300 by 1,300 pixels with corresponding street labels in separate text files. The dataset contains both 8-band multispectral images and 3-band RGB images. Figure 3 shows four sample RGB satellite images and their corresponding generated masks. I used the <a href="http://github.com/avanetten/cresi/blob/main/cresi/data_prep/speed_masks.py">speed_masks.py</a> script from the CRESI repository to generate the segmentation masks.</p>

<p><img src="/assets/images/f3-moscow-satellite-image-dataset.png" alt="Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits" style="max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 3</strong>. Satellite image 3-channel RGB chips from Moscow (top row) and corresponding pixel segmentation masks with varying speed limits (bottom row) (image by author)</em></small></p>

<p>There is a JSON configuration file that must be updated for all remaining components: training and validation split, training, and inference. <a href="http://github.com/avanetten/cresi/blob/main/cresi/configs/sn5_baseline_aws.json">An example configuration can be found here</a>. I perform an 80:20 training/validation split, making sure to point to the correct folder of satellite images and corresponding masks for training. The configuration parameters are explained in more in the <a href="http://github.com/intel/intel-extension-for-pytorch/tree/master/examples/cpu/usecase_spacenet5">notebook under examples in GitHub for Intel Extension for PyTorch here</a>.</p>

<h3 id="training-a-resnet34--unet-model">Training a ResNet34 + UNet Model</h3>

<p>I made some changes to the <code class="language-plaintext highlighter-rouge">cresi</code> code described below in order to run on a CPU and optimize the training. To run natively on a CPU, replace <code class="language-plaintext highlighter-rouge">self.model = nn.DataParallel(model).cuda()</code> with <code class="language-plaintext highlighter-rouge">self.model = nn.DataParallel(model)</code> in the <a href="https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py">train.py</a> script. In the <a href="https://github.com/avanetten/cresi/blob/main/cresi/01_train.py">01_train.py</a> script, remove <code class="language-plaintext highlighter-rouge">torch.randn(10).cuda()</code>.</p>

<p>To optimize training, add <code class="language-plaintext highlighter-rouge">import intel_extension_for_pytorch as ipex</code> to the import statements in the <a href="https://github.com/avanetten/cresi/blob/main/cresi/net/pytorch_utils/train.py">train.py</a> script. Just after defining the model and optimizer as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>self.model = nn.DataParallel(model)
self.optimizer = optimizer(self.model.parameters(), lr=config.lr)
</code></pre></div></div>

<p>Add the <code class="language-plaintext highlighter-rouge">ipex.optimize</code> line to use BF16 precision, instead of FP32: \</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>self.model, self.optimizer = ipex.optimize(self.model, 
    optimizer=self.optimizer,dtype=torch.bfloat16)
</code></pre></div></div>

<p>Add a line to do mixed-precision training just before running a forward pass and calculating the loss function:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with torch.cpu.amp.autocast():
    if verbose:
        print("input.shape, target.shape:", input.shape, target.shape)
    output = self.model(input)
    meter = self.calculate_loss_single_channel(output, target, meter, training, iter_size)
</code></pre></div></div>

<p>Now that we have optimized our training code, we can move onto training our model.</p>

<p>Like the <a href="https://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c">winner of the SpaceNet 5 competition</a>, I trained a ResNet34 encoder + UNet decoder model. It is pretrained from ImageNet weights, and the backbone is left completely unfrozen during training. The training can be run with the <a href="https://github.com/avanetten/cresi/blob/main/cresi/01_train.py">01_train.py</a> script, but in order to control the use of hardware I used a helper script. There are actually two helper scripts: one that comes with stock PyTorch and one that comes with Intel Extension for PyTorch. They both accomplish the same thing, but the first one from stock is <code class="language-plaintext highlighter-rouge">torch.backends.xeon.run_cpu</code>, and the second one from Intel Extension for PyTorch is <code class="language-plaintext highlighter-rouge">ipexrun</code>.</p>

<p>Here is what I ran in the command-line:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m torch.backends.xeon.run_cpu --ninstances 1 \
  --ncores_per_instance 32 \
  --log_path /home/devcloud/spacenet5data/moscow/v10_xeon4_devcloud22.04/logs/run_cpu_logs \
  /home/devcloud/cresi/cresi/01_train.py \
  /home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ipexrun --ninstances 1 \
--ncore_per_instance 32 \
/home/devcloud/cresi/cresi/01_train.py \
/home/devcloud/cresi/cresi/configs/ben/v10_xeon4_baseline_ben.json --fold=0
</code></pre></div></div>

<p>In both cases, I am asking PyTorch to run training on one socket with 32 cores. Upon running, I get a printout of what environment variables get set in the backend to understand how PyTorch is using the hardware:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO - Use TCMalloc memory allocator
INFO - OMP_NUM_THREADS=32
INFO - Using Intel OpenMP
INFO - KMP_AFFINITY=granularity=fine,compact,1,0
INFO - KMP_BLOCKTIME=1
INFO - LD_PRELOAD=/home/devcloud/.conda/envs/py39/lib/libiomp5.so:/home/devcloud/.conda/envs/py39/lib/libtcmalloc.so
INFO - numactl -C 0-31 -m 0 /home/devcloud/.conda/envs/py39/bin/python -u 01_train.py configs/ben/v10_xeon4_baseline_ben.json --fold=0
</code></pre></div></div>

<p>During training, I make sure that my total loss function is decreasing (i.e., the model is converging on a solution).</p>

<h3 id="inference">Inference</h3>

<p>After training a model, we can start to make predictions from satellite images alone. In the eval.py inference script, add import intel_extension_for_pytorch as ipex to the import statements. After loading the PyTorch model, use Intel Extension for PyTorch to optimize the model for BF16 inference:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>model = torch.load(os.path.join(path_model_weights, 
    'fold{}_best.pth'.format(fold)), 
    map_location = lambda storage, 
    loc: storage)
model.eval()
model = ipex.optimize(model, dtype = torch.bfloat16)
</code></pre></div></div>

<p>Just prior to running prediction, add two lines for mixed precision:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with torch.no_grad():
    with torch.cpu.amp.autocast():
        for data in pbar:
            samples = torch.autograd.Variable(data['image'], volatile=True)
            predicted = predict(model, samples, flips=self.flips)
</code></pre></div></div>

<p>To run inference, we can use the <a href="https://github.com/avanetten/cresi/blob/main/cresi/02_eval.py">02_eval.py</a> script. Now that we have a trained model, we can make predictions on satellite images (Figure 4). We can see that it does seem to map the roads closely to the image!</p>

<p><img src="/assets/images/f4-moscow-satellite-image-complete.png" alt="Moscow satellite image and accompanying prediction of roads" style="max-height:800px; max-width: 100%; display: block; margin-left: auto; margin-right: auto" /></p>

<p><small style="line-height: 1.1"><em><strong>Figure 4</strong>. Moscow satellite image and accompanying prediction of roads (image by author)</em></small></p>

<p>I realize that the model I’ve trained is overfit to the Moscow image data and probably won’t generalize well to other cities. However, the <a href="http://medium.com/the-downlinq/spacenet-5-winning-model-release-end-of-the-road-fd02e00b826c">winning solution to this challenge</a> used data from six cities (Las Vegas, Paris, Shanghai, Khartoum, Moscow, Mumbai) and performs well on new cities. In the future, one thing that would be worth testing is training on all six cities and running inference on another city to reproduce their results.</p>

<h2 id="note-on-post-processing">Note on Post-Processing</h2>

<p>There are further post-processing steps that can be performed to add the mask as graph features to maps. You can read more about the post-processing steps here:</p>

<p><a href="http://medium.com/the-downlinq/the-spacenet-5-baseline-part-3-extracting-road-speed-vectors-from-satellite-imagery-5d07cd5e1d21">The SpaceNet 5 Baseline — Part 3: Extracting Road Speed Vectors from Satellite Imagery</a></p>

<p><a href="https://github.com/avanetten/cresi/tree/main/cresi">Post-processing scripts</a></p>

<h2 id="conclusions">Conclusions</h2>

<p>In summary, we:</p>

<ul>
  <li>Created 1,352 image training masks (with speed limits) to correspond to our training satellite image data (from .geojson text file labels)</li>
  <li>Defined our configuration file for training and inference</li>
  <li>Split up our data into training and validation sets</li>
  <li>Optimized our code for CPU training, including using Intel Extension for PyTorch and BF16</li>
  <li>Trained a performant ResNet34 + UNet model on a 4th Gen Intel Xeon CPU</li>
  <li>Ran initial inference to see the prediction of a speed limit mask</li>
</ul>

<p>You can find <a href="http://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/">detailed benchmarks here for the 4th Gen Intel Xeon CPU here</a>.</p>

<h2 id="next-steps">Next Steps</h2>

<p>Extend the optimizations on an Intel CPU by using the Intel Extension for PyTorch:</p>

<p><code class="language-plaintext highlighter-rouge">pip install intel-extension-for-pytorch</code></p>

<p><code class="language-plaintext highlighter-rouge">git clone https://github.com/intel/intel-extension-for-pytorch</code></p>

<p><a href="http://linkedin.com/in/bconsolvo">Get in touch with me on LinkedIn</a> if you have any more questions!</p>

<p>More information about the Intel Extension for PyTorch <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html">can be found here</a>.</p>

<h3 id="get-the-software">Get the Software</h3>

<p>I encourage you to check out Intel’s other <strong><a href="https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html">AI Tools</a></strong> and <strong><a href="https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html">Framework</a></strong> optimizations and learn about the open, standards-based <strong><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html">oneAPI</a></strong> multiarchitecture, multivendor programming model that forms the foundation of Intel’s AI software portfolio.</p>

<p>For more details about 4th Gen Intel Xeon Scalable processor, visit <strong><a href="https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html">AI Platform</a></strong> where you can learn about how Intel is empowering developers to run high-performance, efficient end-to-end AI pipelines.</p>

<h3 id="pytorch-resources">PyTorch Resources</h3>

<ul>
  <li><a href="http://pytorch.org/get-started/pytorch-2.0/">PyTorch Get Started</a></li>
  <li><a href="https://dev-discuss.pytorch.org/t/pytorch-release-2-0-execution-update/1077">Dev Discussions</a></li>
  <li><a href="http://pytorch.org/docs/2.0/">Documentation</a></li>
</ul>

                </article>
            </div>
        </div>
    </div>

<!--    


 -->
    <div class="container-fluid docs-tutorials-resources">
  <div class="container">
    <div class="row">
      <div class="col-md-4 text-center">
        <h2>Docs</h2>
        <p>Access comprehensive developer documentation for PyTorch</p>
        <a class="with-right-arrow" href="/docs">View Docs</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Tutorials</h2>
        <p>Get in-depth tutorials for beginners and advanced developers</p>
        <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
      </div>

      <div class="col-md-4 text-center">
        <h2>Resources</h2>
        <p>Find development resources and get your questions answered</p>
        <a class="with-right-arrow" href="/resources">View Resources</a>
      </div>
    </div>
  </div>
</div>

<footer class="site-footer">
  <div class="container footer-container">
    <div class="footer-logo-wrapper">
      <a href="https://pytorch.org" class="footer-logo"></a>
    </div>

    <div class="footer-links-wrapper">
      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="https://pytorch.org">PyTorch</a></li>
          <li><a href="/get-started">Get Started</a></li>
          <li><a href="/features">Features</a></li>
          <li><a href="/ecosystem">Ecosystem</a></li>
          <li><a href="/blog">Blog</a></li>
          <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md" target="_blank">Contributing</a></li>
          <li><a href="https://github.com/pytorch/pytorch/security/policy" target="_blank">Security</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><a href="/resources">Resources</a></li>
          <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
          <li><a href="/docs">Docs</a></li>
          <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
          <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">GitHub Issues</a></li>
          <li><a href="/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>Stay up to date</p></li>
          <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
          <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
          <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
          <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          <li><a href="https://social.lfx.dev/@pytorch" rel="me" target="_blank">Mastodon</a></li>
        </ul>
      </div>

      <div class="footer-links-col">
        <ul>
          <li class="list-title"><p>PyTorch Podcasts</p></li>
          <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
          <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
          <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
          <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
        </ul>
      </div>
    </div>

    <div class="privacy-policy">
      <ul>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
        <li class="privacy-policy-links">|</li>
        <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
      </ul>
      <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation. 
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see 
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source 
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC, 
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
    </div>
  </div>
  <img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>

</footer>

<div class="mobile-main-menu">
  <div class="container-fluid">
    <div class="container">
      <div class="mobile-main-menu-header-container">
        <a class="header-logo" href="https://pytorch.org" aria-label="PyTorch"></a>
        <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
      </div>
    </div>
  </div>

  <div class="mobile-main-menu-links-container">

    <div class="main-menu">
      <ul>
        <li class="navSearchWrapper reactNavSearchWrapper tabletSearchWrapper" key="search">
          <div class="mobile-search-border">
            <input
              id="mobile-search-input"
              type="text"
              title="Search"
            />
            <div id="mobile-search-icon"></div>
          </div>
        </li>

        <li class="">
          <a href="/get-started">Get Started</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a>Ecosystem</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference - 2024</a>
          </li>
          <li>
            <a href="/ecosystem">Tools</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          <a>Edge</a>
        </li>
        <ul class="resources-mobile-menu-items">
          <li>
            <a href="/edge">About PyTorch Edge</a>
          </li>
          <li>
            <a href="/executorch-overview">ExecuTorch</a>
          </li>
        </ul>

        <li class="active">
          <a href="/blog">Blog</a>
        </li>

        <li>
          <a href="https://pytorch.org/tutorials">Tutorials</a>
        </li>

        <li class="resources-mobile-menu-title">
          <a href="/docs">Docs</a>
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/docs">PyTorch</a>
          </li>

          <li class="">
            <a href="/audio">torchaudio</a>
          </li>

          <li class="">
            <a href="/text">torchtext</a>
          </li>

          <li class="">
            <a href="/vision">torchvision</a>
          </li>

          <li class="">
            <a href="/torcharrow">torcharrow</a>
          </li>

          <li class="">
            <a href="/data">TorchData</a>
          </li> 

          <li class="">
            <a href="/torchrec">TorchRec</a>
          </li>

          <li class="">
            <a href="/serve">TorchServe</a>
          </li>

          <li class="">
            <a href="/xla/release/1.6/index.html">PyTorch on XLA Devices</a>
          </li>
        </ul>

        <li class="resources-mobile-menu-title">
          Resources
        </li>

        <ul class="resources-mobile-menu-items">
          <li class="">
            <a href="/features">About</a>
          </li>

          <li>
            <a href="/foundation">PyTorch Foundation</a>
          </li>
          
          <li>
            <a href="/#community-module">Community</a>
          </li>
          
          <li class="">
            <a href="/community-stories">Community stories</a>
          </li>

          <li class="">
            <a href="/resources">Developer Resources</a>
          </li>

          <li>
            <a href="/events">Events</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.org">Forum</a>
          </li>

          <li class="">
            <a href="/hub">Models (Beta)</a>
          </li>

        </ul>

        <li id="github-mobile-menu-link">
          <a href="https://github.com/pytorch/pytorch">GitHub</a>
        </li>
      </ul>
    </div>

  </div>
</div>


<script src="/assets/mobile-menu.js"></script>
<script src="/assets/scroll-to-anchor.js"></script>
<script src="/assets/external-links-new-tab.js"></script>

  <script src="/assets/search-bar.js"></script>

<script src="/assets/cookie-banner.js"></script>

<script type="text/javascript">
  mobileMenu.bind();
  anchors.add('.pytorch-article h2, .pytorch-article h3, .pytorch-article h4, .pytorch-article h5');

  // Add class to links that have code blocks, since we cannot create links in code blocks
  $("a code.highlighter-rouge").each(function(e) {
    $(this).closest("a").addClass("has-code");
  });

  scrollToAnchor.bind();

  var hasStaticHeader = $(".blog-header, .blog-detail-header, .resources-header, .get-started-header, .features-header, .ecosystem-header, .hub-header, .mobile-header").length > 0;

  if (!hasStaticHeader) {
    $(window).on("scroll", function() {
      var top = $(this).scrollTop();
      var fullPosition = $(".main-background").height() - $(".header-holder").height();

      if (top <= 40) {
        $(".header-holder").css({"backgroundColor": "rgba(0, 0, 0, 0.165)"});
      } else if (top >= fullPosition) {
        $(".header-holder").css({"backgroundColor": "#000000"});
      } else {
        var bgColor = "rgba(0, 0, 0, " + top / fullPosition + ")";
        $(".header-holder").css({"backgroundColor": bgColor});
      }
    });
  }
</script>


  <script src="/assets/track-events.js"></script>
  <script>trackEvents.bind();</script>



<div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="/assets/images/pytorch-x.svg">
  </div>
</div>


</body>

</html>

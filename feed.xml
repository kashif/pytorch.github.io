<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-06-14T11:35:47-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Reducing Model Checkpointing Times by Over 10x with PyTorch Distributed Asynchronous Checkpointing</title>
      <link href="https://pytorch.org/blog/reducing-checkpointing-times/" rel="alternate" type="text/html" title="Reducing Model Checkpointing Times by Over 10x with PyTorch Distributed Asynchronous Checkpointing" />
      <published>2024-06-12T00:00:00-07:00</published>
      <updated>2024-06-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/reducing-checkpointing-times</id>
      <content type="html" xml:base="https://pytorch.org/blog/reducing-checkpointing-times/">&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt;   With PyTorch distributed’s new asynchronous checkpointing feature, developed with feedback from IBM, we show how IBM Research Team is able to implement and reduce effective checkpointing time by a factor of 10-20x.  Example: 7B model ‘down time’ for a checkpoint goes from an average of 148.8 seconds to 6.3 seconds, or 23.62x faster.&lt;/p&gt;

&lt;p&gt;This directly translates into either more net training progress for every given 24 hour period while continuing to robustly checkpoint or more frequent checkpoints to shorten recovery window/time.&lt;/p&gt;

&lt;p&gt;In this note, we showcase the usage code and architecture that makes asynchronous checkpointing possible, along with timing results verified by IBM’s Research team.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/reducing-checkpointing-times/fg1.png&quot; alt=&quot;Async Checkpointing vs Standard Checkpointing&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Model checkpointing is a vital part of large model training, but checkpointing is an expensive process as each checkpoint process involves blocking training progress in order to save out the latest model weights.  However, not checkpointing or reducing checkpointing frequency can result in a significant loss in training progress. For example, failures such as a deadlock, straggler, and gpu errors require the training process to be restarted.  In order to restart from a failure, all (training) workers must stop their training process and be restarted from the last saved checkpoint.&lt;/p&gt;

&lt;p&gt;Thus, the inherent tension between robustness to failures vs training progress plays out as a tradeoff, but now with asynchronous checkpointing, PyTorch Distributed is able to significantly reduce this tension and enable frequent checkpoint with minimal impact to the overall training time.&lt;/p&gt;

&lt;p&gt;For background, it was almost exactly &lt;a href=&quot;https://pytorch.org/blog/performant-distributed-checkpointing/&quot;&gt;a year ago&lt;/a&gt; that we showcased how distributed checkpointing had massively sped up checkpointing times from the original torch.save() functionality.  As IBM Research had noted, torch.save could take up to 30 minutes to checkpoint a single 11B model (PyTorch 1.13).&lt;/p&gt;

&lt;p&gt;With advancements in distributed checkpointing, checkpoints could be done in under 4 minutes for up to 30B model sizes.&lt;/p&gt;

&lt;p&gt;With asynchronous checkpointing, the training time lost due to checkpointing now moves to under 30 seconds, and often as short as 6 seconds.&lt;/p&gt;

&lt;p&gt;To be clear, asynchronous checkpointing does not compress the actual serialization checkpointing time as the previous update showcased.  Rather it moves the final checkpointing process off the critical path (to cpu threads) to allow GPU training to continue while finalizing the checkpoint under separate threads.&lt;/p&gt;

&lt;p&gt;However, to the user, the effect is nearly the same in that down time for training due to checkpointing is substantially reduced, in many cases by 10x or even 20x.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/reducing-checkpointing-times/fg2.png&quot; alt=&quot;Async Dist Checkpointing&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As the above speedup chart shows, asynchronous checkpointing produces a 10x to 23x further improvement over the previous large improvements from a year ago.&lt;/p&gt;

&lt;h2 id=&quot;how-does-asynchronous-checkpointing-work&quot;&gt;How does Asynchronous Checkpointing work?&lt;/h2&gt;

&lt;p&gt;Asynchronous checkpointing modularizes the checkpointing process into two parts rather than one monolithic process.  The first phase copies the data from each gpu/rank from GPU to CPU.  This is the visible downtime to the user and can take from 6 - 14 seconds for 7B-13B model sizes. The second phase asynchronously copies the data from CPU memory to disk to persist the checkpoint.&lt;/p&gt;

&lt;p&gt;Once data is copied to CPU in the first phase, the GPU is free to immediately resume training.  Hence with asynchronous checkpointing the downtime for checkpointing is simply the time needed to copy over the latest model states to CPU.&lt;/p&gt;

&lt;p&gt;At the same time that training resumes, non-blocking CPU threads work with the freshly arrived data in memory to complete the full checkpointing/serialization process to disk (i.e. persistent save).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/reducing-checkpointing-times/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that PyTorch’s Distributed Checkpointer relies on collective communication calls for per-rank metadata necessary to optimize saves, as well as a final synchronization which marks checkpointing as complete and makes the action atomic. This can interfere with distributed training (as distributed training also relies upon similar calls to synchronize training across multiple GPUs) if the Checkpointing thread utilizes the same process group used for training.&lt;/p&gt;

&lt;p&gt;Specifically, a race condition between the calls could potentially cause training and asynch checkpointing save threads to wait on collective calls at the same time, resulting in a true collective hang.&lt;/p&gt;

&lt;p&gt;We avoided this scenario by initializing a separate process group for async checkpointing.  This separates the checkpointing collectives into their own logical process group, which thus ensures it will not interfere with collective calls in the main training threads.&lt;/p&gt;

&lt;h2 id=&quot;how-do-i-use-asynchronous-checkpointing-in-my-training&quot;&gt;How do I use Asynchronous Checkpointing in my training?&lt;/h2&gt;

&lt;p&gt;Usage of Asynchronous checkpointing is relatively straightforward.  Using the latest nightly version of PyTorch, you will want to initialize your process group with both nccl and gloo.  Gloo is required for the cpu threads portion.&lt;/p&gt;

&lt;p&gt;From there, create a duplicate process group which the asynchronous checkpointing will utilize.
Then train as usual but at the point when you want to checkpoint, use the asynchronous save api, passing in the states to save, the checkpoint id and the checkpoint process group.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/reducing-checkpointing-times/fg4.png&quot; alt=&quot;Code snippet&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Asynchronous checkpointing is also fully implemented in &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;torchtitan&lt;/a&gt;.  Here, it is implemented for use with pre-training your own Llama2 or Lllama3 model.  Using it is as simple as updating the toml config file:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/reducing-checkpointing-times/fg5.png&quot; alt=&quot;Code snippet&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future work&lt;/h2&gt;

&lt;p&gt;Checkpointing has made huge strides over the past year.  Moving from almost half an hour checkpoints to under 5 minutes with distributed checkpointing and now to under 30 seconds with asynchronous checkpointing.&lt;/p&gt;

&lt;p&gt;The last frontier - zero overhead checkpointing where even the &amp;lt; 30 seconds is eliminated by streaming the updated weights during the backward pass such that checkpoint data is already on cpu at the point asynchronous checkpointing would kick in.&lt;/p&gt;

&lt;p&gt;This would effectively move large model training to where checkpointing has no disruption or downtime enabling both more robustness (as checkpoints could be taken more frequently) and faster training progress due to no downtime for checkpointing.&lt;/p&gt;

&lt;p&gt;Source code link: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/state_dict_saver.py&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/distributed/checkpoint/state_dict_saver.py&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Meta: Lucas Pasqualin, Less Wright, Iris Zhang (PyTorch), Chien-Chin Huang; IBM Research: Swaminathan Sundararaman, Saransh Gupta, Raghu Ganti</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary: With PyTorch distributed’s new asynchronous checkpointing feature, developed with feedback from IBM, we show how IBM Research Team is able to implement and reduce effective checkpointing time by a factor of 10-20x. Example: 7B model ‘down time’ for a checkpoint goes from an average of 148.8 seconds to 6.3 seconds, or 23.62x faster.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Foundation Welcomes New Executive Director</title>
      <link href="https://pytorch.org/blog/new-executive-director/" rel="alternate" type="text/html" title="PyTorch Foundation Welcomes New Executive Director" />
      <published>2024-06-11T00:00:00-07:00</published>
      <updated>2024-06-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/new-executive-director</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-executive-director/">&lt;p&gt;&lt;img src=&quot;/assets/images/matt-white.jpg&quot; alt=&quot;Matt White&quot; style=&quot;max-width:220px;float:right;margin-left: 20px;&quot; /&gt;
The PyTorch Foundation is excited to welcome Matt White, our new executive director. The PyTorch Foundation formed in 2022 with the goal to drive adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects with PyTorch. Over the past 2 years, we’ve seen excellent growth across the project – with both contributor and member growth.&lt;/p&gt;

&lt;p&gt;“I am honored to be a part of the PyTorch Foundation, working with such a passionate and skilled community,” said Matt White. “I am looking forward to working with our contributors and members to advance the PyTorch ecosystem through research, cutting edge technologies and open source best practices.”&lt;/p&gt;

&lt;p&gt;Matt is a career technologist, researcher and innovator and has over 25 years of experience in AI, data, autonomous systems and simulations. He is the Co-founder and Chair of the Open Metaverse Foundation, a part of the Linux Foundation. Previously, Matt was the Director of the Generative AI Commons at the Linux Foundation, leading the advancement of open science and open-source artificial intelligence projects. He is also the GM of AI at the Linux Foundation.&lt;/p&gt;

&lt;h2 id=&quot;learn-more-about-the-pytorch-foundation&quot;&gt;Learn more about the PyTorch Foundation:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Join as a &lt;a href=&quot;https://pytorch.org/join&quot;&gt;member&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Read our latest &lt;a href=&quot;https://pytorch.org/blog/&quot;&gt;announcements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Access technical resources on &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;GitHub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation is excited to welcome Matt White, our new executive director. The PyTorch Foundation formed in 2022 with the goal to drive adoption of AI tooling by fostering and sustaining an ecosystem of open source, vendor-neutral projects with PyTorch. Over the past 2 years, we’ve seen excellent growth across the project – with both contributor and member growth.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">INT4 Decoding GQA CUDA Optimizations for LLM Inference</title>
      <link href="https://pytorch.org/blog/int4-decoding/" rel="alternate" type="text/html" title="INT4 Decoding GQA CUDA Optimizations for LLM Inference" />
      <published>2024-06-06T00:00:00-07:00</published>
      <updated>2024-06-06T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/int4-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/int4-decoding/">&lt;h4 id=&quot;an-efficient-decoding-grouped-query-attention-with-low-precision-kv-cache&quot;&gt;An efficient decoding Grouped-Query Attention with low-precision KV cache&lt;/h4&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Generative AI has taken the world by storm with its ability to generate content like humans.  Many of these generative AI tools are powered by large language models (LLMs), like Meta &lt;a href=&quot;https://llama.meta.com/llama3/&quot;&gt;Llama&lt;/a&gt; models and OpenAI’s &lt;a href=&quot;https://openai.com/gpt-4&quot;&gt;ChatGPT&lt;/a&gt;.  One of the main challenges of LLMs is supporting large “context lengths” (also known as “sequence lengths”).  The context length refers to the number of tokens that the model uses to understand the input context and generate responses.  Longer context lengths generally translate into higher precision and quality in the responses.  However, long context lengths are compute and memory intensive.  This is mainly due to the following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The computational complexity of attention layers increases proportionally with the context length (the growth rate depends on the attention algorithm).  As a result, when using long context lengths, the attention layers can become a bottleneck, particularly during the prefill phase where attentions are compute bound.&lt;/li&gt;
  &lt;li&gt;The KV cache size grows linearly with the context length, thus, putting higher pressure on the memory requirement and consequently slowing down the already memory-bound attention decoding.  Moreover, since the memory capacity is limited, the batch size reduces when the KV cache gets bigger, which generally results in a drop in throughput.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The computational complexity growth is difficult to solve compared to the other problem mentioned above.  One way to address the KV cache size growth problem is to use low precision KV cache.  From our experiments, group-wise INT4 quantization provides comparable results in terms of accuracy compared to BF16 KV cache during the decode phase in Meta Llama 2 inference.  However, we did not observe any latency improvement, despite reading 4x lesser data in attention decoding layers.  This means that the INT4 attention is 4x less efficient at utilizing precious HBM bandwidth than BF16 attention.&lt;/p&gt;

&lt;p&gt;In this note, we discuss the CUDA optimizations that we applied to INT4 GQA (grouped-query attention – the attention layer that we use in the LLM inference phase) to improve its performance by up to &lt;strong&gt;1.8x on the NVIDIA A100 GPU&lt;/strong&gt; and &lt;strong&gt;1.9x on the NVIDIA H100 GPU&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;optimized CUDA INT4 GQA&lt;/strong&gt; outperformed &lt;a href=&quot;https://pytorch.org/blog/flash-decoding/&quot;&gt;INT4 Flash-Decoding GQA&lt;/a&gt; (the best performing INT4 GQA that we used in the experiment mentioned above) by &lt;strong&gt;1.4x-1.7x on A100&lt;/strong&gt; and &lt;strong&gt;1.09x-1.3x on H100.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;optimized CUDA INT4 GQA&lt;/strong&gt; performs better than &lt;strong&gt;BF16 Flash-Decoding GQA&lt;/strong&gt; by &lt;strong&gt;1.5x-1.7x on A100 and 1.4x-1.7x on H100.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;h3 id=&quot;gqa-for-llm-inference&quot;&gt;GQA for LLM Inference&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.13245&quot;&gt;Grouped-Query Attention (GQA)&lt;/a&gt; is a variant of multi-head attention (MHA) where each KV cache head is shared across a group of query heads.  Our LLM inference adopts GQA as an attention layer in both the prefill and decode phases in order to reduce the capacity requirement for the KV cache.  We use multiple GPUs in inference where the KV cache and query heads are distributed across GPUs.  Each GPU runs an attention layer with a single KV head and a group of Q heads.  Therefore, when viewed from a single GPU perspective, the GQA component can also be described as &lt;a href=&quot;https://arxiv.org/abs/1911.02150&quot;&gt;MQA (Multi-Query Attention)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The simplified workflow of decoding GQA is illustrated in Figure 1.  GQA takes three main inputs: input query (denoted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt;), K cache (denoted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt;), and V cache (denoted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;).  Our current GQA inference uses BF16 for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; is a 4D BF16 tensor of shape (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;Q&lt;/sub&gt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; is a 4D BF16 tensor of shape (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;sub&gt;max&lt;/sub&gt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;KV&lt;/sub&gt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; is a 4D BF16 tensor of shape (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;sub&gt;max&lt;/sub&gt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;KV&lt;/sub&gt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;where&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; is the batch size (the number of input prompts)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;Q&lt;/sub&gt;&lt;/code&gt; is the number of query heads&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;KV&lt;/sub&gt;&lt;/code&gt; is the number of KV heads (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;Q&lt;/sub&gt;&lt;/code&gt; must be divisible by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;KV&lt;/sub&gt;&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;sub&gt;max&lt;/sub&gt;&lt;/code&gt; is the maximum context length&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt; is the head dimension (fixed to 128)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GQA is simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bmm(softmax(bmm(Q, K&lt;sup&gt;T&lt;/sup&gt;) / sqrt(D)), V)&lt;/code&gt;.  This yields a single output tensor (denoted as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt;) which is a 4D BF16 tensor that has the same shape as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt;.  Note that matrix multiplications are performed using BF16, however, accumulation and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt; are carried out in FP32.  We call this “BF16 GQA” as the KV cache is BF16.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg1.png&quot; alt=&quot;Figure 1: The simplified workflow of BF16 GQA for LLM inference&quot; style=&quot;width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt; The simplified workflow of BF16 GQA for LLM inference&lt;/p&gt;

&lt;h3 id=&quot;int4-gqa&quot;&gt;INT4 GQA&lt;/h3&gt;

&lt;p&gt;To further reduce the size of the KV cache, we explore the possibility of using INT4 for KV cache instead of BF16.  We estimate the potential performance improvement by calculating the computational intensity (CI) of INT4 GQA and comparing it to that of BF16 GQA, as CI represents FLOPS per byte.  We compute the CI for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PV&lt;/code&gt; (as shown in Equation 1) as they take KV cache as an operand.  Note that we disregard the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Q&lt;/code&gt; load as it is negligible compared to the KV cache.  We also ignore any intermediate data loads/stores that are not on global memory.  Thus, the CI only takes into account the computation FLOPS and KV cache loads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/eq.jpg&quot; alt=&quot;Equation 1&quot; style=&quot;width:100%;display:block;max-width:400px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Equation (1)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Assuming that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;Q&lt;/sub&gt;&lt;/code&gt; = 8 and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;sub&gt;KV&lt;/sub&gt;&lt;/code&gt; = 1, CI for BF16 KV cache is 8 while CI for INT4 KV cache is 32.  The CIs indicate that both BF16 and INT4 GQAs are memory bound (the peak CIs for BF16 tensor cores for A100 and H100 are &lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf&quot;&gt;312 TF / 2 TB/s = 141&lt;/a&gt; and &lt;a href=&quot;https://www.nvidia.com/en-us/data-center/h100/&quot;&gt;990 TF / 3.35 TB/s = 269&lt;/a&gt;; note that these TF numbers are without sparsity).  Moreover, with INT4 KV cache, we should expect up to 4x performance improvement compared to BF16 GQA.&lt;/p&gt;

&lt;p&gt;To enable INT4 KV cache support in GQA, we can dequantize the KV cache from INT4 to BF16 before passing it to the BF16 GQA operator.  However, since KV cache is typically large, copying it from/to global memory can be costly.  Moreover, decoding GQA is a memory bound operation (the memory unit is utilized much more heavily than the compute unit).  Figure 2 shows the NCU profile of the &lt;a href=&quot;https://github.com/facebookresearch/xformers/blob/9f6abadabdec17cd4b5c301632a44bf8216a7f35/xformers/csrc/attention/cuda/fmha/autogen/impl/cutlassF_bf16_aligned.cu#L33&quot;&gt;FMHA CUTLASS BF16 GQA kernel in xFormers&lt;/a&gt;, which is one of the state of the art implementations of GQA.  From the figure, it is obvious that memory is a bottleneck.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg2.png&quot; alt=&quot;Figure 2: The NCU profile of the FMHA CUTLASS BF16 kernel in xFormers&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt; The NCU profile of the &lt;a href=&quot;https://github.com/facebookresearch/xformers/blob/9f6abadabdec17cd4b5c301632a44bf8216a7f35/xformers/csrc/attention/cuda/fmha/autogen/impl/cutlassF_bf16_aligned.cu#L33&quot;&gt;FMHA CUTLASS BF16 kernel in xFormers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A more efficient alternative is to fuse INT4 dequantization with the GQA operation (shown in Figure 3).  In other words, having GQA read INT4 KV cache directly and perform the INT4 to BF16 conversion within the kernel.  This change can potentially reduce the amount of global memory reads required for the KV cache, which could lead to a decrease in latency.  We call this “INT4 GQA.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg3.png&quot; alt=&quot;Figure 3: The workflow of fused INT4 GQA&quot; style=&quot;width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt; The workflow of fused INT4 GQA&lt;/p&gt;

&lt;p&gt;We list the state of the art implementations of GQA in the table below along with their features in Table 1.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt; State of the art GQA implementations&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Implementation&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Denote&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Fused INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;a href=&quot;https://pytorch.org/blog/flash-decoding/&quot;&gt;Flash-Decoding&lt;/a&gt; (Triton implementation)
   &lt;/td&gt;
   &lt;td&gt;FD
   &lt;/td&gt;
   &lt;td&gt;Yes
   &lt;/td&gt;
   &lt;td&gt;Yes
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;Flash Attention (v2.3.3)&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;FA
   &lt;/td&gt;
   &lt;td&gt;Yes
   &lt;/td&gt;
   &lt;td&gt;No
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CUDA baseline
   &lt;/td&gt;
   &lt;td&gt;CU
   &lt;/td&gt;
   &lt;td&gt;Yes
   &lt;/td&gt;
   &lt;td&gt;Yes
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;All implementations, except for CU, support both split-K and non split-K.  CU only has the split-K implementation.  Only FA has a heuristic in the backend to determine whether to run the split-K or non split-K kernel.  For other implementations, users must explicitly choose which version to run.  In this note, we focus on long context lengths (in our experiments, we use a context length of 8192) and therefore opt for the split-K version wherever possible.&lt;/p&gt;

&lt;p&gt;As the baseline, we measured the performance of the state of the art GQA implementations on NVIDIA A100 and H100 GPUs.  The latency (time in microseconds) and achieved bandwidth (GB/s) are reported in Table 2.  Note that we ran a range of split-Ks (from 2 to 128 splits) and reported the best performance for each implementation.  For all experiments, we use a context length of 8192.  For INT4 GQA, we used row-wise quantization (i.e., num quantized groups = 1).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 2&lt;/strong&gt; Baseline GQA performance&lt;/p&gt;

&lt;p&gt;On A100&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;139
   &lt;/td&gt;
   &lt;td&gt;133
   &lt;/td&gt;
   &lt;td&gt;183
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;245
   &lt;/td&gt;
   &lt;td&gt;229
   &lt;/td&gt;
   &lt;td&gt;335
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;433
   &lt;/td&gt;
   &lt;td&gt;555
   &lt;/td&gt;
   &lt;td&gt;596
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;826
   &lt;/td&gt;
   &lt;td&gt;977
   &lt;/td&gt;
   &lt;td&gt;1127
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1607
   &lt;/td&gt;
   &lt;td&gt;1670
   &lt;/td&gt;
   &lt;td&gt;2194
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Effective Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;965
   &lt;/td&gt;
   &lt;td&gt;1012
   &lt;/td&gt;
   &lt;td&gt;736
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;1097
   &lt;/td&gt;
   &lt;td&gt;1175
   &lt;/td&gt;
   &lt;td&gt;802
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;1240
   &lt;/td&gt;
   &lt;td&gt;968
   &lt;/td&gt;
   &lt;td&gt;901
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;1301
   &lt;/td&gt;
   &lt;td&gt;1100
   &lt;/td&gt;
   &lt;td&gt;954
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1338
   &lt;/td&gt;
   &lt;td&gt;1287
   &lt;/td&gt;
   &lt;td&gt;980
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;On H100&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;91
   &lt;/td&gt;
   &lt;td&gt;90
   &lt;/td&gt;
   &lt;td&gt;114
   &lt;/td&gt;
   &lt;td&gt;70
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;148
   &lt;/td&gt;
   &lt;td&gt;146
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;113
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;162
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;271
   &lt;/td&gt;
   &lt;td&gt;298
   &lt;/td&gt;
   &lt;td&gt;361
   &lt;/td&gt;
   &lt;td&gt;205
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;294
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;515
   &lt;/td&gt;
   &lt;td&gt;499
   &lt;/td&gt;
   &lt;td&gt;658
   &lt;/td&gt;
   &lt;td&gt;389
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;558
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1011
   &lt;/td&gt;
   &lt;td&gt;1260
   &lt;/td&gt;
   &lt;td&gt;756
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;1066
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Effective Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;1481
   &lt;/td&gt;
   &lt;td&gt;1496
   &lt;/td&gt;
   &lt;td&gt;1178
   &lt;/td&gt;
   &lt;td&gt;511
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;371
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;1815
   &lt;/td&gt;
   &lt;td&gt;1840
   &lt;/td&gt;
   &lt;td&gt;1345
   &lt;/td&gt;
   &lt;td&gt;631
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;443
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;1982
   &lt;/td&gt;
   &lt;td&gt;1802
   &lt;/td&gt;
   &lt;td&gt;1487
   &lt;/td&gt;
   &lt;td&gt;699
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;487
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;2087
   &lt;/td&gt;
   &lt;td&gt;2156
   &lt;/td&gt;
   &lt;td&gt;1634
   &lt;/td&gt;
   &lt;td&gt;736
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;513
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;2150
   &lt;/td&gt;
   &lt;td&gt;2127
   &lt;/td&gt;
   &lt;td&gt;1706
   &lt;/td&gt;
   &lt;td&gt;757
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;537
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;First, let’s discuss the BF16 GQA performance: CU ranks last in terms of performance among all implementations.  FD and FA have comparable performance.  When the batch size is less than or equal to 64, FA utilizes the split-K kernel and performs slightly better than FD.  However, when the batch size is greater than 64, FD performs better.&lt;/p&gt;

&lt;p&gt;The same trend holds true for INT4 GQAs. However, we did not measure the performance of FA as it does not support INT4 KV cache. FD outperforms CU for all cases.&lt;/p&gt;

&lt;p&gt;When comparing the latencies of FD between BF16 and INT4 GQAs, we find that they are almost identical.  This suggests that &lt;em&gt;INT4 GQA is highly inefficient&lt;/em&gt;, which can be further confirmed by the significantly lower achievable bandwidth for INT4 GQA compared to BF16 GQA.  The same trend is also true when looking at the performance of CU.&lt;/p&gt;

&lt;h3 id=&quot;cuda-with-tensor-cores-int4-gqa-implementation&quot;&gt;CUDA with Tensor Cores INT4 GQA Implementation&lt;/h3&gt;

&lt;p&gt;In this section, we briefly describe our baseline implementation which is CUDA with tensor cores INT4 GQA (CU).  Each thread block processes only one KV head and a group of query heads from one input prompt.  Therefore, each thread block performs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mm(softmax(mm(Q, K&lt;sup&gt;T&lt;/sup&gt;) / sqrt(D)), V)&lt;/code&gt;; notice that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mm&lt;/code&gt; is being performed not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bmm&lt;/code&gt;.  Moreover, since this is a split-K implementation, tokens in the KV cache are split among different thread blocks.  Note that each thread block contains 4 warps (each warp contains 32 threads for NVIDIA A100 and H100 GPUs).  Work in each thread block is split among warps.  Within each warp, we use the &lt;a href=&quot;https://bruce-lee-ly.medium.com/nvidia-tensor-core-introduction-to-wmma-api-programming-21bcfee4ec45&quot;&gt;WMMA&lt;/a&gt; API to compute matrix multiplication on tensor cores.  Figure 4 demonstrates the work partitioning in CU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg4.jpg&quot; alt=&quot;Figure 4: CU work partitioning&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt; CU work partitioning&lt;/p&gt;

&lt;h2 id=&quot;optimizing-cuda-with-tensor-cores-kernel-of-int4-gqa&quot;&gt;Optimizing CUDA with Tensor Cores Kernel of INT4 GQA&lt;/h2&gt;

&lt;p&gt;In this note, we discuss the optimizations that we have applied to the CUDA with tensor cores implementation of INT4 GQA (CU).  The ideal goal is to improve the INT4 GQA performance by 4 times based on the CI analysis in the previous section.  Note that the query size is negligible compared to the KV cache size when the context length is long.&lt;/p&gt;

&lt;p&gt;In our analysis, we used the &lt;a href=&quot;https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html&quot;&gt;NVIDIA Nsight Compute (NCU)&lt;/a&gt; as the main profiler.  Our general bottleneck elimination approach is to minimize the stall cycles.  We applied 10 optimizations to INT4 GQA, three of which are specific for NVIDIA A100/H100 GPUs.  These optimizations are well known CUDA optimization techniques which can be generalized to many applications.&lt;/p&gt;

&lt;p&gt;It is worth noting that the reason that we choose to optimize the CUDA implementation rather than the Flash-Decoding implementation (FD) (which is Triton based) is because with CUDA, we have a better control of how the low-level instructions are being generated.  Many optimization techniques that we apply such as, operating on tensor core fragments directly (Optimizations 7-9), cannot be done through Triton since it does not expose low-level details to developers.  However, these optimizations can be integrated into the compiler-based solution to make the optimizations available to broader operators, which is indeed a part of our future plan.&lt;/p&gt;

&lt;h3 id=&quot;optimization-1-unroll-k-loads&quot;&gt;Optimization 1: Unroll &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; Loads&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The NCU profile shows that during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loading, there are only 2 global loads followed by &lt;em&gt;memory stalls&lt;/em&gt; at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dequantize_permuted_int4&lt;/code&gt;.  The memory stalls are the long scoreboard stalls which indicates the waits for global memory access.  This suggests that the kernel does not issue sufficient memory loads&lt;/p&gt;

&lt;p&gt;to hide the global load latency.  The kernel issues data loading, and then waits to consume the data immediately causing the global load latency to be exposed.  The stalls are shown in Figure 5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg5.png&quot; alt=&quot;Figure 5: K loading before unrolling&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt; K loading before unrolling (the numbers that the arrows point to are stall cycles caused by global memory wait)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In the baseline implementation, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; to load 8 INT4 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; values in a single load and we perform 2 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; loads in each iteration, which is 16 INT4 K values.  To allow for a better global load latency hiding, we issue 8 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; loads instead of two before consuming the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; values in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dequantize_permuted_int4&lt;/code&gt;.  This allows the compiler to unroll the loads as well as reorder the instructions to hide the global load latency better.  Figure 6 shows the NCU profile of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loading after unrolling.  Comparing Figure 5 and Figure 6, we effectively reduce the stall cycles by unrolling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg6.png&quot; alt=&quot;Figure 6: K loading after unrolling&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt; K loading after unrolling (the numbers that the arrows point to are stall cycles caused by global memory wait)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 3&lt;/strong&gt; Performance of Optimization 1 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 1&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;134
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;267
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.02&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.07&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;237
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;302
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.99&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;422
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;339
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.02&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.08&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;806
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;355
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.01&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.07&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1550
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;369
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.02&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.07&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-2-improve-p-type-casting-fp32-bf16&quot;&gt;Optimization 2: Improve &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; Type Casting (FP32-&amp;gt;BF16)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since the product of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax(bmm(Q, K&lt;sup&gt;T&lt;/sup&gt;) / sqrt(D))&lt;/code&gt; is FP32 (denoted as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; in Figure 3), the kernel has to convert &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; from FP32 to BF16 before feeding it to the next &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bmm&lt;/code&gt; computation.  The kernel performs the FP32 to BF16 conversion of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; by copying the FP32 data from one location in shared memory to another location in shared memory.  This causes stalls during the shared memory access (shown in Figure 7) which might be caused by (1) the shared memory indirection; and (2) the shared memory bank conflict since each thread accesses an 16-bit element (because of this, two threads can access the same memory bank simultaneously).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg7.png&quot; alt=&quot;Figure 7: P type casting before Optimization 2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; type casting before Optimization 2 (the number that the arrow points to is stall cycles caused by shared memory wait)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We use all threads in the thread block to do in-place type conversion.  Each thread operates on two consecutive elements in order to avoid the shared memory bank conflict when storing BF16.  All threads work on the same head (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt;) at the same time to guarantee correctness of the conversion.  The in-place conversion steps are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each thread loads 2 FP32 token elements from the same head from the shared memory into registers&lt;/li&gt;
  &lt;li&gt;Call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__syncthreads()&lt;/code&gt; to make sure that every thread finishes reading the data&lt;/li&gt;
  &lt;li&gt;Each thread converts its data to 2 BF16 token elements and then stores the results to the same shared memory&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some optimizations that we apply to the implementation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use vector types (especially &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nv_bfloat2&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Unroll data loading/storing, i.e., performing multiple loads before calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__syncthreads()&lt;/code&gt; and performing multiple stores after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__syncthreads()&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After this optimization, long stalls are not observed during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; type casting as shown in Figure 8.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg8.png&quot; alt=&quot;Figure 8: P type casting after Optimization 2&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; type casting after Optimization 2 (the numbers that the arrow points to are stall cycles caused by shared memory wait)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Culprits:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Since we unroll data loading/storing by using registers as an intermediate storage, the number of registers per thread increases resulting in reduced occupancy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 4&lt;/strong&gt; Performance of Optimization 2 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 2&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 2&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;126
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;285
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.14&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;221
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;324
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.06&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.16&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;395
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.15&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;749
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;382
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.16&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1435
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;399
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.16&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-3-remove-local-memory-usage-for-max-qkt-computation&quot;&gt;Optimization 3: Remove Local Memory Usage for max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the softmax computation, the kernel has to compute max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; for each head. It uses a temporary “thread-local” storage for storing per-thread max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; results (one float value for each head).  Depending on the compiler, the thread-local storage can be allocated on registers (on chip) or the local memory (off chip == global memory).  Unfortunately, in the baseline, the thread-local storage resides in the local memory which is much slower than the registers (shown in Figure 9).  We suspect that this is because the compiler cannot determine the indices of thread-local storage at compile time (since the number of heads (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt;) in the kernel is a runtime variable).  Accessing local memory as if accessing registers can hurt the performance of the kernel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg9.png&quot; alt=&quot;Figure 9: Local memory access during max QKT computation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9&lt;/strong&gt; Local memory access during max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We realize that we do not need &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;H&lt;/code&gt; (number of heads) floats as temporary storage per thread since each thread can compute max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; for only one head instead of all the heads.  Thus, we only need one float per thread, which can be easily stored in a register.  To accumulate the max results among warps, we use shared memory.  This optimization eliminates the local memory usage during max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 5&lt;/strong&gt; Performance of Optimization 3 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 3&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 3&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;119
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;300
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.14&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.20&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;206
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;348
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.14&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;368
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;389
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.24&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;696
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;411
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.24&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1338
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;428
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.18&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.24&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-4-remove-local-memory-usage-for-row-sum&quot;&gt;Optimization 4: Remove local memory usage for row sum&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Similar to&lt;a href=&quot;https://www.internalfb.com/diff/D50183201&quot;&gt; &lt;/a&gt;Optimization 3, the local memory usage problem is also observed during the row sum computation in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt; computation.  Since local memory is off chip, accessing it as if accessing registers can hurt the performance of the kernel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;We apply the same solution as the max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation for the row sum computation.  That is to have each thread compute a row sum of only one head, which requires only one float per thread.  This eliminates the need for local memory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 6&lt;/strong&gt; Performance of Optimization 4 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 4&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 4&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;118
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;302
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.15&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;204
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.15&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.26&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;393
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.19&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;688
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;416
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.18&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.26&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1328
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.19&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-5-add-prefetch-for-v-load&quot;&gt;Optimization 5: Add prefetch for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; load&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The same issue as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loading is observed when loading &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt;.  That is, the kernel issues data loading, and then waits to consume the data immediately causing the global load latency to be exposed.  However, when using the unrolling technique mentioned above, the compiler allocates the temporary buffer on local memory instead of registers causing a large slow down.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We adopt the data prefetching technique for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; loading.  We load the next iteration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; values immediately after the current iteration values are consumed.  This allows the data loading to be overlapped with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PK&lt;/code&gt; computation resulting in better kernel performance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 7&lt;/strong&gt; Performance of Optimization 5 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 5&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 5&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;109
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;327
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.31&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;194
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;370
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.33&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;414
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.32&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;649
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;441
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.26&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.33&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1244
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;460
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.27&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.33&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-6-add-group-wise-int4-groups--4-with-vector-load&quot;&gt;Optimization 6: Add Group-Wise INT4 (Groups = 4) with Vector Load&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Prior to this optimization, CU only supported row-wise INT4 quantization.  That is, every column in each row shares the same scales.  The scales of each row are stored in the first 4 bytes of each row as shown in Figure 10.  In the kernel, each thread loads only one row at a time.  Since each row contains 68 bytes (4 bytes for scales and 64 bytes for data), it cannot guarantee that every row aligns with a size of any vector type.  Thus, vector loads cannot be used for loading the KV cache.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg10.jpg&quot; alt=&quot;Figure 10: The layout of each row of INT4 KV cache with row-wise quantization&quot; style=&quot;width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10&lt;/strong&gt; The layout of each row of INT4 KV cache with row-wise quantization&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We have implemented support for group-wise INT4 quantization with num groups = 4.  In this case, columns in each row in the KV cache tensor are divided into 4 equal groups.  Columns within the same group share the same scales for quantization/dequantization.  The data layout for INT4 KV cache is shown in Figure 11.   The scales for all groups are serialized and stored at the beginning of each row.  The INT4 data is also serialized and laid out next to the scales.&lt;/p&gt;

&lt;p&gt;Because the number of bytes in each row now becomes 80 bytes, we can use a vector type, i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint2&lt;/code&gt; in our case, to load data.  (We &lt;strong&gt;do not&lt;/strong&gt; use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint4&lt;/code&gt; since each thread loads only 16 INT4s at a time due to the tensor core fragment size.)  Vector load is generally better than scalar load since it does not cause extra byte loads.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg11.jpg&quot; alt=&quot;Figure 11: The layout of each row of INT4 KV cache with row-wise quantization&quot; style=&quot;width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11&lt;/strong&gt; The layout of each row of INT4 KV cache with row-wise quantization&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 8&lt;/strong&gt; Performance of Optimization 6 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;111
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;322
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.23&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.29&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;192
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;372
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.22&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.34&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;346
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;414
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.32&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;642
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;446
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.27&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.35&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1244
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;460
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.27&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.33&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 9&lt;/strong&gt; Performance of Optimization 6 for INT4 GQA (group-wise quantization with num groups = 4)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;325
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;1.31
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;219
   &lt;/td&gt;
   &lt;td&gt;195
   &lt;/td&gt;
   &lt;td&gt;385
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;1.36
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;392
   &lt;/td&gt;
   &lt;td&gt;347
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;1.39
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;719
   &lt;/td&gt;
   &lt;td&gt;638
   &lt;/td&gt;
   &lt;td&gt;468
   &lt;/td&gt;
   &lt;td&gt;527
   &lt;/td&gt;
   &lt;td&gt;1.41
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1375
   &lt;/td&gt;
   &lt;td&gt;1225
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;550
   &lt;/td&gt;
   &lt;td&gt;1.43
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-7-compute-max-qkt-from-wmma-fragment-directly-a100h100-specific&quot;&gt;Optimization 7: Compute max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; From WMMA Fragment Directly (A100/H100 specific)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We observe large stalls due to shared memory accessing during the max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation (showing as large short scoreboard stalls) as shown in Figure 12.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg12.png&quot; alt=&quot;Figure 12: Stalls due to shared memory access during max QKT computation&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 12&lt;/strong&gt; Stalls due to shared memory access during max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation (the number that the arrow points to is stall cycles caused by shared memory wait)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We bypass shared memory when computing max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; by computing it from the WMMA fragment (i.e., the tensor core fragment) directly.  The layout of the WMMA fragment is specific to the GPU architecture.  In this optimization, we only enabled this optimization for the NVIDIA A100/H100 GPUs. Other GPUs will still use shared memory for the max &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; computation. By bypassing shared memory, we effectively eliminate the stalls caused by shared memory access.  The tensor core layout of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; fragment which is used for storing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; results is shown in Figure 13.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg13.jpg&quot; alt=&quot;Figure 13: C fragment (QKT storage) tensor core layout on A100/H100&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 13&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; fragment (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; storage) tensor core layout on A100/H100&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 10&lt;/strong&gt; Performance of Optimization 7 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 7&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 7&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;107
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;333
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.27&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.33&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;183
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;391
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.28&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.40&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;333
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;430
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.30&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.37&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;620
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;461
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.31&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.40&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1206
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;475
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.31&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.38&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 11&lt;/strong&gt; Performance of Optimization 7 for INT4 GQA (group-wise quantization with num groups = 4)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CUDA_WMMA Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 7&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 7&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;111
   &lt;/td&gt;
   &lt;td&gt;325
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;380
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.04&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;219
   &lt;/td&gt;
   &lt;td&gt;195
   &lt;/td&gt;
   &lt;td&gt;187
   &lt;/td&gt;
   &lt;td&gt;385
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;449
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.04&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;392
   &lt;/td&gt;
   &lt;td&gt;347
   &lt;/td&gt;
   &lt;td&gt;333
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;506
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.18&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.04&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;719
   &lt;/td&gt;
   &lt;td&gt;638
   &lt;/td&gt;
   &lt;td&gt;615
   &lt;/td&gt;
   &lt;td&gt;468
   &lt;/td&gt;
   &lt;td&gt;527
   &lt;/td&gt;
   &lt;td&gt;547
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.04&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1375
   &lt;/td&gt;
   &lt;td&gt;1225
   &lt;/td&gt;
   &lt;td&gt;1184
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;550
   &lt;/td&gt;
   &lt;td&gt;569
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.16&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.03&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-8-write-fp32-bf16-results-to-p-fragment-directly-a100h100-specific&quot;&gt;Optimization 8: Write FP32-&amp;gt;BF16 Results to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; Fragment Directly (A100/H100 specific)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;During the FP32-BF16 conversion for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment, the kernel loads the FP32 data from shared memory, does the conversion and then stores the BF16 data back to shared memory.  Moreover, the conversion requires many thread block synchronizations (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__syncthreads()&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Due to the data partitioning design of the kernel, each warp performs only one pass through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment.  Thus, we do not have to write the conversion results back to the shared memory for future usage.  To avoid writing the BF16 data to the shared memory and thread block synchronizations, we have each warp load the FP32 data of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; WMMA fragment from the shared memory, do the conversion and then write the BF16 data directly to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment.&lt;/p&gt;

&lt;p&gt;Note that this optimization is applied to only the NVIDIA A100 and H100 GPUs because the WMMA fragment layout is architecture dependent.  For non-A100/H100 GPUs, the kernel will fallback to the original path.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment tensor core layout is shown in Figure 14.  Note that this layout is specific to the NVIDIA A100/H100 GPU.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg14.jpg&quot; alt=&quot;Figure 14: P fragment tensor core layout on A100/H100&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 14&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment tensor core layout on A100/H100&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 12&lt;/strong&gt; Performance of Optimization 8 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 8&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;101
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;353
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.35&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.41&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;174
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;410
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.34&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.47&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;317
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;451
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.36&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.43&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;590
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;485
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.38&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.47&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1143
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;501
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.38&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.45&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 13&lt;/strong&gt; Performance of Optimization 8 for INT4 GQA (group-wise quantization with num groups = 4)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CUDA_WMMA Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 8&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 8&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;106
   &lt;/td&gt;
   &lt;td&gt;325
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;396
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.22&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;219
   &lt;/td&gt;
   &lt;td&gt;195
   &lt;/td&gt;
   &lt;td&gt;180
   &lt;/td&gt;
   &lt;td&gt;385
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;467
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.08&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;392
   &lt;/td&gt;
   &lt;td&gt;347
   &lt;/td&gt;
   &lt;td&gt;319
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;528
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.23&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.09&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;719
   &lt;/td&gt;
   &lt;td&gt;638
   &lt;/td&gt;
   &lt;td&gt;596
   &lt;/td&gt;
   &lt;td&gt;468
   &lt;/td&gt;
   &lt;td&gt;527
   &lt;/td&gt;
   &lt;td&gt;565
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.07&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1375
   &lt;/td&gt;
   &lt;td&gt;1225
   &lt;/td&gt;
   &lt;td&gt;1138
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;550
   &lt;/td&gt;
   &lt;td&gt;591
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.08&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-9-swizzle-p-shared-memory-layouts-a100h100-specific&quot;&gt;Optimization 9: Swizzle P Shared Memory Layouts (A100/H100 specific)&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We observe large shared memory bank conflicts during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; loading.  The amount of bank conflict depends on the memory access stride.  For instance, for split-Ks = 32 and max seq length = 8192, we observed that only 4 out of 32 banks are being accessed in parallel (memory access stride = 256).  From Figure 14, when all threads access element 0, threads that have the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;threadIdx.x % 4&lt;/code&gt; access the same bank.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg15.jpg&quot; alt=&quot;Figure 15: P fragment in shared memory before swizzling&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 15&lt;/strong&gt; P fragment in shared memory before swizzling&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We shuffle the layout of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; load/store in the shared memory in such a way that avoids bank conflicts.  In other words, we store the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QK&lt;sup&gt;T&lt;/sup&gt;&lt;/code&gt; results (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; fragment) and load them (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment) using the swizzled layout.  Moreover, instead of using the original memory access stride which is dependent on the number of tokens per thread block, we use the fragment’s column size as the stride which is constant.  Thus, the load and store of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P&lt;/code&gt; fragment is always contiguous.&lt;/p&gt;

&lt;p&gt;The new layouts for the C and P fragments are shown in Figure 16.  With the new layout, it is guaranteed that 16 banks are being accessed in parallel as shown in Figure 17.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg16.jpg&quot; alt=&quot;Figure 16: The swizzled layouts of C and P fragments&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 16&lt;/strong&gt; The swizzled layouts of C and P fragments&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg17.jpg&quot; alt=&quot;Figure 17: P fragment in shared memory after swizzling&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 17&lt;/strong&gt; P fragment in shared memory after swizzling&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 14&lt;/strong&gt; Performance of Optimization 9 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 9&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 9&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;98
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;365
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.39&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.46&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;167
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.41&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.54&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;299
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;479
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.45&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.52&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;549
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;521
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.48&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.58&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;1060
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;540
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.49&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.56&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 15&lt;/strong&gt; Performance of Optimization 9 for INT4 GQA (group-wise quantization with num groups = 4)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CUDA_WMMA Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 9&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 9&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;105
   &lt;/td&gt;
   &lt;td&gt;325
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;400
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.23&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.10&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;219
   &lt;/td&gt;
   &lt;td&gt;195
   &lt;/td&gt;
   &lt;td&gt;174
   &lt;/td&gt;
   &lt;td&gt;385
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.26&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.12&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;392
   &lt;/td&gt;
   &lt;td&gt;347
   &lt;/td&gt;
   &lt;td&gt;302
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;558
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.30&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.15&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;719
   &lt;/td&gt;
   &lt;td&gt;638
   &lt;/td&gt;
   &lt;td&gt;560
   &lt;/td&gt;
   &lt;td&gt;468
   &lt;/td&gt;
   &lt;td&gt;527
   &lt;/td&gt;
   &lt;td&gt;601
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.28&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.14&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1375
   &lt;/td&gt;
   &lt;td&gt;1225
   &lt;/td&gt;
   &lt;td&gt;1065
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;550
   &lt;/td&gt;
   &lt;td&gt;632
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.29&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.15&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;optimization-10-pad-shared-memory-for-int4-dequantization&quot;&gt;Optimization 10: Pad Shared Memory for INT4 Dequantization&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Problem Analysis:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once the kernel reads the INT4 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; cache from global memory, it performs dequantization and stores the results (BF16) in the shared memory.  Then, the BF16 data is loaded to the WMMA fragment from shared memory (via the WMMA interface).  We observed a large number of bank conflicts for both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; accesses.  For instance, for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; stores, only 4 out of 32 banks are being accessed in parallel.  For &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loads, 16 banks are being accessed in parallel.  The same also occurs for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; stores and loads.  See the figures in the solution section.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We pad the shared memory to reduce the bank conflict.  Specifically, we pad each row by 2.  That is, the row stride of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; becomes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F_K&lt;/code&gt; + 2 and the row stride of V becomes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F_N&lt;/code&gt; + 2 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F_K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;F_N&lt;/code&gt; are the fixed widths of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;V&lt;/code&gt; WMMA fragments, respectively).  With this optimization, we are able to reduce the bank conflict by 1.8x as shown in Figure 18.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg18.png&quot; alt=&quot;Figure 18: Bank conflicts before and after Optimization 10&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 18&lt;/strong&gt; Bank conflicts before and after Optimization 10&lt;/p&gt;

&lt;p&gt;After Optimization 10, for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; stores, 32 banks are being accessed in parallel (shown in Figure 19), while for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; loads, 29 banks are accessed in parallel (shown in Figure 20).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg19.jpg&quot; alt=&quot;Figure 19: K fragment store shared memory layout without and with padding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 19&lt;/strong&gt; K fragment store shared memory layout without and with padding&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg20.jpg&quot; alt=&quot;Figure 20: K fragment load shared memory layout without and with padding&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 20&lt;/strong&gt; K fragment load shared memory layout without and with padding&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 16&lt;/strong&gt; Performance of Optimization 10 for INT4 GQA (row-wise quantization)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CU&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CU baseline&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 10&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;94
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;380
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.45&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.52&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;151
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;475
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.55&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.71&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;266
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;538
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.63&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.71&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;586
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.67&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.77&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;930
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;616
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.70&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.79&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Table 17&lt;/strong&gt; Performance of Optimization 10 for INT4 GQA (group-wise quantization with num groups = 4)&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;3&quot;&gt;&lt;strong&gt;Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Speed up&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;CUDA_WMMA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td rowspan=&quot;2&quot;&gt;&lt;strong&gt;vs CUDA_WMMA Opt 6&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 10&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 6&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Opt 10&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;129
   &lt;/td&gt;
   &lt;td&gt;116
   &lt;/td&gt;
   &lt;td&gt;99
   &lt;/td&gt;
   &lt;td&gt;325
   &lt;/td&gt;
   &lt;td&gt;364
   &lt;/td&gt;
   &lt;td&gt;425
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.31&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.17&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;219
   &lt;/td&gt;
   &lt;td&gt;195
   &lt;/td&gt;
   &lt;td&gt;161
   &lt;/td&gt;
   &lt;td&gt;385
   &lt;/td&gt;
   &lt;td&gt;431
   &lt;/td&gt;
   &lt;td&gt;523
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.36&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.21&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;392
   &lt;/td&gt;
   &lt;td&gt;347
   &lt;/td&gt;
   &lt;td&gt;282
   &lt;/td&gt;
   &lt;td&gt;429
   &lt;/td&gt;
   &lt;td&gt;484
   &lt;/td&gt;
   &lt;td&gt;598
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.39&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.23&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;719
   &lt;/td&gt;
   &lt;td&gt;638
   &lt;/td&gt;
   &lt;td&gt;509
   &lt;/td&gt;
   &lt;td&gt;468
   &lt;/td&gt;
   &lt;td&gt;527
   &lt;/td&gt;
   &lt;td&gt;662
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.41&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.25&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1375
   &lt;/td&gt;
   &lt;td&gt;1225
   &lt;/td&gt;
   &lt;td&gt;965
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
   &lt;td&gt;550
   &lt;/td&gt;
   &lt;td&gt;698
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.43&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;1.27&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performance-evaluation&quot;&gt;Performance Evaluation&lt;/h2&gt;

&lt;h3 id=&quot;microbenchmark-results&quot;&gt;Microbenchmark results&lt;/h3&gt;

&lt;p&gt;We also evaluated BF16 GQA performance using our optimized kernel (as shown in Table 19).  CU still performs generally worse than FD and FA for BF16.  This is expected since our optimizations are INT4 focused.&lt;/p&gt;

&lt;p&gt;While INT4 GQA is still not as efficient as BF16 GQA (see the achieved bandwidths), it is important to note that when comparing FD BF16 GQA performance against CU INT4 GQA performance, &lt;strong&gt;we can see that the latency of INT4 is smaller than that of BF16&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 19&lt;/strong&gt; Performance of BF16 GQA and INT GQA after CU optimizations&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;On A100&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;139
   &lt;/td&gt;
   &lt;td&gt;133
   &lt;/td&gt;
   &lt;td&gt;183
   &lt;/td&gt;
   &lt;td&gt;163
   &lt;/td&gt;
   &lt;td&gt;137
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;143
   &lt;/td&gt;
   &lt;td&gt;94
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;245
   &lt;/td&gt;
   &lt;td&gt;229
   &lt;/td&gt;
   &lt;td&gt;335
   &lt;/td&gt;
   &lt;td&gt;276
   &lt;/td&gt;
   &lt;td&gt;234
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;257
   &lt;/td&gt;
   &lt;td&gt;151
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;433
   &lt;/td&gt;
   &lt;td&gt;555
   &lt;/td&gt;
   &lt;td&gt;596
   &lt;/td&gt;
   &lt;td&gt;517
   &lt;/td&gt;
   &lt;td&gt;432
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;455
   &lt;/td&gt;
   &lt;td&gt;266
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;826
   &lt;/td&gt;
   &lt;td&gt;977
   &lt;/td&gt;
   &lt;td&gt;1127
   &lt;/td&gt;
   &lt;td&gt;999
   &lt;/td&gt;
   &lt;td&gt;815
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;866
   &lt;/td&gt;
   &lt;td&gt;489
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1607
   &lt;/td&gt;
   &lt;td&gt;1670
   &lt;/td&gt;
   &lt;td&gt;2194
   &lt;/td&gt;
   &lt;td&gt;1879
   &lt;/td&gt;
   &lt;td&gt;1581
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;1659
   &lt;/td&gt;
   &lt;td&gt;930
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Effective Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;965
   &lt;/td&gt;
   &lt;td&gt;1012
   &lt;/td&gt;
   &lt;td&gt;736
   &lt;/td&gt;
   &lt;td&gt;824
   &lt;/td&gt;
   &lt;td&gt;262
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;250
   &lt;/td&gt;
   &lt;td&gt;380
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;1097
   &lt;/td&gt;
   &lt;td&gt;1175
   &lt;/td&gt;
   &lt;td&gt;802
   &lt;/td&gt;
   &lt;td&gt;972
   &lt;/td&gt;
   &lt;td&gt;305
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;278
   &lt;/td&gt;
   &lt;td&gt;475
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;1240
   &lt;/td&gt;
   &lt;td&gt;968
   &lt;/td&gt;
   &lt;td&gt;901
   &lt;/td&gt;
   &lt;td&gt;1039
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;314
   &lt;/td&gt;
   &lt;td&gt;538
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;1301
   &lt;/td&gt;
   &lt;td&gt;1100
   &lt;/td&gt;
   &lt;td&gt;954
   &lt;/td&gt;
   &lt;td&gt;1075
   &lt;/td&gt;
   &lt;td&gt;351
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;331
   &lt;/td&gt;
   &lt;td&gt;586
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1338
   &lt;/td&gt;
   &lt;td&gt;1287
   &lt;/td&gt;
   &lt;td&gt;980
   &lt;/td&gt;
   &lt;td&gt;1144
   &lt;/td&gt;
   &lt;td&gt;362
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;345
   &lt;/td&gt;
   &lt;td&gt;616
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;On H100&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Time (us)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;91
   &lt;/td&gt;
   &lt;td&gt;90
   &lt;/td&gt;
   &lt;td&gt;114
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;70
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;96
   &lt;/td&gt;
   &lt;td&gt;64
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;148
   &lt;/td&gt;
   &lt;td&gt;146
   &lt;/td&gt;
   &lt;td&gt;200
   &lt;/td&gt;
   &lt;td&gt;183
   &lt;/td&gt;
   &lt;td&gt;113
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;162
   &lt;/td&gt;
   &lt;td&gt;101
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;271
   &lt;/td&gt;
   &lt;td&gt;298
   &lt;/td&gt;
   &lt;td&gt;361
   &lt;/td&gt;
   &lt;td&gt;308
   &lt;/td&gt;
   &lt;td&gt;205
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;294
   &lt;/td&gt;
   &lt;td&gt;170
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;515
   &lt;/td&gt;
   &lt;td&gt;499
   &lt;/td&gt;
   &lt;td&gt;658
   &lt;/td&gt;
   &lt;td&gt;556
   &lt;/td&gt;
   &lt;td&gt;389
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;558
   &lt;/td&gt;
   &lt;td&gt;306
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;1000
   &lt;/td&gt;
   &lt;td&gt;1011
   &lt;/td&gt;
   &lt;td&gt;1260
   &lt;/td&gt;
   &lt;td&gt;1066
   &lt;/td&gt;
   &lt;td&gt;756
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;1066
   &lt;/td&gt;
   &lt;td&gt;575
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Effective Bandwidth (GB/s)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;BF16 GQA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td colspan=&quot;4&quot;&gt;&lt;strong&gt;INT4 GQA&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FD&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;FA&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU before&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;CU after&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;1481
   &lt;/td&gt;
   &lt;td&gt;1496
   &lt;/td&gt;
   &lt;td&gt;1178
   &lt;/td&gt;
   &lt;td&gt;1341
   &lt;/td&gt;
   &lt;td&gt;511
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;371
   &lt;/td&gt;
   &lt;td&gt;560
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;1815
   &lt;/td&gt;
   &lt;td&gt;1840
   &lt;/td&gt;
   &lt;td&gt;1345
   &lt;/td&gt;
   &lt;td&gt;1470
   &lt;/td&gt;
   &lt;td&gt;631
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;443
   &lt;/td&gt;
   &lt;td&gt;710
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;1982
   &lt;/td&gt;
   &lt;td&gt;1802
   &lt;/td&gt;
   &lt;td&gt;1487
   &lt;/td&gt;
   &lt;td&gt;1743
   &lt;/td&gt;
   &lt;td&gt;699
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;487
   &lt;/td&gt;
   &lt;td&gt;844
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;256
   &lt;/td&gt;
   &lt;td&gt;2087
   &lt;/td&gt;
   &lt;td&gt;2156
   &lt;/td&gt;
   &lt;td&gt;1634
   &lt;/td&gt;
   &lt;td&gt;1934
   &lt;/td&gt;
   &lt;td&gt;736
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;513
   &lt;/td&gt;
   &lt;td&gt;935
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;512
   &lt;/td&gt;
   &lt;td&gt;2150
   &lt;/td&gt;
   &lt;td&gt;2127
   &lt;/td&gt;
   &lt;td&gt;1706
   &lt;/td&gt;
   &lt;td&gt;2015
   &lt;/td&gt;
   &lt;td&gt;757
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;537
   &lt;/td&gt;
   &lt;td&gt;996
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h3 id=&quot;e2e-results&quot;&gt;E2E results&lt;/h3&gt;

&lt;p&gt;We evaluated our optimized INT4 GQA kernel in Llama 2 70B on 8 H100 GPUs. We ran the model end-to-end, but only reported the decode latency.  We use FP8 FFN (feed forward network) to emphasize the attention performance in the decoding phase.  We vary the batch size from 1 to 256 and the context length from 2,048 (2K) to 16,384 (16K).  The E2E performance results are shown in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/int4-decoding/fg21.png&quot; alt=&quot;Figure 21: Meta Llama 2 decode latency (ms) comparison&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 21&lt;/strong&gt; Meta Llama 2 decode latency (ms) comparison (BF16 GQA runs out of memory in large batch size configurations)&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;If you are interested, please checkout our code &lt;a href=&quot;https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu/experimental/gen_ai&quot;&gt;here&lt;/a&gt;.  If you have any questions, please feel free to open an issue on GitHub, and we will be happy to help.  Your contributions are welcome!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sarunya Pumma, Jongsoo Park, Jianyu Huang, Amy Yang, Jaewon Lee, Daniel Haziza, Grigory Sizov, Jeremy Reizenstein, Jeff Johnson, Ying Zhang</name>
        
        
      </author>

      

      

      
        <summary type="html">An efficient decoding Grouped-Query Attention with low-precision KV cache</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Ready, Set, Contribute: PyTorch Docathon Kickoff H1 2024</title>
      <link href="https://pytorch.org/blog/docathon-kickoff-h1-2024/" rel="alternate" type="text/html" title="Ready, Set, Contribute: PyTorch Docathon Kickoff H1 2024" />
      <published>2024-06-04T00:00:00-07:00</published>
      <updated>2024-06-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/docathon-kickoff-h1-2024</id>
      <content type="html" xml:base="https://pytorch.org/blog/docathon-kickoff-h1-2024/">&lt;p&gt;The PyTorch Docathon is now live! This event is dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Our hope with this Docathon is to simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://discord.com/events/878249534336167987/1245440397510180907&quot; target=&quot;_blank&quot;&gt;JOIN THE KICK-OFF EVENT&lt;/a&gt;&lt;br /&gt;
on June 4th at 10 AM PT&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;June 4: Kick-off - join a 30-minutes livestream kick off event on Discord on June 4th at 10 AM PT  &lt;a href=&quot;https://discord.com/events/878249534336167987/1245440397510180907&quot;&gt;here&lt;/a&gt;. If you can’t join the kick-off event, watch our &lt;a href=&quot;https://youtu.be/2D0aej50umA&quot;&gt;welcome video&lt;/a&gt; on YouTube&lt;/li&gt;
  &lt;li&gt;June 4-June 16:  Submissions and Feedback&lt;/li&gt;
  &lt;li&gt;June 17-18: Final Reviews&lt;/li&gt;
  &lt;li&gt;June 20: Winner Announcements&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-contribute&quot;&gt;How to Contribute&lt;/h2&gt;

&lt;p&gt;Review the Docathon H1 2024 issue in the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/127345&quot;&gt;pytorch/pytorch&lt;/a&gt; or&lt;a href=&quot;https://github.com/pytorch/tutorials/issues/2894&quot;&gt; pytorch/tutorials &lt;/a&gt;repo that contain all the necessary information on participating in the Docathon and highlights the specific issues to work on. Remember to sign the CLA in your first PR and adhere to the Code of Conduct guidelines.&lt;/p&gt;

&lt;h2 id=&quot;read-the-code-of-conduct&quot;&gt;Read the Code of Conduct&lt;/h2&gt;

&lt;p&gt;Take a moment to review the PyTorch code of conduct found &lt;a href=&quot;https://docs.github.com/en/site-policy/github-terms/github-community-code-of-conduct&quot;&gt;here&lt;/a&gt;. This document outlines the expectations for behavior and communication within our team, and it is important that everyone is aware of and adheres to these guidelines.&lt;/p&gt;

&lt;h2 id=&quot;join-our-discord&quot;&gt;Join our Discord&lt;/h2&gt;

&lt;p&gt;This channel serves as the main communication hub during the Docathon. You can join it using by using this link:&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;a href=&quot;https://discord.gg/NVYWkYQ6p5&quot; target=&quot;_blank&quot;&gt;JOIN DISCORD SERVER&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When you first join the server, you will have limited access. To gain full access to our Discord PyTorch Docathon Channel:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Enter the server and navigate to the #self-roles channel.&lt;/li&gt;
  &lt;li&gt;In the #self-roles channel, click on the ‘Join Docathon’ button in the relevant post to assign yourself the docathon role.&lt;/li&gt;
  &lt;li&gt;After assigning the role, you will see the ‘PyTorch Docathon H1 2024 Section’ in the left-hand menu for discussions.&lt;/li&gt;
  &lt;li&gt;To help prevent spam we are asking that you change your server username to your GitHub username or the email username you registered with.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;explore-the-github-issues&quot;&gt;Explore the GitHub Issues&lt;/h2&gt;

&lt;p&gt;All the Docathon issues are posted on GitHub. You can find them by the docathon-h1-2024 label in the following participating repositories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/pytorch/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3Adocathon-h1-2024&quot;&gt;pytorch/pytorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/tutorials/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3Adocathon-h1-2024&quot;&gt;pytorch/tutorials&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/xla/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3A%22docathon-h1-2024%22&quot;&gt;pytorch/xla&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch-labs/torchfix/issues?q=is%3Aissue+is%3Aopen+sort%3Aupdated-desc+label%3Adocathon-h1-2024&quot;&gt;pytorch-labs/torchfix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The issues are categorized into three levels of difficulty: easy, medium, and advanced. If this is your first time contributing to PyTorch, we recommend starting with an issue at the easy level.&lt;/p&gt;

&lt;h2 id=&quot;prizes-for-winners&quot;&gt;Prizes for Winners&lt;/h2&gt;

&lt;p&gt;We will have a leaderboard throughout the duration of the Docathon. The more you contribute, the higher you’ll get on the board! Our top three winners will get free admission to &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference 2024&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;thank-you-to-our-partners&quot;&gt;Thank you to our Partners&lt;/h2&gt;

&lt;p&gt;This year, we’re thrilled to work with the PyTorch Teams at Meta, Google and Snowflake to help us put on a successful event. We’ll also be at &lt;a href=&quot;https://u20934166.ct.sendgrid.net/ls/click?upn=u001.I0np5LuiPyKW16wNY408Oo-2Bbsl8LTTFpumobGm2Rp-2F8zbPScG1hdhIF5oBHMWg96q8d7q31-2B1wRrzDpeTgLDCD6FBreEsZKA-2Fi2T-2BA2upbvMD1hmAnpEutE9KOQuxVZnSlQQ1xyyTXB16qRa8VpgLD4ScqnpqqdSffoL4rKHjpyoVMVMoxCsEuKcHJfDWGiMECgZDoegcf9j7bnJPdx0qXN4phU37F77vSXetvmrfy1t6YtpSvgX2eACaznmpmFNfyRtlDRsfdY8YgoZN3rkCMoL0bwCFXdt-2FvIZawhnUV2qppyS0ZgmSCGl-2BufBL0mn8HJIew4LxuXuUB3gfsWD1K6Hb2oiECdgEL-2BK-2BZ6OVUBbdfsupYb6tlGTA2Ic6jTTOODdc5-2B1RKMxIBw1-2FcC8Dzd4uxjdIqgrF4vQ0zdGsVXr2rsWISdAzRwwZ6HxvPYiL8vq_eKG3f-2BVkGDs-2F-2BQAgHZwkvtAslantAPMulCPu-2FMezvzmhqnecR6Zi4bBfzHqa-2BAIYrvYiOg1COidzFz394ty6L-2FhCaPa85b-2BvNqP8-2BnHbrX2cxmOcMxJolrQzGKT9AZVkURRXc3AUORnPJRfwJ32v8Dp-2Fpx9b2Kf973NO10Vsxu9GvjAtGFxlGl-2Bgjhs0tu8Jlhj-2BCG2lXGsFGXnxI1t1hw-3D-3D&quot;&gt;Snowflake Dev Day&lt;/a&gt; on June 6 where you can hear from Meta’s Matthias Reso, and check out our PyTorch booth.&lt;/p&gt;

&lt;p&gt;Happy contributing!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Docathon is now live! This event is dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Our hope with this Docathon is to simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Maximizing Training Throughput Using PyTorch FSDP and Torch.compile</title>
      <link href="https://pytorch.org/blog/maximizing-training-throughput/" rel="alternate" type="text/html" title="Maximizing Training Throughput Using PyTorch FSDP and Torch.compile" />
      <published>2024-05-21T00:00:00-07:00</published>
      <updated>2024-05-21T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/maximizing-training-throughput</id>
      <content type="html" xml:base="https://pytorch.org/blog/maximizing-training-throughput/">&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/maximizing-training/&quot;&gt;Recently&lt;/a&gt;, we demonstrated how FSDP and selective activation checkpointing can be used to achieve &lt;strong&gt;57% MFU (Model Flops Utilization)&lt;/strong&gt; for training a 7B model on A100 GPUs. We also demonstrated how it can train a high quality model, which we open sourced as &lt;a href=&quot;https://huggingface.co/ibm/granite-7b-base&quot;&gt;Granite 7B base model&lt;/a&gt; on Hugging Face Hub under the Apache v2.0 license.&lt;/p&gt;

&lt;p&gt;We continued our quest to improve  the utilization of GPUs by leveraging torch.compile. Using torch.compile and the selective activation checkpointing from our previous work, we achieve a &lt;strong&gt;MFU of 68%&lt;/strong&gt; for the 7B model on A100 GPUs! torch.compile improves training MFU between 10% and 23% for various model sizes.&lt;/p&gt;

&lt;p&gt;This blog is organized into three parts: (1) Challenges addressed in order to train using torch.compile, (2) Numerical parity of compile with no-compile, and (3) MFU report.&lt;/p&gt;

&lt;p&gt;We open sourced all the code and updated it in the &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;fms-fsdp repository.&lt;/a&gt; We are also working with Team PyTorch at Meta to contribute these to the newly released &lt;a href=&quot;https://github.com/pytorch/torchtitan&quot;&gt;torch titan&lt;/a&gt; repository for pre-training.&lt;/p&gt;

&lt;h2 id=&quot;challenges-of-using-torchcompile&quot;&gt;Challenges of using torch.compile&lt;/h2&gt;

&lt;p&gt;torch.compile is a graph compilation technique that improves GPU utilization. For details on how torch compile works, we refer the readers to the recent &lt;a href=&quot;https://pytorch.org/blog/pytorch-2-paper-tutorial/&quot;&gt;PyTorch paper&lt;/a&gt; and associated tutorials. A key challenge in getting torch.compile to perform well is to minimize (or eliminate) graph breaks. We initially started with the Llama implementation provided by Meta, but compiling it caused too many  graph breaks resulting in reduced  training throughput.&lt;/p&gt;

&lt;p&gt;Several portions of the model architecture had to be fixed, with the most important one being the positional embedding layer (RoPE). The typical RoPE implementation uses complex numbers, which was not supported in torch.compile at the time of testing. We implemented RoPE using einops while maintaining parity with the original model architecture implementation. We had to properly cache the frequencies so that we did not run into graph breaks within the RoPE implementation.&lt;/p&gt;

&lt;p&gt;Compiling an FSDP model does result in graph breaks, which the PyTorch team at Meta is working to remove. However, these graph breaks as of PyTorch 2.3 are at FSDP unit boundaries and do not affect throughput significantly.&lt;/p&gt;

&lt;p&gt;When using custom kernels, we need to wrap each kernel by exposing its API to torch.compile.  This involves indicating what parameters are modified in-place, how they are modified, and what shapes and strides will their return values have based on the inputs. In our case, SDPA Flash attention is already integrated appropriately and we were able to get that kernel to work with torch.compile with no graph breaks.&lt;/p&gt;

&lt;p&gt;We also noticed that when  increasing the amount of data from 2T to 6T tokens, the data loader became a bottleneck. A key reason for this is the fact that previously, we implemented document shuffling in our dataloader naively, by having each worker maintain a list of shuffled document pointers.&lt;/p&gt;

&lt;p&gt;With the larger dataset, these pointer lists were growing to hundreds of thousands of entries per worker. Maintaining pointer lists at this scale became expensive enough that cpu contention throttled our training throughput. We re-implemented document shuffling without any pointer lists using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_congruential_generator&quot;&gt;Linear Congruential Generator&lt;/a&gt;. LCG is a pseudorandom number generator algorithm that implements a random walk over a population, providing sampling without replacement.&lt;/p&gt;

&lt;p&gt;We leveraged the same idea to produce implicit bijective mappings from ordered to shuffled document indices.  This enables us to shrink those annoying lists of hundreds of thousands of pointers down to a single integer state for the LCG. This eliminated 80% of the bottleneck and provided a significant boost to our performance. We will devote a separate blog to go into all the details of our performant pre-training data loader.&lt;/p&gt;

&lt;h2 id=&quot;numerical-parity-of-torchcompile-and-torchno-compile&quot;&gt;Numerical Parity of torch.compile and torch.no-compile&lt;/h2&gt;

&lt;p&gt;We had previously observed parity issues when training with compile and no-compile options, with one of these being related to the use of SDPA. After a few days of intense debugging sessions between the PyTorch teams at Meta and IBM, we were able to achieve parity between PyTorch compile and no-compile modes. To document and verify this parity, we take a mini-Llama model architecture of 1.4B size and train it to 100B tokens in four variations – no-compile, compile with no activation checkpointing, compile with selective activation checkpointing, and compile with full activation checkpointing.&lt;/p&gt;

&lt;p&gt;We plot the loss curves and gradient norm for these options below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/max-training-chart.jpg&quot; alt=&quot;Figure 1: Loss curve and gradient norm for various compile options&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Loss curve and gradient norm for various compile options&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Further, we run the lm-evaluation-harness and compare the various model scores on different benchmarks and observe no major differences between compile and no-compile, which is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/max-training-table.png&quot; alt=&quot;Figure 2: lm-evaluation-harness comparison of various benchmarks between compile and no-compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: lm-evaluation-harness comparison of various benchmarks between compile and no-compile&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe from all these results that compile with all its variants is equal to no-compile option, thus demonstrating parity between compile and no-compile.&lt;/p&gt;

&lt;h2 id=&quot;mfu-report&quot;&gt;MFU report&lt;/h2&gt;

&lt;p&gt;Finally, like our previous blog, we compute the MFU for four different model sizes on two clusters.  One cluster is 128 A100 GPUs with 400 Gbps inter-node connectivity,  and the other is 464 H100 GPUs with 3.2 Tbps inter-node connectivity. We use the selective activation checkpointing that we covered &lt;a href=&quot;https://pytorch.org/blog/maximizing-training/&quot;&gt;in the prior blog&lt;/a&gt; in addition to compile. We capture the results in the table below.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;MFU no-compile&lt;/strong&gt; 
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;MFU compile&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percentage gain (%)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;7B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.57
   &lt;/td&gt;
   &lt;td&gt;0.68
   &lt;/td&gt;
   &lt;td&gt;20
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;13B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.51
   &lt;/td&gt;
   &lt;td&gt;0.60
   &lt;/td&gt;
   &lt;td&gt;17
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;34B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;0.54
   &lt;/td&gt;
   &lt;td&gt;15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.50
   &lt;/td&gt;
   &lt;td&gt;0.55
   &lt;/td&gt;
   &lt;td&gt;10
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: MFU results with compile and no compile for Llama2 model architectures on 128 A100 80GB GPUs with 400Gbps internode interconnect&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered mt-5&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Batch size&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;MFU no-compile&lt;/strong&gt; 
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;MFU compile&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Percentage gain&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;7B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.37
   &lt;/td&gt;
   &lt;td&gt;0.45
   &lt;/td&gt;
   &lt;td&gt;21
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;13B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.35
   &lt;/td&gt;
   &lt;td&gt;0.43
   &lt;/td&gt;
   &lt;td&gt;23
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;34B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.32
   &lt;/td&gt;
   &lt;td&gt;0.38
   &lt;/td&gt;
   &lt;td&gt;19
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70B
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;0.32
   &lt;/td&gt;
   &lt;td&gt;0.38
   &lt;/td&gt;
   &lt;td&gt;19
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 2: MFU results with compile and no compile for Llama2 model architectures on 464 H100 80GB GPUs with 3.2Tbps internode interconnect&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We also had an internal production run on 448 GPUs using a Llama2 7B architecture. Using compile and selective activation checkpointing, with a global batch size of 3.7M, we trained for 4T tokens in 13 days 10 hours!&lt;/p&gt;

&lt;p&gt;During training, the data center cooling had to kick in with extra air conditioning and our training team was alerted to this, since we were using the GPUs quite effectively ☺&lt;/p&gt;

&lt;p&gt;One key observation from the tables 1 and 2 is that the MFU numbers do not linearly scale with model size. There are two possible explanations that we are actively investigating, one is the scalability of FSDP as model size increases and when tensor parallel needs to be enabled to more effectively use the GPU and the other is batch size, which can be increased further to get better MFU. We plan to explore FSDP v2 and selective operator checkpointing along with the tensor parallel feature to study the scaling laws of FSDP with model size.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;We plan to start testing FSDP v2 which will be released as part of PyTorch 2.4.  FSDP2 provides per parameter sharding and selective operator checkpointing feature that can potentially provide even better memory-compute tradeoffs.&lt;/p&gt;

&lt;p&gt;We have also been engaged with the PyTorch team at Meta to evaluate the new asynchronous checkpointing feature that can further improve the GPU utilization by reducing the time to write checkpoints.&lt;/p&gt;

&lt;p&gt;We are exploring extending various Triton kernels currently used in inference to perform backward operations to gain speedups beyond inference only.&lt;/p&gt;

&lt;p&gt;Finally, as recent work on use of fp8 is emerging, we plan to explore how we can even further accelerate model training using the new data type that promises a 2x acceleration.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;There are several teams that have been involved in reaching this proof point and we would like to thank the teams across Meta and IBM. Specifically, we extend our gratitude to the Meta PyTorch distributed and compiler teams and IBM Research.&lt;/p&gt;

&lt;p&gt;Multiple people were extensively involved in the effort of achieving torch.compile numerical parity with our models, and we wish to acknowledge the key folks involved in this effort; Animesh Jain and Less Wright at Meta, and Linsong Chu, Davis Wertheimer, Brian Vaughan, Antoni i Viros Martin, Mudhakar Srivatsa,  and Raghu Ganti at IBM Research.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/stasbekman/?originalSubdomain=ca&quot;&gt;Stas Bekman&lt;/a&gt;, who provided extensive feedback and helped improve this blog. Their insights have been invaluable in highlighting key aspects of optimizing the training and exploring further enhancements.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at IBM and Team PyTorch at Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">Recently, we demonstrated how FSDP and selective activation checkpointing can be used to achieve 57% MFU (Model Flops Utilization) for training a 7B model on A100 GPUs. We also demonstrated how it can train a high quality model, which we open sourced as Granite 7B base model on Hugging Face Hub under the Apache v2.0 license.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Achieving Sustainability Goals with PyTorch and Intel AI</title>
      <link href="https://pytorch.org/blog/achieving-sustainability-goals/" rel="alternate" type="text/html" title="Achieving Sustainability Goals with PyTorch and Intel AI" />
      <published>2024-05-15T00:00:00-07:00</published>
      <updated>2024-05-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/achieving-sustainability-goals</id>
      <content type="html" xml:base="https://pytorch.org/blog/achieving-sustainability-goals/">&lt;p&gt;This post was contributed by Intel AI in partnership with the PyTorch Foundation.&lt;/p&gt;

&lt;p&gt;In 2017, the UN Global Compact emphasized digital technology, particularly open source, as crucial for achieving Sustainable Development Goals (SDGs), projecting a potential $2.1 trillion boost to the tech sector by 2030. The SDGs, part of the “2030 Agenda for Sustainable Development,” address global prosperity across various sectors.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.linuxfoundation.org/projects/sustainability&quot;&gt;Linux Foundation’s Sustainability Initiative&lt;/a&gt; aligns projects with sustainable development goals. By assessing project impact, resources can be better allocated for enhancement. Intel is also a contributor to this initiative, and recently presented three use cases with PyTorch and Intel AI to address UN SDG-aligned issues.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/achieving-sustainability-goals.png&quot; alt=&quot;Sustainability Goals&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sdg-15-life-on-land&quot;&gt;SDG 15: Life on Land&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Using a bone likelihood map to pinpoint dinosaur bones, which paves the way for transfer learning to tackle contemporary challenges like wildfire prediction.&lt;/li&gt;
  &lt;li&gt;Employing transfer learning for wildfire prediction and generating data with Stable Diffusion.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sdg-9-industry--innovation-infrastructure&quot;&gt;SDG 9: Industry,  Innovation, Infrastructure&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Identifying crucial minerals, oil, and gas through subsurface models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here are the key highlights from the workshops. Read below for a summary, and be sure to watch the full workshop videos and visit the GitHub repositories.&lt;/p&gt;

&lt;h2 id=&quot;session-1--introduction-to-dinosaur-bone-bed-maps&quot;&gt;Session 1:  Introduction to Dinosaur Bone Bed Maps&lt;/h2&gt;

&lt;p&gt;Bob Chesebrough  recently  led a PyTorch workshop demonstrating how to create a dinosaur bone bed map for Dinosaur National Monument. He shared footage of his discoveries and explained his AI-driven approach, utilizing geological data to pinpoint possible bone-rich areas.&lt;/p&gt;

&lt;p&gt;Attendees learned to set up JupyterLab, access the training section, and launch a BASH shell. Bob’s classification model, applied to aerial images, facilitated heatmap generation to identify potential bone locations, refined through field data. The GitHub repo “Jurassic” guided participants through directory setup and model optimization steps.&lt;/p&gt;

&lt;p&gt;Rahul Unnikrishnan Nair demonstrated the use of PyTorch, focusing on performance enhancements. The workshop covered modeling best practices, such as data transformations, class distribution, dropout layers, and efficient training methods. Training and scoring procedures were examined, with a focus on model accuracy and transportability to other regions. Heatmap creation involved cutting images into tiles, considering context for accurate environmental identification.&lt;/p&gt;

&lt;p&gt;Watch the &lt;a href=&quot;https://www.youtube.com/watch?v=w4JmPkqnD0E&quot;&gt;full workshop video here &lt;/a&gt;and visit the &lt;a href=&quot;https://github.com/intelsoftware/jurassic&quot;&gt;GitHub repository &lt;/a&gt;to access the code sample and experiment with the code using &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/intel_extension_for_pytorch.html&quot;&gt;Intel ® Extension for PyTorch&lt;/a&gt;. Try it out with PyTorch and explore what works best for you. Happy dinosaur bone hunting!&lt;/p&gt;

&lt;h2 id=&quot;session-2-seismic-data-to-subsurface-models-with-openfwi-training-an-ai-model-with-pytorch&quot;&gt;Session 2: Seismic Data to Subsurface Models with OpenFWI: Training an AI Model with PyTorch&lt;/h2&gt;

&lt;p&gt;Seismic exploration is crucial for subsurface imaging in mineral and oil/gas exploration. Full waveform inversion (FWI) recreates subsurface sound wave velocities, akin to ultrasound for the Earth.&lt;/p&gt;

&lt;p&gt;Ben Consolvo, an AI Software Engineering Manager at Intel, presented training AI models directly from seismic data using PyTorch on Intel high-performance processors. FWI, though accurate, is computationally intensive and relies on precise initial models. AI models offer an alternative approach, learning directly from data without the need for precise initializations. Ben explained the challenges of AI models, highlighting the need for diverse datasets and the potential use of CPUs for fine-tuning. He also discussed FWI’s surprising medical applications.&lt;/p&gt;

&lt;p&gt;Watch the&lt;a href=&quot;https://www.youtube.com/watch?v=zvk3Rr-OjU0&quot;&gt; full video here&lt;/a&gt; and go to the&lt;a href=&quot;https://betterprogramming.pub/seismic-data-to-subsurface-models-with-openfwi-bcca0218b4e8&quot;&gt; paper&lt;/a&gt; for more details. The GitHub repo is&lt;a href=&quot;https://github.com/lanl/OpenFWI&quot;&gt; OpenFWI&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;session-3--using-pytorch-to-aid-wildfire-prediction&quot;&gt;Session 3:  Using PyTorch to Aid Wildfire Prediction&lt;/h2&gt;

&lt;p&gt;Forest fires pose significant threats to ecosystems, wildlife, and communities. Machine learning presents a promising approach to enhance prediction accuracy. In this  Earth Day webinar, Bob Chesebrough and Rahul Unnikrishnan Nair demonstrated  image analysis techniques using the MODIS dataset which was used  to predict early forest fire probabilities. Through fine-tuning a ResNet18 model with the Intel® Extension for PyTorch, pre-trained models were adjusted with  aerial photos, utilizing geo-spatial and color data for fire risk assessment.&lt;/p&gt;

&lt;p&gt;Emphasizing the temporal and geographical filtering requirements for dataset analysis, showcasing images from fire-affected areas like Paradise, CA, the model’s adaptability to different hardware configurations was highlighted, along with the utilization of Stable Diffusion for data synthesis when real datasets were unavailable. The presenters encouraged  audience engagement in PyTorch experimentation for early fire detection by  extending a challenge to leverage these tools for critical predictive tasks. Join them in this endeavor to enhance wildfire prevention and protection efforts.&lt;/p&gt;

&lt;p&gt;Watch the&lt;a href=&quot;https://www.youtube.com/watch?v=gSC_IHyx0IM&quot;&gt; full video here&lt;/a&gt; and go to the&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/predicting-forest-fires-using-pytorch.html&quot;&gt; paper&lt;/a&gt; for more details. The GitHub repo is&lt;a href=&quot;https://github.com/IntelSoftware/ForestFirePrediction&quot;&gt; ForestFirePrediction&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-the-intel-speakers&quot;&gt;About the Intel Speakers&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/robertchesebrough/&quot;&gt;Bob Chesebrough&lt;/a&gt;, Sr Solutions Architect&lt;/p&gt;

&lt;p&gt;Bob Chesebrough’s industry experience is software development/AI solution engineering for Fortune 100 companies and national laboratories for over three decades. He is also a hobbyist who has logged over 800 miles and 1000 hours in the field finding dinosaur bones. He and his sons discovered an important fossil of the only known crocodilian from the Jurassic in New Mexico, they have also discovered and logged into the museum 2000+ bones localities and described a new mass bone bed in New Mexico.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/rahulunair/&quot;&gt;Rahul Unnikrishnan Nair&lt;/a&gt;, Architect in Applied AI and the Engineering Lead at Intel® Liftoff&lt;/p&gt;

&lt;p&gt;In his current role at Intel® Liftoff for Startups program, Rahul Nair brings his extensive experience in applied AI and engineering to mentor early-stage AI startups. His dedication lies in helping these startups transform their innovative ideas into fully-fledged, market-ready products with a strong emphasis on use-case-driven, practical engineering and optimization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/bconsolvo/&quot;&gt;Ben Consolvo&lt;/a&gt;, AI Software Engineering Manager&lt;/p&gt;

&lt;p&gt;Ben Consolvo is an AI Solutions Engineering Manager at Intel. He has been building a team and a program around Intel’s AI technology paired with Intel’s hardware offerings. He brings a background and passion in data science, particularly in deep learning (DL) and computer vision. He has applied his skills in DL in the cybersecurity industry to automatically identify phishing websites, as well as to the oil and gas industry to identify subsurface features for geophysical imaging.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.linkedin.com/in/kelli-belcher/&quot;&gt;Kelli Belcher&lt;/a&gt;, AI Solutions Engineer&lt;/p&gt;

&lt;p&gt;Kelli Belcher is an AI Solutions Engineer at Intel with over 5 years of experience across the financial services, healthcare, and tech industries. In her current role, Kelli helps build Machine Learning solutions using Intel’s portfolio of open AI software tools. Kelli has experience with Python, R, SQL, and Tableau, and holds a Master of Science in Data Analytics from the University of Texas.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">This post was contributed by Intel AI in partnership with the PyTorch Foundation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Speeding up ViTs using Block Sparsity</title>
      <link href="https://pytorch.org/blog/speeding-up-vits/" rel="alternate" type="text/html" title="Speeding up ViTs using Block Sparsity" />
      <published>2024-05-14T00:00:00-07:00</published>
      <updated>2024-05-14T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/speeding-up-vits</id>
      <content type="html" xml:base="https://pytorch.org/blog/speeding-up-vits/">&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; We show promising results of up to a &lt;strong&gt;1.46x speedup with &amp;lt;2% drop in accuracy&lt;/strong&gt; on float32 Vision Transformers on A100 GPUs by applying block sparsity on MLP module’s weights. This approach can potentially be applied to other types of transformers including large language models. Our implementation and benchmarks to reproduce our results are available at &lt;a href=&quot;https://github.com/pytorch-labs/superblock&quot;&gt;https://github.com/pytorch-labs/superblock&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;PyTorch has landed a lot of improvements to CUDA kernels that implement block sparse matrix multiplications. Recent updates to Pytorch can lead up to &lt;a href=&quot;https://gist.github.com/cpuhrsch/7fec60079cbe2daeff59c0577f933320&quot;&gt;4.8x speedup&lt;/a&gt; on large matrix multiplication shapes with high sparsity levels over dense baselines.&lt;/p&gt;

&lt;p&gt;In this blog, we show the promising results of applying block sparsity on weights of linear layers of MLP (multi-layer perceptron) layers in vision transformers (ViTs) and show end-to-end model speedups on A100 Nvidia GPUs.&lt;/p&gt;

&lt;p&gt;As a recap, block sparsity sparsifies weights in tiles of blocks of predetermined size, rather than sparsifying individual elements. This particular sparsity pattern is interesting because it is amenable to GPU acceleration via fast sparse kernels. For more information about the differences between different sparsity patterns, or about sparsity as a whole, please check out &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity&quot;&gt;torchao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig1.png&quot; alt=&quot;Illustrations of different types of sparsity.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustrations of different types of sparsity.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;Our approach can be broken down into two distinct steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training the model from scratch using block sparse masks subnets.&lt;/li&gt;
  &lt;li&gt;Folding these masks into our weights to accelerate them for inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We explain our training and inference steps below&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Starting with an uninitialized Vision Transformer, we apply random trainable masks with a specified block size and sparsity level on the weights of output projection linear layer of attention blocks, the weights of the two linear layers inside the MLP, a.k.a., FFN (feed forward networks), as well as the final linear classification layer. The forward pass during training follows the &lt;a href=&quot;https://arxiv.org/abs/2207.00670&quot;&gt;supermask approach&lt;/a&gt;, as each mask is converted to binary map using a tuned threshold based on sparsity requirements, e.g., if we want 80% sparsity, we will have the threshold automatically tuned to keep top 20% weights. The masks are of a square &amp;lt;block size&amp;gt;x&amp;lt;block size&amp;gt; elements, where &amp;lt;block size&amp;gt; is a hyperparameter. The priority of the weights is dependent on the mask value or score which is trained. We &lt;a href=&quot;https://github.com/pytorch-labs/superblock/blob/7a469210c7bcb846dd8b6bfa848d104312312126/supermask.py#L130&quot;&gt;multiply the binary masks of each layer with the weights&lt;/a&gt; to sparsify the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig2.png&quot; alt=&quot;Illustration of the Supermask sparsification approach&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustration of the &lt;a href=&quot;https://arxiv.org/abs/2207.00670&quot;&gt;Supermask&lt;/a&gt; sparsification approach.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;p&gt;After training, the &lt;a href=&quot;https://github.com/pytorch-labs/superblock/blob/7a469210c7bcb846dd8b6bfa848d104312312126/supermask.py#L122-L125&quot;&gt;dense weights can be turned to sparse weights by multiplying with the mask&lt;/a&gt; and stored for inference. At this stage, although the weights have a high percentage of zero values, they are still stored in dense format. We use PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html&quot;&gt;to_sparse_bsr()&lt;/a&gt; API to to convert the weights to &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html#sparse-bsr-docs&quot;&gt;Block Sparse Representation&lt;/a&gt; (BSR) format that stores only the non-zero values and the indices of their blocks. This step only needs to be done once and the results can be cached for runtime.&lt;/p&gt;

&lt;p&gt;During runtime, no changes in code are required. We just pass any input tensor to the model, and when the forward() function of the sparsified linear layers are invoked, PyTorch takes care of invoking the optimized matrix multiplication for block sparse weights. This should work for A100 as well as H100 NVIDIA GPUs.&lt;/p&gt;

&lt;h2 id=&quot;results-microbenchmarks&quot;&gt;Results: Microbenchmarks&lt;/h2&gt;

&lt;p&gt;To validate the viability of block sparsity from a performance standpoint, we first ran a series of microbenchmarks using this &lt;a href=&quot;https://github.com/pytorch/ao/blob/73f8efce1e950235f58dc917ee204517ec74bba0/benchmarks/benchmark_gpu_sparsity.py&quot;&gt;simple script&lt;/a&gt;. Using the linear shapes from ViT-b, we compared the speedup of our block sparse kernels across a single linear layer as we varied the sparsity level and block size of the weight matrix.&lt;/p&gt;

&lt;p&gt;We run using PyTorch 2.3.0.dev20240305+cu121 nightly on NVIDIA A100s and report the speedup of each sparsity configuration compared to dense baseline. We observed positive speedups when block size &amp;gt;=32 or sparsity level &amp;gt;= 0.8 for float32, while for bfloat16 we observe smaller speedups and usually for block size 64 and higher sparsities. Hence, for end-to-end speedups on the model, we will focus in this blog on float32 and leave bfloat16 for future work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig3.png&quot; alt=&quot;Micro benchmarking results on linear layers of ViT-b-16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig4.png&quot; alt=&quot;Micro benchmarking results on linear layers of ViT-b-16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Micro benchmarking results on linear layers of ViT-b-16.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;results-vision-transformers&quot;&gt;Results: Vision Transformers&lt;/h2&gt;

&lt;p&gt;Once we confirmed that we were able to show speedups over the linear layers, we focused on showing end-to-end speedups on &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html&quot;&gt;ViT_B_16&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We trained this model from scratch on ImageNet dataset using the standard &lt;a href=&quot;https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16&quot;&gt;ViT_B_16 recipe&lt;/a&gt;. We show speedups for sparsifying MLP modules and leave sparsifying weights of input and output projections of attention for future work.&lt;/p&gt;

&lt;p&gt;We looked at wall-clock inference speedup, focusing on batch size 256. We found that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For 90% sparsity we can get 1.24x, 1.37x, 1.65x speedups for block sizes 16, 32, and 64 respectively.&lt;/li&gt;
  &lt;li&gt;To obtain speedup, the minimum sparsity for block sizes 16, 32, and 64 are 0.86, 0.82, and 0.7 respectively. Hence, as expected, the larger the block size, the smaller sparsity we need to obtain speedup.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We note a limitation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sparse_bsr()&lt;/code&gt; API: that layers need to be multiples of the block size. Since the dimensions of the last FC classification layer in ViT was not a multiple of the block size, they were not converted to BSR representation in our experiments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig5.png&quot; alt=&quot;Speedup on ViT-b-16 with batch size 256 on MLP modules across different batch sparsities and block sizes.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Speedup on ViT-b-16 with batch size 256 on MLP modules across different batch sparsities and block sizes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We also explored the speedup for different batch sizes for 90% sparsity. We observed a speedup over the baseline for batch sizes starting from 16 and upwards. While bigger block sizes have bigger speedups at the largest batch sizes, the smallest possible batch size to obtain &amp;gt;1 speedup is smaller for smaller block sizes.&lt;/p&gt;

&lt;p&gt;We believe on-device hardware can obtain speedups for batch size 1 as they - unlike server GPUs - can be fully utilized at such small batch sizes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig6.png&quot; alt=&quot;Speedup on ViT-b-16 with 90% sparsity on MLP modules across different batch sizes and block sizes.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Speedup on ViT-b-16 with 90% sparsity on MLP modules across different batch sizes and block sizes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Looking at the Top-1 accuracy on ImageNet=blurred test set of the sparsified models for different block sizes and sparsities, we see a few expected results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;low levels of sparsity (&amp;lt;=70%) have no meaningful regression in accuracy&lt;/li&gt;
  &lt;li&gt;mid levels of sparsity (&amp;gt;=80% to &amp;lt;90%) have limited regression in accuracy&lt;/li&gt;
  &lt;li&gt;high levels of sparsity (&amp;gt;=90%) removes so many weights that accuracy is significantly impacted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More research could be done to improve accuracies of higher sparsities and larger block sizes. We hope that the block sparsity support in PyTorch and the illustrated speedups in this blog will encourage researchers to explore more accurate sparsification approaches.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig7.png&quot; alt=&quot;Accuracies on training ViT-b-16 on ImageNet-blurred using the SuperMask approach.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Accuracies on training ViT-b-16 on ImageNet-blurred using the SuperMask approach.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We have shown promising speedups for block sparsifying MLP modules ViT in float32 precision.  There is still more work to be done in order to observe speedups on bfloat16 and we hope to obtain progress on that soon. Possible next steps to further optimize block sparsity on vision transformers and transformers in general:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform block sparsity on attention input and output projections.&lt;/li&gt;
  &lt;li&gt;Perform block sparsity during finetuning rather than training from scratch.&lt;/li&gt;
  &lt;li&gt;Perform further optimizations on the matmul kernels for ViT’s linear operator specific shapes (especially for 80% and lower sparsity).&lt;/li&gt;
  &lt;li&gt;Combine with other optimizations such as int8 and torch.compile()&lt;/li&gt;
  &lt;li&gt;Explore other weight sparsification algorithms, e.g., &lt;a href=&quot;https://arxiv.org/abs/2205.14107&quot;&gt;Spartan&lt;/a&gt;, to improve accuracy&lt;/li&gt;
  &lt;li&gt;Explore selecting weights to sparsify (e.g., specific transformer layers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please reach out to &lt;a href=&quot;mailto:melhoushi@meta.com&quot;&gt;melhoushi@meta.com&lt;/a&gt; if you have questions or are interested in contributing to block sparsification!&lt;/p&gt;

&lt;p&gt;Additionally if you’re broadly interested in sparsity please feel free to reach out to &lt;a href=&quot;https://github.com/jcaip&quot;&gt;@jcaip&lt;/a&gt; / &lt;a href=&quot;mailto:jessecai@meta.com&quot;&gt;jessecai@meta.com&lt;/a&gt; and please come check out &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao&lt;/a&gt;, a community we’re building for architecture optimization techniques like quantization and sparsity.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>FAIR at Meta: Mostafa Elhoushi, Sensors and Systems at Meta Reality Labs Research: Syed Shakib Sarwar, Aaryan Kothapalli, Mia Kasperek, Barbara De Salvo, PyTorch at Meta: Christian Puhrsch, Jesse Cai, Joe Isaacson, Quantsight: Andrew James, Pearu Peterson, Nikita Vedeneev</name>
        
        
      </author>

      

      

      
        <summary type="html">TLDR: We show promising results of up to a 1.46x speedup with &amp;lt;2% drop in accuracy on float32 Vision Transformers on A100 GPUs by applying block sparsity on MLP module’s weights. This approach can potentially be applied to other types of transformers including large language models. Our implementation and benchmarks to reproduce our results are available at https://github.com/pytorch-labs/superblock.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Enhancing Deep Learning Workflows: PyTorch Ecosystem Tools</title>
      <link href="https://pytorch.org/blog/enhancing-deep-learning/" rel="alternate" type="text/html" title="Enhancing Deep Learning Workflows: PyTorch Ecosystem Tools" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/enhancing-deep-learning</id>
      <content type="html" xml:base="https://pytorch.org/blog/enhancing-deep-learning/">&lt;p&gt;Welcome to the thriving PyTorch ecosystem, where a wealth of tools and libraries await, purpose-built to elevate your experience in deep learning as a developer or researcher. The Ecosystem Tools pages host  many projects from experts spanning academia, industry, application development, and machine learning.&lt;/p&gt;

&lt;p&gt;Initially, PyTorch aimed to establish a thriving community, enabling developers to access each other’s tools, engage in meaningful discussions, and explore the wealth of resources available within the community.&lt;/p&gt;

&lt;p&gt;Today, the PyTorch ecosystem has grown to feature over 100 projects tailored to your needs, providing robust support, enhanced speed, and effortless integration with PyTorch. If your project aligns with our mission, we invite you to &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;submit&lt;/a&gt; it and join this dynamic ecosystem.&lt;/p&gt;

&lt;p&gt;New this month, we’ve moved all of our Ecosystem blogs over to our PyTorch.org website to host a space where our community can show off the latest innovations with our users. Read on to hear about the latest projects in the ecosystem!&lt;/p&gt;

&lt;h2 id=&quot;explore-the-latest-tools-and-frameworks-in-the-ecosystem&quot;&gt;Explore the Latest Tools and Frameworks in the Ecosystem&lt;/h2&gt;

&lt;p&gt;As we continue into 2024, we’re thrilled to showcase an impressive array of ecosystem tools that significantly enrich the PyTorch community. These tools cover a wide range of domains, including pose estimation, profiling, and even quantum computing. Let’s explore each one to witness firsthand how they are reshaping the PyTorch landscape, opening up exciting possibilities for developers.&lt;/p&gt;

&lt;h3 id=&quot;anomalib&quot;&gt;&lt;a href=&quot;https://github.com/openvinotoolkit/anomalib&quot;&gt;Anomalib&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on image-based anomaly detection, where the goal of the algorithm is to identify anomalous images, or anomalous pixel regions within images in a dataset. Anomalib is constantly updated with the latest algorithms and training/inference extensions.&lt;/p&gt;

&lt;h3 id=&quot;diffusers&quot;&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers&quot;&gt;Diffusers&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you’re looking for a simple inference solution or training your own diffusion models, Diffusers is a modular toolbox that supports both.&lt;/p&gt;

&lt;h3 id=&quot;pomegranate&quot;&gt;&lt;a href=&quot;https://pomegranate.readthedocs.io/en/latest/&quot;&gt;Pomegranate&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Pomegranate is a versatile machine learning library that integrates seamlessly with PyTorch. It provides a wide range of probabilistic models and tools for probabilistic modeling tasks. Pomegranate empowers users to build complex models such as hidden Markov models (HMMs), Bayesian networks, and Gaussian mixture models (GMMs). By combining the strengths of PyTorch and Pomegranate, developers can leverage the power of deep learning and probabilistic modeling to tackle various machine learning challenges.&lt;/p&gt;

&lt;h3 id=&quot;pypose&quot;&gt;&lt;a href=&quot;https://pypose.org/&quot;&gt;PyPose&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;PyPose is a PyTorch-based library designed for pose estimation tasks. With PyPose, developers can efficiently train and deploy models for human pose estimation, a fundamental computer vision problem. By leveraging PyTorch’s flexibility and performance, PyPose simplifies the process of building accurate pose estimation models. Its intuitive APIs and pre-trained models make it an excellent choice for researchers and developers exploring human pose estimation applications.&lt;/p&gt;

&lt;h3 id=&quot;pypots&quot;&gt;&lt;a href=&quot;https://github.com/WenjieDu/PyPOTS&quot;&gt;PyPOTS&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;A python toolbox/library for data mining on partially-observed time series with PyTorch, including SOTA models supporting tasks of imputation, classification, clustering, and forecasting on incomplete (irregularly-sampled) multivariate time series with missing values.&lt;/p&gt;

&lt;h3 id=&quot;octoml-profiler&quot;&gt;&lt;a href=&quot;https://github.com/octoml/octoml-profile&quot;&gt;OctoML Profiler&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OctoML Profiler is a performance profiling tool that aids in optimizing PyTorch models. This tool helps developers identify performance bottlenecks and inefficiencies within their deep learning models. By providing insights into memory usage, compute time, and data movement, the OctoML Profiler enables developers to fine-tune their models for improved efficiency. With this valuable feedback, developers can optimize their models for deployment on various hardware platforms.&lt;/p&gt;

&lt;h3 id=&quot;open-compass&quot;&gt;&lt;a href=&quot;https://github.com/open-compass/opencompass&quot;&gt;Open Compass&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OpenCompass is a one-stop platform for large model evaluation, aiming to provide a fair, open, and reproducible benchmark for large model evaluation. Its main features include: Comprehensive support for models and datasets, efficient distributed evaluation, diversified evaluation paradigms, modular design with high extensibility and experiment management and reporting mechanism.&lt;/p&gt;

&lt;h3 id=&quot;renate&quot;&gt;&lt;a href=&quot;https://renate.readthedocs.io/en/latest/&quot;&gt;Renate&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Renate is a PyTorch-based library for neural architecture search (NAS). It simplifies the process of automatically searching for optimal neural network architectures tailored to specific tasks. Renate leverages techniques like reinforcement learning and evolutionary algorithms to efficiently explore the architecture space. By using Renate, developers can save significant time and resources while discovering highly performant models.&lt;/p&gt;

&lt;h3 id=&quot;roma&quot;&gt;&lt;a href=&quot;https://github.com/naver/roma&quot;&gt;RoMa&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;RoMa is a standalone library to handle rotation representations with PyTorch (rotation matrices, quaternions, rotation vectors, etc). It aims for robustness, ease-of-use, and efficiency.&lt;/p&gt;

&lt;h3 id=&quot;substra&quot;&gt;&lt;a href=&quot;https://github.com/Substra&quot;&gt;Substra&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Substra is an open source federated learning (FL) software. It enables the training and validation of machine learning models on distributed datasets. It provides a flexible Python interface and a web application to run federated learning training at scale. Substra’s main usage is in production environments. It has already been deployed and used by hospitals and biotech companies. Substra can also be used on a single machine to perform FL simulations and debug code.&lt;/p&gt;

&lt;h3 id=&quot;torchquantum&quot;&gt;&lt;a href=&quot;https://hanruiwanghw.wixsite.com/torchquantum&quot;&gt;TorchQuantum&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;TorchQuantum is a powerful library that combines the PyTorch framework with quantum computing concepts. It enables developers to explore quantum machine learning algorithms and build hybrid classical-quantum models. By integrating the principles of quantum computing into PyTorch, TorchQuantum opens up new possibilities for solving complex problems that traditional deep learning approaches may struggle with.&lt;/p&gt;

&lt;h3 id=&quot;tiatoolbox&quot;&gt;&lt;a href=&quot;https://github.com/TissueImageAnalytics/tiatoolbox&quot;&gt;TIAToolbox&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The TIAToolbox (Text-Image-Augmentation Toolbox) is a PyTorch library designed to augment text and image data for deep learning tasks. It offers a comprehensive set of tools for data augmentation, including transformations, noise injection, and image/text synthesis. By applying TIAToolbox, developers can enrich their training datasets, improve model generalization, and enhance the robustness of their deep learning models.&lt;/p&gt;

&lt;h3 id=&quot;torchdistill&quot;&gt;&lt;a href=&quot;https://github.com/yoshitomo-matsubara/torchdistill&quot;&gt;torchdistill&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;torchdistill is a coding-free framework built on PyTorch for reproducible deep learning and knowledge distillation studies. The framework is designed to enable users to design experiments by declarative PyYAML configuration files and supports high-level module abstractions.&lt;/p&gt;

&lt;h3 id=&quot;torchopt&quot;&gt;&lt;a href=&quot;https://torchopt.readthedocs.io/en/latest/#&quot;&gt;TorchOpt&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;TorchOpt is a PyTorch library focused on optimization algorithms for deep learning. It provides a collection of state-of-the-art optimization techniques, such as stochastic gradient descent (SGD) variants, adaptive learning rate methods, and optimization schedules. TorchOpt empowers developers to fine-tune their models efficiently, converge faster, and achieve better performance in various deep learning tasks.&lt;/p&gt;

&lt;h3 id=&quot;usb&quot;&gt;&lt;a href=&quot;https://usb.readthedocs.io/&quot;&gt;USB&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;USB, or Unified Speech-to-Text Benchmark, is a PyTorch-based toolkit for training and evaluating speech recognition models. It provides standardized datasets and evaluation metrics to facilitate fair and accurate comparisons between different speech recognition architectures. By using USB, researchers and developers can benchmark their models against state-of-the-art systems and drive advancements in the field of automatic speech recognition.&lt;/p&gt;

&lt;h3 id=&quot;zeus&quot;&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Zeus is the current state-of-the-art in deep learning energy measurement and optimization. It has monitor components that allow users to measure GPU energy consumption and optimizer components that automatically optimize DNN or GPU knobs based on measurements from the monitor component.&lt;/p&gt;

&lt;h2 id=&quot;be-part-of-our-ecosystem&quot;&gt;Be Part of Our Ecosystem&lt;/h2&gt;

&lt;p&gt;Our  diverse ecosystem tools are instrumental in PyTorch’s success.. They provide essential  support for tasks such as pose estimation, probabilistic modeling, performance profiling, model interpretability, speech recognition, quantum computing, data augmentation, optimization, and neural architecture search.&lt;/p&gt;

&lt;p&gt;Leveraging these tools empowers developers and researchers to accelerate their deep learning workflows and unlock new possibilities in the field of AI.&lt;/p&gt;

&lt;p&gt;Have a tool that would be a good fit for the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem&lt;/a&gt;? If you can answer the below questions, we’d love for you to &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;submit your tool for review&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Does your project complement PyTorch, enhancing user experience, introducing new capabilities, or accelerating training and inference processes?
    &lt;ul&gt;
      &lt;li&gt;Examples could include visualization tools, a kernel library or a framework that sits on top to enable research in a particular area such as NLP.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Is the project ready for broad developer usage?
    &lt;ul&gt;
      &lt;li&gt;For example, is the project stable, will it be maintained, and is there adequate supporting infrastructure, documentation, and technical support to allow a developer to successfully use it?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you to all of our contributors and collaborators in our ecosystem! Here’s to a great 2024.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Welcome to the thriving PyTorch ecosystem, where a wealth of tools and libraries await, purpose-built to elevate your experience in deep learning as a developer or researcher. The Ecosystem Tools pages host many projects from experts spanning academia, industry, application development, and machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing depyf: mastering torch.compile with ease</title>
      <link href="https://pytorch.org/blog/introducing-depyf/" rel="alternate" type="text/html" title="Introducing depyf: mastering torch.compile with ease" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-depyf</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-depyf/">&lt;p&gt;&lt;img src=&quot;/assets/images/depyf.png&quot; alt=&quot;depyf logo&quot; style=&quot;width:100%;display: block; max-width: 400px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are thrilled to introduce &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt;, a new project to the PyTorch ecosystem designed to help users understand, learn, and adapt to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;!&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a cornerstone of PyTorch 2.x, offering a straightforward path to accelerate machine learning workflows with just a single line of code for both training and inference. The mere inclusion of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@torch.compile&lt;/code&gt; can&lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt; dramatically enhance the performance of your code&lt;/a&gt;. However, identifying the optimal insertion point for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is not easy, not to mention the complexity of adjusting various knobs for maximum efficiency.&lt;/p&gt;

&lt;p&gt;The intricacies of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; stack, encompassing Dynamo, AOTAutograd, Inductor, and more, present a &lt;strong&gt;steep learning curve&lt;/strong&gt;. These components, essential for deep learning performance optimization, can be daunting without a solid foundation in the subject.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: For an introductory example of how torch.compile works, please refer to this&lt;a href=&quot;https://depyf.readthedocs.io/en/latest/walk_through.html&quot;&gt; walk-through explanation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-common-tool-torch_compile_debug&quot;&gt;A common tool: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;To demystify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, the common approach involves leveraging the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; environment variable. While it provides more information, deciphering the output remains a formidable task.&lt;/p&gt;

&lt;p&gt;For example, when we have the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And run it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG=1 python test.py&lt;/code&gt; , we will get a directory named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_compile_debug/run_2024_02_05_23_02_45_552124-pid_9520&lt;/code&gt; , under which there are these files:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── torchdynamo
│   └── debug.log
└── torchinductor
   ├── aot_model___0_debug.log
   ├── aot_model___10_debug.log
   ├── aot_model___11_debug.log
   ├── model__4_inference_10.1
   │   ├── fx_graph_readable.py
   │   ├── fx_graph_runnable.py
   │   ├── fx_graph_transformed.py
   │   ├── ir_post_fusion.txt
   │   ├── ir_pre_fusion.txt
   │   └── output_code.py
   ├── model__5_inference_11.2
   │   ├── fx_graph_readable.py
   │   ├── fx_graph_runnable.py
   │   ├── fx_graph_transformed.py
   │   ├── ir_post_fusion.txt
   │   ├── ir_pre_fusion.txt
   │   └── output_code.py
   └── model___9.0
       ├── fx_graph_readable.py
       ├── fx_graph_runnable.py
       ├── fx_graph_transformed.py
       ├── ir_post_fusion.txt
       ├── ir_pre_fusion.txt
       └── output_code.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The generated files and logs often raise more questions than they answer, leaving developers puzzled over the meaning and relationships within the data. Common puzzles for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model__4_inference_10.1&lt;/code&gt; mean?&lt;/li&gt;
  &lt;li&gt;I have one function but three &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model__xxx.py&lt;/code&gt; in the directory, what is their correspondence?&lt;/li&gt;
  &lt;li&gt;What are those &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOAD_GLOBAL&lt;/code&gt; stuff in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;debug.log&lt;/code&gt; ?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-better-tool-depyf-comes-to-rescue&quot;&gt;A better tool: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; comes to rescue&lt;/h2&gt;

&lt;p&gt;Let’s see how &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; can help developers to resolve the above challenges. To use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; , simply execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install depyf&lt;/code&gt; or follow the project page&lt;a href=&quot;https://github.com/thuml/depyf&quot;&gt; https://github.com/thuml/depyf&lt;/a&gt; to install the latest version, and then surround the main code within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.prepare_debug&lt;/code&gt; .&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   import depyf
   with depyf.prepare_debug(&quot;depyf_debug_dir&quot;):
       main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python test.py&lt;/code&gt; , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; will produce a directory named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt; (the argument of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_debug&lt;/code&gt; function). Under the directory, there would be these files:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── __compiled_fn_0 AFTER POST GRAD 0.py
├── __compiled_fn_0 Captured Graph 0.py
├── __compiled_fn_0 Forward graph 0.py
├── __compiled_fn_0 kernel 0.py
├── __compiled_fn_3 AFTER POST GRAD 0.py
├── __compiled_fn_3 Captured Graph 0.py
├── __compiled_fn_3 Forward graph 0.py
├── __compiled_fn_3 kernel 0.py
├── __compiled_fn_4 AFTER POST GRAD 0.py
├── __compiled_fn_4 Captured Graph 0.py
├── __compiled_fn_4 Forward graph 0.py
├── __compiled_fn_4 kernel 0.py
├── __transformed_code_0_for_torch_dynamo_resume_in_toy_example_at_8.py
├── __transformed_code_0_for_toy_example.py
├── __transformed_code_1_for_torch_dynamo_resume_in_toy_example_at_8.py
└── full_code_for_toy_example_0.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And there are two obvious benefits:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The long and difficult-to-understand &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdynamo/debug.log&lt;/code&gt; is gone. Its content is cleaned up and shown as human-readable source code, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;full_code_for_xxx.py&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__transformed_code_{n}_for_xxx.py&lt;/code&gt; . It is worth to note, that the most tedious and difficult job of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; is to decompile the bytecode inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdynamo/debug.log&lt;/code&gt; into Python source code, freeing developers from intimidating internals of Python.&lt;/li&gt;
  &lt;li&gt;The correspondence between function names and computation graphs are respected. For example, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__transformed_code_0_for_toy_example.py&lt;/code&gt; , we can see a function named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0&lt;/code&gt; , and we will immediately know its corresponding computation graphs are in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0_xxx.py&lt;/code&gt; , because they share the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0&lt;/code&gt; prefix name.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Starting with &lt;code&gt;full_code_for_xxx.py&lt;/code&gt; , and following the functions involved, users will have a clear view of what &lt;code&gt;torch.compile&lt;/code&gt; does to their code.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;one-more-thing-step-through-debuggability&quot;&gt;One more thing: step-through debuggability&lt;/h2&gt;

&lt;p&gt;Stepping through code line by line using debuggers is a great way to understand how code works. However, under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; , those files are only for users’ information, and cannot be executed with the data users concern.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: By “debug”, we mean the process of inspecting and improving a program, rather than correcting buggy code.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A standout feature of &lt;code&gt;depyf&lt;/code&gt; is its capability to facilitate step-through debugging for &lt;code&gt;torch.compile&lt;/code&gt;&lt;/strong&gt;: all of the files it generates are linked with runtime code objects inside Python interpreter, and we can set breakpoints in these files. The usage is simple, just add one context manager &lt;code&gt;with depyf.debug()&lt;/code&gt; , and it should do the trick:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   import depyf
   with depyf.prepare_debug(&quot;depyf_debug_dir&quot;):
       main()
   with depyf.debug():
       main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just one caveat: the workflow of debugging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; deviates from standard debugging workflow. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, many codes are &lt;strong&gt;dynamically&lt;/strong&gt; generated. Therefore, we need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;launch the program&lt;/li&gt;
  &lt;li&gt;when the program exits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.prepare_debug(&quot;depyf_debug_dir&quot;)&lt;/code&gt; , code will be available in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;when the program enters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.debug()&lt;/code&gt; , it will automatically set a breakpoint internally, so that the program is paused.&lt;/li&gt;
  &lt;li&gt;navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt; to set breakpoints.&lt;/li&gt;
  &lt;li&gt;continue to run the code, and debuggers will hit these breakpoints!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depyf-screenshot.png&quot; alt=&quot;depyf screenshot&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a screenshot of what it looks like. All code and tensor variables are live, and we can inspect any variable, and step through the code, as in our daily debugging workflow now! The only difference is that we are debugging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; generated code rather than human-written code.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; serves as an invaluable tool for accelerating PyTorch code effortlessly. For those looking to delve deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, whether to leverage its full potential or to integrate custom operations, the learning curve can be very steep though. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; is designed to lower this barrier, offering a user-friendly experience to understand, learn, and adapt to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Do explore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; and experience its benefits firsthand! The project is open-source and readily available at&lt;a href=&quot;https://github.com/thuml/depyf&quot;&gt; https://github.com/thuml/depyf&lt;/a&gt;. Installation is straightforward via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install depyf&lt;/code&gt;. We hope &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; can enhance everyone’s development workflow with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kaichao You</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Learning Energy Measurement and Optimization</title>
      <link href="https://pytorch.org/blog/zeus/" rel="alternate" type="text/html" title="Deep Learning Energy Measurement and Optimization" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/zeus</id>
      <content type="html" xml:base="https://pytorch.org/blog/zeus/">&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig1.png&quot; alt=&quot;Zeus logo&quot; style=&quot;width:100%;display: block; max-width: 400px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is authored by &lt;a href=&quot;https://jaewonchung.me/about&quot;&gt;Jae-Won Chung&lt;/a&gt;, a PhD student at the University of Michigan and the lead of the &lt;a href=&quot;https://ml.energy&quot;&gt;ML.ENERGY Initiative&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep learning consumes quite a bit of energy. For instance, training a single 200B LLM on AWS p4d instances consumed around 11.9 GWh (source: &lt;a href=&quot;https://mvdirona.com/jrh/talksandpapers/JamesHamiltonCIDR2024.pdf&quot;&gt;CIDR 2024 keynote&lt;/a&gt;), which is an amount that can single-handedly power more than a thousand &lt;a href=&quot;https://www.eia.gov/tools/faqs/faq.php?id=97&amp;amp;t=3&quot;&gt;average US households&lt;/a&gt; for a year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus&lt;/a&gt; is an open-source toolbox for measuring and optimizing the energy consumption of deep learning workloads. Our goal is to make energy optimization based on accurate measurements as easy as possible for diverse deep learning workloads and setups by offering composable tools with minimal assumptions.&lt;/p&gt;

&lt;p&gt;Zeus largely provides two types of tools:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Programmatic and command line GPU energy &lt;strong&gt;measurement&lt;/strong&gt; tools&lt;/li&gt;
  &lt;li&gt;Several energy &lt;strong&gt;optimization&lt;/strong&gt; tools that find the best ML and/or GPU configurations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Zeus can benefit those who would like to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;measure and optimize their electricity cost&lt;/li&gt;
  &lt;li&gt;reduce heat dissipation from their GPUs (by lowering power draw)&lt;/li&gt;
  &lt;li&gt;report energy usage from research and development&lt;/li&gt;
  &lt;li&gt;reduce carbon footprint from electricity usage&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-1-measuring-energy&quot;&gt;Part 1: Measuring Energy&lt;/h2&gt;

&lt;p&gt;Just like performance optimization, accurate measurement is the basis of effective energy optimization. Popular proxies for estimating power consumption like the maximum power draw of the hardware &lt;a href=&quot;https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/&quot;&gt;can sometimes be vastly off&lt;/a&gt; compared to actual measurement.&lt;/p&gt;

&lt;p&gt;To make energy measurement as easy and transparent as possible, the core utility Zeus offers is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; class. Let’s take a look at the actual snippet:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# All four GPUs are measured simultaneously.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_indices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Measure total time and energy within the window.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measurement windows can arbitrarily be overlapped.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;epoch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;epoch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Epoch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_energy&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; J&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Entire training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_energy&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; J&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What you see above is a typical PyTorch training loop which uses four GPUs for data parallel training. Inside, we created an instance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; and passed in a list of GPU indices to monitor. Then, using the monitor, we can measure the time and energy consumption of arbitrary execution &lt;em&gt;windows&lt;/em&gt; within the training script by pairing calls to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;begin_window&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end_window&lt;/code&gt;. Multiple windows can overlap and nest in arbitrary ways without affecting the measurement of each, as long as their names are different.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; adds very little overhead – typically single digit milliseconds – around the window. This allows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; to be used in various applications. For instance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/leaderboard&quot;&gt;The ML.ENERGY Leaderboard&lt;/a&gt;: The first open-source benchmark on how much energy LLM text generation consumes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/leaderboard&quot;&gt;The ML.ENERGY Colosseum&lt;/a&gt;: An online service that lets users compare LLM responses side-by-side based on response quality &lt;em&gt;and&lt;/em&gt; energy consumption.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See our &lt;a href=&quot;https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/&quot;&gt;blog post&lt;/a&gt; for a deeper technical dive into accurate GPU energy measurement.&lt;/p&gt;

&lt;h2 id=&quot;part-2-optimizing-energy&quot;&gt;Part 2: Optimizing Energy&lt;/h2&gt;

&lt;p&gt;Let me introduce you to two of the energy optimizers provided by Zeus.&lt;/p&gt;

&lt;h3 id=&quot;globalpowerlimitoptimizer&quot;&gt;GlobalPowerLimitOptimizer&lt;/h3&gt;

&lt;p&gt;GPUs allow users to configure its maximum power draw, called &lt;em&gt;power limit&lt;/em&gt;. Typically, as you lower the GPU’s power limit from the default maximum, computation may get slightly slower, but you’ll save disproportionately more energy. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; in Zeus automatically finds the optimal GPU power limit globally across all GPUs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The optimizer measures time and energy through the ZeusMonitor.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_indices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_epoch_begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_step_begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_step_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_epoch_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In our familiar PyTorch training loop, we have instantiated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; and passed it an instance of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt;, through which the optimizer sees the GPUs. Then, we just need to let the optimizer know about training progress (step and epoch boundaries), and the optimizer will transparently do all the necessary profiling and converge to the optimal power limit.&lt;/p&gt;

&lt;p&gt;If you’re using the HuggingFace &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer&quot;&gt;Trainer&lt;/a&gt; or &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/sft_trainer&quot;&gt;SFTTrainer&lt;/a&gt;, integration is even easier:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# ZeusMonitor actually auto-detects CUDA_VISIBLE_DEVICES.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pl_optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Pass in the optimizer as a Trainer callback. Also works for SFTTrainer.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/code&gt; wraps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; so that it automatically detects step and epoch boundaries. We have example integrations &lt;a href=&quot;https://github.com/ml-energy/zeus/tree/master/examples/huggingface&quot;&gt;here&lt;/a&gt;, including running Gemma 7B supervised fine-tuning with QLoRA.&lt;/p&gt;

&lt;p&gt;Now, we know how to integrate the optimizer, but what is the &lt;em&gt;optimal&lt;/em&gt; power limit? We know different users can have different preferences regarding trading off time and energy, so we allow users to specify an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OptimumSelector&lt;/code&gt; (basically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Strategy_pattern&quot;&gt;Strategy Pattern&lt;/a&gt;) to express their needs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Built-in strategies for selecting the optimal power limit.
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Energy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;MaxSlowdownConstraint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Minimize energy while tolerating at most 10% slowdown.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;MaxSlowdownConstraint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some of the built-in strategies include “Minimize time” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time&quot;&gt;Time&lt;/a&gt;, this might still reduce the power limit from the default since some workloads exhibit almost no slowdown even on lower power limits), “Minimize energy” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy&quot;&gt;Energy&lt;/a&gt;), “Somewhere in between” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost&quot;&gt;ZeusCost&lt;/a&gt;), and “Minimize energy given maximum slowdown” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint&quot;&gt;MaxSlowdownConstraint&lt;/a&gt;). Users can also create their own optimum selectors as needed.&lt;/p&gt;

&lt;h3 id=&quot;pipelinefrequencyoptimizer&quot;&gt;PipelineFrequencyOptimizer&lt;/h3&gt;

&lt;p&gt;The pipeline frequency optimizer, based on our research paper &lt;a href=&quot;https://ml.energy/zeus/research_overview/perseus&quot;&gt;Perseus&lt;/a&gt;, is our latest work on energy optimization for large model training, like GPT-3. Perseus can reduce the energy consumption of large model training with no or negligible training throughput degradation. We’ll briefly talk about how.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig2.png&quot; alt=&quot;one iteration of training with four stage pipeline parallelism&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above is a visualization of one iteration of training with four stage &lt;em&gt;pipeline parallelism&lt;/em&gt; running with the 1F1B schedule. Each box is either a forward or a backward computation, and is colored with its power consumption.&lt;/p&gt;

&lt;p&gt;The key observation here is that when models are partitioned into pipeline stages, it’s very difficult to slice them in perfectly equal sizes. This leads to forward/backward boxes of varying widths and therefore computation &lt;em&gt;idle time&lt;/em&gt; between boxes. You would notice that those smaller boxes can run slightly slower than wider boxes and the overall critical path (blue line) will not change at all.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig3.png&quot; alt=&quot;one iteration of training with four stage pipeline parallelism&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s what Perseus automatically does. Based on profiling, it identifies computation boxes that are not on the critical path and figures out the precise amount of slowdown for each box that minimizes energy consumption. When done correctly, computations we slowed down will consume less power &amp;amp; energy, but the overall iteration time of the pipeline does not change.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://ml.energy/zeus/optimize/pipeline_frequency_optimizer/&quot;&gt;our guide&lt;/a&gt; to get started with Perseus!&lt;/p&gt;

&lt;h2 id=&quot;final-words&quot;&gt;Final Words&lt;/h2&gt;

&lt;p&gt;For users who run their own on-premise compute, energy consumption and the resulting electricity bill is not something that can be easily overlooked. On a larger scale, energy consumption is not just about electricity bills, but also about data center power delivery. With thousands of GPUs running in clusters, finding stable, affordable, and sustainable electricity sources to power data centers is becoming &lt;a href=&quot;https://www.cbre.com/insights/reports/north-america-data-center-trends-h1-2023&quot;&gt;increasingly challenging&lt;/a&gt;. Finding ways to reduce energy disproportionately more than slowdown leads to lower average power consumption, which can help with the power delivery challenge.&lt;/p&gt;

&lt;p&gt;With Zeus, we hope to take the first step towards deep learning energy measurement and optimization.&lt;/p&gt;

&lt;p&gt;Wondering where to go from here? Here are a couple helpful links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/zeus&quot;&gt;Zeus homepage/documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus GitHub repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus/tree/master/examples&quot;&gt;Zeus usage and integration examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy&quot;&gt;ML.ENERGY Initiative&lt;/a&gt; (i.e., the people building Zeus)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jae-Won Chung</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
</feed>



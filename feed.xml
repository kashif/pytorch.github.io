<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-10-31T11:15:02-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Deploying LLMs with TorchServe + vLLM</title>
      <link href="https://pytorch.org/blog/deploying-llms-torchserve-vllm/" rel="alternate" type="text/html" title="Deploying LLMs with TorchServe + vLLM" />
      <published>2024-10-31T00:00:00-07:00</published>
      <updated>2024-10-31T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/deploying-llms-torchserve-vllm</id>
      <content type="html" xml:base="https://pytorch.org/blog/deploying-llms-torchserve-vllm/">&lt;p&gt;The vLLM engine is currently one of the top-performing ways to execute large language models (LLM). It provides the &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html&quot;&gt;vllm serve&lt;/a&gt; command as an easy option to deploy a model on a single machine. While this is convenient, to serve these LLMs in production and at scale some advanced features are necessary.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TorchServe offers these essential production features (like custom metrics and model versioning) and through its flexible custom handler design, makes it very easy to integrate features such as retrieval-augmented generation (RAG) or safeguards like &lt;a href=&quot;https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/&quot;&gt;Llama Guard&lt;/a&gt;. It is therefore natural to pair the vLLM engine with TorchServe to create a full-fledged LLM serving solution for production.&lt;/p&gt;

&lt;p&gt;Before going into the specifics of the integration, we will demonstrate the deployment of a Llama-3.1-70B-Instruct model using TorchServe’s vLLM docker image.&lt;/p&gt;

&lt;h2 id=&quot;quickly-getting-started-with-llama-31-on-torchserve--vllm&quot;&gt;Quickly getting started with Llama 3.1 on TorchServe + vLLM&lt;/h2&gt;

&lt;p&gt;To get started we need to build the &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/docker/Dockerfile.llm&quot;&gt;new TS LLM Docker&lt;/a&gt; container image by checking out the &lt;a href=&quot;https://github.com/pytorch/serve&quot;&gt;TorchServe repository&lt;/a&gt; and execute the following command from the main folder:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build --pull . -f docker/Dockerfile.vllm -t ts/vllm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The container uses our new LLM launcher script &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts.llm_launcher&lt;/code&gt; which takes a Hugging Face model URI or local folder and spins up a local TorchServe instance with the vLLM engine running in the backend. To serve a model locally, you can create an instance of the container with the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#export token=&amp;lt;HUGGINGFACE_HUB_TOKEN&amp;gt;
docker run --rm -ti --shm-size 10g --gpus all -e HUGGING_FACE_HUB_TOKEN=$token -p 
8080:8080 -v data:/data ts/vllm --model_id meta-llama/Meta-Llama-3.1-70B-Instruct --disable_token_auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test the endpoint locally with this curl command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -X POST -d '{&quot;model&quot;:&quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;, &quot;prompt&quot;:&quot;Hello, my name is&quot;, &quot;max_tokens&quot;: 200}' --header &quot;Content-Type: application/json&quot; &quot;http://localhost:8080/predictions/model/1.0/v1/completions&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The docker stores the model weights in the local folder “data” which gets mounted as /data inside the container. To serve your custom local weights simply copy them into data and point the model_id to /data/&amp;lt;your weights&amp;gt;.&lt;/p&gt;

&lt;p&gt;Internally, the container uses our new &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ts.llm_launcher&lt;/code&gt; script to launch TorchServe and deploy the model. The launcher simplifies the deployment of an LLM with TorchServe into a single command line and can also be used outside the container as an efficient tool for experimentation and testing. To use the launcher outside the docker, follow the &lt;a href=&quot;https://github.com/pytorch/serve?tab=readme-ov-file#-quick-start-with-torchserve&quot;&gt;TorchServe installation steps&lt;/a&gt; and then execute the following command to spin up a 8B Llama model:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# after installing TorchServe and vLLM run
python -m ts.llm_launcher --model_id meta-llama/Meta-Llama-3.1-8B-Instruct  --disable_token_auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If multiple GPUs are available the launcher will automatically claim all visible devices and apply tensor parallelism (see &lt;a href=&quot;https://developer.nvidia.com/blog/cuda-pro-tip-control-gpu-visibility-cuda_visible_devices/&quot;&gt;CUDA_VISIBLE_DEVICES&lt;/a&gt; to specify which GPUs to use).&lt;/p&gt;

&lt;p&gt;While this is very convenient, it’s important to note that it does not encompass all the functionalities provided by TorchServe. For those looking to leverage more advanced features, a model archive needs to be created. While this process is a bit more involved than issuing a single command, it bears the advantage of custom handlers and versioning. While the former allows to implement RAG inside the preprocessing step, the latter lets you test different versions of a handler and model before deploying on a larger scale.&lt;/p&gt;

&lt;p&gt;Before we provide the detailed steps to create and deploy a model archive, let’s dive into the details of the vLLM engine integration.&lt;/p&gt;

&lt;h2 id=&quot;torchserves-vllm-engine-integration&quot;&gt;TorchServe’s vLLM Engine Integration&lt;/h2&gt;

&lt;p&gt;As a state-of-the-art serving framework, vLLM offers a plethora of advanced features, including PagedAttention, continuous batching, rapid model execution through CUDA graphs, and support for various quantization methods such as GPTQ, AWQ, INT4, INT8, and FP8. It also provides integration for important parameter-efficient adapter methods like LoRA and access to a wide range of model architectures including Llama and Mistral. vLLM is maintained by the vLLM team and a thriving open-source community.&lt;/p&gt;

&lt;p&gt;To facilitate quick deployment, it offers a serving mode based on FastAPI to serve LLMs over HTTP. For a tighter, more flexible integration the project also provides the &lt;a href=&quot;https://docs.vllm.ai/en/latest/dev/engine/llm_engine.html&quot;&gt;vllm.LLMEngine&lt;/a&gt; which offers interfaces to process requests on a continuous basis. We leveraged the &lt;a href=&quot;https://docs.vllm.ai/en/latest/dev/engine/async_llm_engine.html&quot;&gt;asynchronous variant&lt;/a&gt; for the integration into TorchServe.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/serve/&quot;&gt;TorchServe&lt;/a&gt; is an easy-to-use, open-source solution for serving PyTorch models in production. As a production-tested serving solution, TorchServe offers numerous benefits and features beneficial for deploying PyTorch models at scale. By combining it with the inference performance of the vLLM engine these benefits can now also be used to deploy LLMs at scale.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg2.png&quot; alt=&quot;Torchserve highlights and integrations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To maximize hardware utilization it is generally a good practice to batch requests from multiple users together. Historically, TorchServe only offered a synchronized mode to collect requests from various users. In this mode, TorchServe waits for a predefined amount of time (e.g., batch_delay=200ms) or until enough requests (e.g., batch_size=8) have arrived. When one of these events is triggered, the batched data gets forwarded to the backend where the model is applied to the batch, and the model output is returned to the users through the frontend. This works especially well for traditional vision models where outputs for each request usually finish at the same time.&lt;/p&gt;

&lt;p&gt;For generative use cases, particularly text generation, the assumption that requests are ready simultaneously is no longer valid, as responses will have varying lengths. Although TorchServe supports continuous batching (the ability to add and remove requests dynamically), this mode only accommodates a static maximum batch size. With the introduction of PagedAttention, even this assumption of a maximum batch size becomes more flexible, as vLLM can combine requests of different lengths in a highly adaptable manner to optimize memory utilization.&lt;/p&gt;

&lt;p&gt;To achieve optimal memory utilization, i.e., to fill unused gaps in memory (think Tetris), vLLM requires complete control over the decision of which requests to process at any given time. To provide this flexibility, we had to reevaluate how TorchServe handles user requests. Instead of the previous synchronous processing mode, we introduced an &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/examples/large_models/vllm/llama3/model-config.yaml#L7&quot;&gt;asynchronous mode&lt;/a&gt; (see diagram below) where incoming requests are directly forwarded to the backend, making them available for vLLM. The backend feeds the vllm.AsyncEngine, which can now select from all available requests. If streaming mode is enabled and the first token of a request is available, the backend will send out the result immediately and continue sending tokens until the final token is generated.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/deploying-llms-torchserve-vllm/fg3.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/serve/blob/master/ts/torch_handler/vllm_handler.py&quot;&gt;Our implementation of the VLLMHandler&lt;/a&gt; enables users to quickly deploy any model compatible with vLLM using a configuration file, while still offering the same level of flexibility and customizability through a custom handler. Users are free to add e.g. custom &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/ts/torch_handler/vllm_handler.py#L108&quot;&gt;preprocessing&lt;/a&gt; or &lt;a href=&quot;https://github.com/pytorch/serve/blob/ba8c268fe09cb9396749a9ae5d480ba252764d71/ts/torch_handler/vllm_handler.py#L160&quot;&gt;post-processing&lt;/a&gt; steps by inheriting from VLLMHandler and overriding the respective class methods.&lt;/p&gt;

&lt;p&gt;We also support single-node, multi-GPU &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/vllm/Readme.md#distributed-inference&quot;&gt;distributed inference&lt;/a&gt;, where we configure vLLM to use tensor parallel sharding of the model to either increase capacity for smaller models or enable larger models that do not fit on a single GPU, such as the 70B Llama variants. Previously, TorchServe only supported distributed inference using torchrun, where multiple backend worker processes were spun up to shard the model. vLLM manages the creation of these processes internally, so we &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/examples/large_models/vllm/Readme.md#distributed-inference&quot;&gt;introduced the new “custom” parallelType to TorchServe&lt;/a&gt; which launches a single backend worker process and provides the list of assigned GPUs. The backend process can then launch its own subprocesses if necessary.&lt;/p&gt;

&lt;p&gt;To facilitate integration of TorchServe + vLLM into docker-based deployments, we provide a separate &lt;a href=&quot;https://github.com/pytorch/serve?tab=readme-ov-file#-quick-start-llm-deployment-with-docker&quot;&gt;Dockerfile&lt;/a&gt; based on &lt;a href=&quot;https://hub.docker.com/r/pytorch/torchserve&quot;&gt;TorchServe’s GPU docker image&lt;/a&gt;, with vLLM added as a dependency. We chose to keep the two separate to avoid increasing the docker image size for non-LLM deployments.&lt;/p&gt;

&lt;p&gt;Next, we will demonstrate the steps required to deploy a Llama 3.1 70B model using TorchServe + vLLM on a machine with four GPUs.&lt;/p&gt;

&lt;h2 id=&quot;step-by-step-guide&quot;&gt;Step-by-Step Guide&lt;/h2&gt;

&lt;p&gt;For this step-by-step guide we assume the &lt;a href=&quot;https://github.com/pytorch/serve/tree/master?tab=readme-ov-file#-quick-start-with-torchserve&quot;&gt;installation of TorchServe&lt;/a&gt; has finished successfully. Currently, vLLM is not a hard-dependency for TorchServe so let’s install the package using pip:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ pip install -U vllm==0.6.1.post2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the following steps, we will (optionally) download the model weights, explain the configuration, create a model archive, deploy and test it:&lt;/p&gt;

&lt;h3 id=&quot;1-optional-download-model-weights&quot;&gt;1. (Optional) Download Model Weights&lt;/h3&gt;

&lt;p&gt;This step is optional, as vLLM can also handle downloading the weights when the model server is started. However, pre-downloading the model weights and sharing the cached files between TorchServe instances can be beneficial in terms of storage usage and startup time of the model worker. If you choose to download the weights, use the huggingface-cli and execute:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# make sure you have logged into huggingface with huggingface-cli login before
# and have your access request for the Llama 3.1 model weights approved

huggingface-cli download meta-llama/Meta-Llama-3.1-70B-Instruct --exclude original/*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will download the files under $HF_HOME, and you can alter the variable if you want to place the files elsewhere. Please ensure that you update the variable wherever you run TorchServe and make sure it has access to that folder.&lt;/p&gt;

&lt;h3 id=&quot;2-configure-the-model&quot;&gt;2. Configure the Model&lt;/h3&gt;

&lt;p&gt;Next, we create a YAML configuration file that contains all the necessary parameters for our model deployment. The first part of the config file specifies how the frontend should launch the backend worker, which will ultimately run the model in a handler. The second part includes parameters for the backend handler, such as the model to load, followed by various parameters for vLLM itself. For more information on possible configurations for the vLLM engine, please refer to this &lt;a href=&quot;https://docs.vllm.ai/en/latest/models/engine_args.html#engine-args&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo '
# TorchServe frontend parameters
minWorkers: 1            
maxWorkers: 1            # Set the number of worker to create a single model instance
startupTimeout: 1200     # (in seconds) Give the worker time to load the model weights
deviceType: &quot;gpu&quot; 
asyncCommunication: true # This ensures we can cummunicate asynchronously with the worker
parallelType: &quot;custom&quot;   # This lets TS create a single backend prosses assigning 4 GPUs
parallelLevel: 4

# Handler parameters
handler:
    # model_path can be a model identifier for Hugging Face hub or a local path
    model_path: &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;
    vllm_engine_config:  # vLLM configuration which gets fed into AsyncVLLMEngine
        max_num_seqs: 16
        max_model_len: 512
        tensor_parallel_size: 4
        served_model_name:
            - &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;
            - &quot;llama3&quot;
'&amp;gt; model_config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3-create-the-model-folder&quot;&gt;3. Create the Model Folder&lt;/h3&gt;

&lt;p&gt;After creating the model configuration file (model_config.yaml), we will now create a model archive that includes the configuration and additional metadata, such as versioning information. Since the model weights are large, we will not include them inside the archive. Instead, the handler will access the weights by following the model_path specified in the model configuration. Note that in this example, we have chosen to use the “no-archive” format, which creates a model folder containing all necessary files. This allows us to easily modify the config files for experimentation without any friction. Later, we can also select the mar or tgz format to create a more easily transportable artifact.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir model_store
torch-model-archiver --model-name vllm --version 1.0 --handler vllm_handler --config-file model_config.yaml --archive-format no-archive --export-path model_store/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-deploy-the-model&quot;&gt;4. Deploy the Model&lt;/h3&gt;

&lt;p&gt;The next step is to start a TorchServe instance and load the model. Please note that we have disabled token authentication for local testing purposes. It is highly recommended to implement some form of authentication when publicly deploying any model.&lt;/p&gt;

&lt;p&gt;To start the TorchServe instance and load the model, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torchserve --start --ncs  --model-store model_store --models vllm --disable-token-auth
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can monitor the progress of the model loading through the log statements. Once the model has finished loading, you can proceed to test the deployment.&lt;/p&gt;

&lt;h3 id=&quot;5-test-the-deployment&quot;&gt;5. Test the Deployment&lt;/h3&gt;

&lt;p&gt;The vLLM integration uses an OpenAI API compatible format so we can either use a specialized tool for this purpose or curl. The JSON data we are using here includes the model identifier as well as the prompt text. Other options and their default values can be found in the &lt;a href=&quot;https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html&quot;&gt;vLLMEngine&lt;/a&gt; docs.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;echo '{
  &quot;model&quot;: &quot;llama3&quot;,
  &quot;prompt&quot;: &quot;A robot may not injure a human being&quot;,
  &quot;stream&quot;: 0
}' | curl --header &quot;Content-Type: application/json&quot;   --request POST --data-binary @-   http://localhost:8080/predictions/vllm/1.0/v1/completions
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output of the request looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
  &quot;id&quot;: &quot;cmpl-cd29f1d8aa0b48aebcbff4b559a0c783&quot;,
  &quot;object&quot;: &quot;text_completion&quot;,
  &quot;created&quot;: 1727211972,
  &quot;model&quot;: &quot;meta-llama/Meta-Llama-3.1-70B-Instruct&quot;,
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;text&quot;: &quot; or, through inaction, allow a human being to come to harm.\nA&quot;,
      &quot;logprobs&quot;: null,
      &quot;finish_reason&quot;: &quot;length&quot;,
      &quot;stop_reason&quot;: null,
      &quot;prompt_logprobs&quot;: null
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 10,
    &quot;total_tokens&quot;: 26,
    &quot;completion_tokens&quot;: 16
  }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When streaming is False TorchServe will collect the full answer and send it in one go after the last token was created. If we flip the stream parameter we will receive piecewise data containing a single token in each message.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this blog post, we explored the new, native integration of the vLLM inference engine into TorchServe. We demonstrated how to locally deploy a Llama 3.1 70B model using the ts.llm_launcher script and how to create a model archive for deployment on any TorchServe instance. Additionally, we discussed how to build and run the solution in a Docker container for deployment on Kubernetes or EKS. In future works, we plan to enable multi-node inference with vLLM and TorchServe, as well as offer a pre-built Docker image to simplify the deployment process.&lt;/p&gt;

&lt;p&gt;We would like to express our gratitude to Mark Saroufim and the vLLM team for their invaluable support in the lead-up to this blog post.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Matthias Reso, Ankith Gunapal, Simon Mo, Li Ning, Hamid Shojanazeri</name>
        
        
      </author>

      

      

      
        <summary type="html">The vLLM engine is currently one of the top-performing ways to execute large language models (LLM). It provides the vllm serve command as an easy option to deploy a model on a single machine. While this is convenient, to serve these LLMs in production and at scale some advanced features are necessary.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Triton Kernel Compilation Stages</title>
      <link href="https://pytorch.org/blog/triton-kernel-compilation-stages/" rel="alternate" type="text/html" title="Triton Kernel Compilation Stages" />
      <published>2024-10-30T00:00:00-07:00</published>
      <updated>2024-10-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/triton-kernel-compilation-stages</id>
      <content type="html" xml:base="https://pytorch.org/blog/triton-kernel-compilation-stages/">&lt;p&gt;The Triton open-source programming language and compiler offers a high-level, python-based approach to create efficient GPU code. In this blog, we highlight the underlying details of how a triton program is compiled and the intermediate representations. For an introduction to Triton, we refer readers to this &lt;a href=&quot;https://openai.com/index/triton/&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;triton-language-and-compilation&quot;&gt;Triton Language and Compilation&lt;/h2&gt;

&lt;p&gt;The Triton programming language supports different types of modern GPUs and follows a blocked programming approach. As an example, we will follow the &lt;a href=&quot;https://github.com/triton-lang/triton/blob/main/python/tutorials/01-vector-add.py&quot;&gt;Triton vector add tutorial&lt;/a&gt; with minor modifications. The vector addition kernel and helper function is defined as:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
import triton
import triton.language as tl

@triton.jit
def add_kernel(x_ptr,  # *Pointer* to first input vector.
               y_ptr,  # *Pointer* to second input vector.
               output_ptr,  # *Pointer* to output vector.
               n_elements, 
               BLOCK_SIZE: tl.constexpr, 
               ):
  
    pid = tl.program_id(axis=0) 
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
 
    mask = offsets &amp;lt; n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)
 
def add(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    assert x.is_cuda and y.is_cuda and output.is_cuda
    n_elements = output.numel()

    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
    triton_kernel=add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)
    torch.cuda.synchronize()

    # Save compilation stages - some of the stages identified here are specific to NVIDIA devices:
    with open('triton_IR.txt', 'w') as f:
        print(triton_kernel.asm['ttir'], file=f)
    with open('triton_TTGIR.txt', 'w') as f:
        print(triton_kernel.asm['ttgir'], file=f)
    with open('triton_LLVMIR.txt', 'w') as f:
        print(triton_kernel.asm['llir'], file=f)
    with open('triton_PTX.ptx', 'w') as f:
        print(triton_kernel.asm['ptx'], file=f)
    with open('triton_cubin.txt', 'w') as f:
        print(triton_kernel.asm['cubin'], file=f)

    return output

torch.manual_seed(0)
size = 98432
x = torch.rand(size, device='cuda')
y = torch.rand(size, device='cuda')
output_torch = x + y
output_triton = add(x, y)
print(output_torch)
print(output_triton)
print(f'The maximum difference between torch and triton is '
      f'{torch.max(torch.abs(output_torch - output_triton))}')    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Triton vector add kernel includes the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@triton.jit&lt;/code&gt; decorator. The Triton compiler will compile functions marked by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@triton.jit&lt;/code&gt;, which lowers the function through multiple compilation stages. The helper function &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; allocates the output tensor, computes the appropriate GPU grid size, and additionally saves the intermediate compilation stages.&lt;/p&gt;

&lt;p&gt;Focusing on the compilation process, the Triton kernel is lowered to device specific assembly through a series of stages outlined in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/triton-kernel-compilation-stages.jpg&quot; alt=&quot;compilation process&quot; style=&quot;width:100%; max-width: 500px; margin-left: auto; margin-right: auto; display: block&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The kernel is compiled by first walking the abstract syntax tree (AST) of the decorated python function to create the Triton Intermediate Representation (Triton-IR). The Triton-IR is an unoptimized, machine independent intermediate representation. It introduces tile-level programming requirements and is based on the open-source LLVM compiler project. Next the Triton compiler optimizes and converts the Triton-IR into the stages Triton-GPU IR (Triton-TTGIR) and then LLVM-IR. Both the Triton-IR and Triton-GPUIR representations are written as MLIR dialects, where MLIR is a subproject of LLVM that aims to improve compilation for heterogeneous hardware.&lt;/p&gt;

&lt;p&gt;For the Triton vector add tutorial kernel, the example Triton IR snippet is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;module {
  tt.func public @add_kernel(%arg0: !tt.ptr&amp;lt;f32&amp;gt; {tt.divisibility = 16 : i32} loc(&quot;/u/saraks/triton_blog/01-vector-add.py&quot;:28:0), %arg1: !tt.ptr&amp;lt;f32&amp;gt; {tt.divisibility = 16 : i32} loc(&quot;/u/saraks/triton_blog/01-vector-add.py&quot;:28:0), %arg2: !tt.ptr&amp;lt;f32&amp;gt; {tt.divisibility = 16 : i32} loc(&quot;/u/saraks/triton_blog/01-vector-add.py&quot;:28:0), %arg3: i32 {tt.divisibility = 16 : i32} loc(&quot;/u/saraks/triton_blog/01-vector-add.py&quot;:28:0)) attributes {noinline = false} {
    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)
    %0 = tt.get_program_id x : i32 loc(#loc2)
    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)
    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor&amp;lt;1024xi32&amp;gt; loc(#loc4)
    %3 = tt.splat %1 : i32 -&amp;gt; tensor&amp;lt;1024xi32&amp;gt; loc(#loc5)
    %4 = arith.addi %3, %2 : tensor&amp;lt;1024xi32&amp;gt; loc(#loc5)
    %5 = tt.splat %arg3 : i32 -&amp;gt; tensor&amp;lt;1024xi32&amp;gt; loc(#loc6)
    %6 = arith.cmpi slt, %4, %5 : tensor&amp;lt;1024xi32&amp;gt; loc(#loc6)
    %7 = tt.splat %arg0 : !tt.ptr&amp;lt;f32&amp;gt; -&amp;gt; tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc7)
    %8 = tt.addptr %7, %4 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt;, tensor&amp;lt;1024xi32&amp;gt; loc(#loc7)
    %9 = tt.load %8, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc8)
    %10 = tt.splat %arg1 : !tt.ptr&amp;lt;f32&amp;gt; -&amp;gt; tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc9)
    %11 = tt.addptr %10, %4 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt;, tensor&amp;lt;1024xi32&amp;gt; loc(#loc9)
    %12 = tt.load %11, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc10)
    %13 = arith.addf %9, %12 : tensor&amp;lt;1024xf32&amp;gt; loc(#loc11)
    %14 = tt.splat %arg2 : !tt.ptr&amp;lt;f32&amp;gt; -&amp;gt; tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc12)
    %15 = tt.addptr %14, %4 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt;, tensor&amp;lt;1024xi32&amp;gt; loc(#loc12)
    tt.store %15, %13, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc13)
    tt.return loc(#loc14)
  } loc(#loc)
} loc(#loc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice that the main functions in the Triton kernel are now represented as:&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Triton kernel&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Triton IR&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;x = tl.load(x_ptr + offsets, mask=mask)
   &lt;/td&gt;
   &lt;td&gt;%9 = tt.load %8, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc8)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;y = tl.load(y_ptr + offsets, mask=mask)
   &lt;/td&gt;
   &lt;td&gt;%12 = tt.load %11, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc10)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;output = x + y 
   &lt;/td&gt;
   &lt;td&gt;%13 = arith.addf %9, %12 : tensor&amp;lt;1024xf32&amp;gt; loc(#loc11)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;tl.store(output_ptr + offsets, output, mask=mask)
   &lt;/td&gt;
   &lt;td&gt;tt.store %15, %13, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;&amp;gt; loc(#loc13)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;At the Triton IR stage, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%arg0: !tt.ptr&amp;amp;lt;f32&amp;gt;&lt;/code&gt; and the following tensor references show that the intermediate representation is already specialized by the data type.&lt;/p&gt;

&lt;p&gt;We ran this example on a Tesla V100-SXM2-32GB GPU with CUDA Version 12.2, Python version 3.11.9, and PyTorch 2.4.1 with the default version of Triton that is installed with PyTorch. On this device, the simple vector addition has the following Triton GPU IR snippet with lines omitted for clarity:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#blocked = #triton_gpu.blocked&amp;lt;{sizePerThread = [4], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]}&amp;gt;
module attributes {&quot;triton_gpu.num-ctas&quot; = 1 : i32, &quot;triton_gpu.num-warps&quot; = 4 : i32, triton_gpu.target = &quot;cuda:70&quot;, &quot;triton_gpu.threads-per-warp&quot; = 32 : i32} {
  tt.func public @add_kernel(%arg0: !tt.ptr&amp;lt;f32&amp;gt; {tt.divisibility = 16 : i32}
    ⋮
    %9 = tt.load %8, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;, #blocked&amp;gt; loc(#loc8)
    ⋮
    %12 = tt.load %11, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;, #blocked&amp;gt; loc(#loc10)
    %13 = arith.addf %9, %12 : tensor&amp;lt;1024xf32, #blocked&amp;gt; loc(#loc11)
    ⋮
    tt.store %15, %13, %6 : tensor&amp;lt;1024x!tt.ptr&amp;lt;f32&amp;gt;, #blocked&amp;gt; loc(#loc13)
    ⋮
  } loc(#loc)
} loc(#loc)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this stage, some of the hardware specific information is included. For example, the compute capability is included along with details on how the tensors are distributed to cores and warps or for AMD GPUs on wavefronts. In this example, the tensors are represented as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#blocked&lt;/code&gt; layout. In this encoding, each warp owns a contiguous portion of the tensor. Currently, other possible memory optimizations include layouts such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;slice&lt;/code&gt; (restructures and distributes a tensor along a dimension), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dot_op&lt;/code&gt;(optimized layout for block matrix product), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shared&lt;/code&gt;(indicates GPU shared memory), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvidia_mma&lt;/code&gt; (produced by NVIDIA tensor cores), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;amd_mfma&lt;/code&gt; (produced by AMD MFMA matrix core), and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;amd_wmma&lt;/code&gt; (produced by AMD WMMA matrix core). As announced at the recent Triton conference, this layout representation will transition to a new linear layout to unify layouts within and across backends. The stage from Triton-GPUIR to LLVM-IR converts the Triton-GPUIR to LLVM’s representation. At this time, Triton has third-party backend support for NVIDIA and AMD devices, but other device support is under active development by the open-source community.&lt;/p&gt;

&lt;p&gt;A small subset of the LLVM-IR vector add arguments shown below for illustration:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  %19 = extractvalue { i32, i32, i32, i32 } %18, 0, !dbg !16
  %39 = extractvalue { i32, i32, i32, i32 } %38, 0, !dbg !18
  %23 = bitcast i32 %19 to float, !dbg !16
  %43 = bitcast i32 %39 to float, !dbg !18
  %56 = fadd float %23, %43, !dbg !19
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After some pointer arithmetic and an inline assembly call to retrieve the data from global memory, the vector elements are extracted and cast to the correct type. Finally they are added together and later written to global memory through an inline assembly expression.&lt;/p&gt;

&lt;p&gt;The final stages of the Triton compilation process lower the LLVM-IR to a device specific binary. For the example vector add, on an NVIDIA GPU, the next intermediate is PTX (Parallel Thread Execution). The low-level PTX syntax specifies the execution at the thread level of NVIDIA devices, starting with the CUDA 1.0 release. For an in-depth guide on PTX, see &lt;a href=&quot;https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#&quot;&gt;NVIDIA’s documentation&lt;/a&gt;. In the vector add, the kernel parameters are passed from the host to the kernel, addresses are assigned and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; instructions facilitate the thread-level data access, ultimately representing the element addition calls with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add.f32&lt;/code&gt; such as the example below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	add.f32 	%f17, %f1, %f9// add type float32, output register, input register for x, input register for y 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Triton compiler orchestrates the final stage with different hardware backends managing how the assembly code is compiled into binary. The Triton kernel is now ready for use.&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Triton provides a high-level abstraction to program and compile kernels for different types of hardware. In this post, we highlight the different stages of the Triton code representations and Triton compiler. For details on including custom Triton kernels or accelerating different workloads with Triton kernels, check out the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;PyTorch Triton tutorial&lt;/a&gt;, the blog posts on &lt;a href=&quot;https://pytorch.org/blog/accelerating-triton&quot;&gt;Triton GPTQ kernels&lt;/a&gt;, &lt;a href=&quot;https://pytorch.org/blog/accelerating-llama3/&quot;&gt;Llama3 FP8 Inference with Triton&lt;/a&gt;, and &lt;a href=&quot;https://pytorch.org/blog/cuda-free-inference-for-llms/&quot;&gt;CUDA-Free Inference for LLMs&lt;/a&gt;, or the &lt;a href=&quot;https://pytorch.org/assets/pytorch2-2.pdf&quot;&gt;PyTorch 2.2 Section on Triton code generation&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sara Kokkila-Schumacher*, Brian Vaughan*, Raghu Ganti*, and Less Wright+ (*IBM Research, +Meta)</name>
        
        
      </author>

      

      

      
        <summary type="html">The Triton open-source programming language and compiler offers a high-level, python-based approach to create efficient GPU code. In this blog, we highlight the underlying details of how a triton program is compiled and the intermediate representations. For an introduction to Triton, we refer readers to this blog.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Getting started with PyTorch, ExecuTorch, and Ethos-U85 in three easy steps</title>
      <link href="https://pytorch.org/blog/pt-executorch-ethos-u85/" rel="alternate" type="text/html" title="Getting started with PyTorch, ExecuTorch, and Ethos-U85 in three easy steps" />
      <published>2024-10-28T00:00:00-07:00</published>
      <updated>2024-10-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-executorch-ethos-u85</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-executorch-ethos-u85/">&lt;h2 id=&quot;executorch-support-for-ethos-u85&quot;&gt;ExecuTorch support for Ethos-U85&lt;/h2&gt;

&lt;p&gt;In the rapidly evolving landscape of machine learning, PyTorch has emerged as a leading framework for model development, given its flexibility and comprehensive ecosystem. Arm has worked with Meta to &lt;a href=&quot;https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/executorch-and-tosa-enabling-pytorch-on-arm-platforms&quot;&gt;introduce support for Arm platforms in ExecuTorch&lt;/a&gt;, that further simplifies this process, making it seamless to deploy PyTorch models on edge devices.&lt;/p&gt;

&lt;p&gt;The Arm Ethos-U85 NPU is the highest performing Ethos NPU addressing the growing demand for running advanced AI inference workloads at the edge, including transformer-based networks like LLMs. Arm offers reference designs, including the Corstone-320 IoT reference design platform, around the Ethos-U to accelerate and simplify the chip development cycle. The reference design platform includes, among many items, a Fixed Virtual Platform (FVP) that simulates an entire system, enabling cutting edge embedded software development and neural network deployment for the Ethos-U85.&lt;/p&gt;

&lt;p&gt;Today, Arm is extending the support for developers building IoT edge applications, by supporting ExecuTorch beta on Ethos-U85. Leveraging ExecuTorch, developers can now efficiently land their natively developed PyTorch models to enable intelligent and responsive IoT solutions built on Arm.&lt;/p&gt;

&lt;p&gt;With this package now available, thousands of developers looking to create Edge AI applications, can start their model and application development months before the platforms arrive on the market.&lt;/p&gt;

&lt;h2 id=&quot;getting-started-with-executorch-on-ethos-u85&quot;&gt;Getting started with ExecuTorch on Ethos-U85&lt;/h2&gt;

&lt;p&gt;A full development environment has been provided in the public ExecuTorch GitHub repository. This provides an integrated and tested development flow with all necessary components.&lt;/p&gt;

&lt;p&gt;The three simple steps are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/executorch/main/getting-started-setup.html&quot;&gt;Set up ExecuTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/executorch/main/executorch-arm-delegate-tutorial.html&quot;&gt;Set up the Arm Build environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/executorch/main/executorch-arm-delegate-tutorial.html#delegated-quantized-workflow&quot;&gt;Compile and Run models on the arm_executor_runner&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can then build on this flow for compiling and running models, to capture runtime behavior from the Ethos-U85 driver, such as cycle count information.&lt;/p&gt;

&lt;p&gt;To make the process easier for end users, we have also added scripts to the ExecuTorch repository:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/executorch/main/getting-started-setup.html&quot;&gt;Set up ExecuTorch&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/arm/setup.sh&quot;&gt;setup.sh&lt;/a&gt;: Download the necessary software.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/arm/run.sh&quot;&gt;run.sh&lt;/a&gt;: to compile and run the model on the Corstone-320 FVP&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To build other models, you can use the ahead of time compiler script  &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/arm/aot_arm_compiler.py&quot;&gt;aot_arm_compiler.py,&lt;/a&gt; which takes a PyTorch program (nn.module) to an ExecuTorch program (.pte flatbuffer file). To write custom applications which use ExecuTorch you can follow the application flow in the example &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/arm/executor_runner&quot;&gt;executor_runner&lt;/a&gt; application.&lt;/p&gt;

&lt;p&gt;We support approximately 40 core ATen operators and already support end-to-end deployment of models such as Mobilenetv2. Ongoing efforts to support further operators will enable more PyTorch models every week .&lt;/p&gt;

&lt;p&gt;As more functionality is added, it will be demonstrated through the tutorial materials for Ethos-U on &lt;a href=&quot;https://pytorch.org/executorch/main/index.html&quot;&gt;pytorch.org&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-this-deployment-flow-works-in-more-detail&quot;&gt;How this deployment flow works in more detail&lt;/h2&gt;

&lt;p&gt;Leveraging the extensibility of ExecuTorch and the expressiveness of Arm’s &lt;a href=&quot;https://www.mlplatform.org/tosa/&quot;&gt;Tensor Operator Set Architecture (TOSA)&lt;/a&gt;, we have enabled Ethos-U support in ExecuTorch. The Ethos-U compiler, &lt;a href=&quot;https://pypi.org/project/ethos-u-vela/&quot;&gt;Vela&lt;/a&gt;, has been enhanced with a TOSA front-end, making it possible to compile models for all products in the Ethos-U family. Combining these components into a cohesive workflow involves the following steps.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Converting a PyTorch model into a deployable ExecuTorch program (AOT flow)&lt;/li&gt;
  &lt;li&gt;Compile the ExecuTorch program into an executable, which can be deployed on Corstone-320 (runtime flow)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;the-executorch-ahead-of-time-aot-flow&quot;&gt;The ExecuTorch Ahead of time (AOT) flow&lt;/h3&gt;

&lt;p&gt;The process begins by converting a PyTorch model into a quantized TOSA representation using the PyTorch dynamo export flow. This allows us to generate an Ethos-U set of machine instructions, known as a command stream, utilizing the Vela compiler TOSA frontend. The command stream is bundled into an ExecuTorch program, represented by a flatbuffer file (.pte). This file contains everything the ExecuTorch runtime needs to perform inference using Ethos-U hardware.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-executorch-ethos-u85/fg1.jpg&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-executorch-runtime-flow&quot;&gt;The ExecuTorch Runtime flow&lt;/h3&gt;

&lt;p&gt;The ExecuTorch runtime, written in C/C++, is designed to support multiple backends. We have extended it to include support for the Ethos-U device driver. Following this flow will produce a self-contained compiled executable. Deploying the executable on the Corstone-320 FVP is straightforward and requires only the appropriate flags when calling the FVP.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-executorch-ethos-u85/fg2.jpg&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ethos-u85-and-corstone-320&quot;&gt;Ethos-U85 and Corstone-320&lt;/h2&gt;

&lt;p&gt;The Ethos-U family of NPUs offers high performance and energy-efficient solutions for edge AI. The Ethos-U55 (also supported by ExecuTorch) is widely deployed in many Cortex-M heterogeneous systems, while the Ethos-U65 extends the applicability of the Ethos-U family to Cortex-A-based systems and increases the performance.&lt;/p&gt;

&lt;p&gt;Ethos-U85 further extends the Ethos-U product line, supporting current and future workloads on the edge using &lt;a href=&quot;https://newsroom.arm.com/blog/enabling-next-gen-edge-ai-applications-with-transformer-networks&quot;&gt;transformer-based networks&lt;/a&gt;. Ethos-U85 delivers a 4x performance uplift and 20% higher energy efficiency compared to its predecessor, with up to 85% utilization on popular networks. Notable feature of Ethos-U85 includes;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;configurations from 128 to 2048 MACs/cycle, delivering up 4 TOP/s at 1GHz&lt;/li&gt;
  &lt;li&gt;Compatible with Cortex-A and Cortex-M based systems&lt;/li&gt;
  &lt;li&gt;Native support for major neural networks though support for TOSA&lt;/li&gt;
  &lt;li&gt;Full hardware acceleration of all major neural networks&lt;/li&gt;
  &lt;li&gt;For a full list of features, see the &lt;a href=&quot;https://developer.arm.com/documentation/102684/0000&quot;&gt;Ethos-U85 Technical Overview&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pt-executorch-ethos-u85/fg3.png&quot; alt=&quot;A typical compute subsystem design with Ethos-U85&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A typical compute subsystem design with Ethos-U85&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;We are adding new operator support every week, extending ExecuTorch core ATen operator coverage, and enabling a wider range of models to run on Ethos-U. Our ongoing efforts focus on improving performance to ensure models run as optimally as possible on Ethos-U.&lt;/p&gt;

&lt;p&gt;The ExecuTorch delegate framework supports fallback to running operators not supported by Ethos-U on the CPU using reference kernel implementations. We will work towards optimal performance on Cortex-M CPUs using CMSIS-NN, providing the best possible support for fallback operators and ensuring optimal performance for devices without Ethos-U capability.&lt;/p&gt;

&lt;p&gt;The package above with the Corstone-320 FVP are more steps to simplify application development, so please, go ahead, check out the code and build process and send us feedback. Meanwhile we will be busy making weekly releases to enable more features, models and to extract the maximum performance out of the hardware.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Robert Elliott, Fredrik Knutsson, and Mark Quartermain</name>
        
        
      </author>

      

      

      
        <summary type="html">ExecuTorch support for Ethos-U85</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Unleashing the Power of AI on Mobile: LLM Inference for Llama 3.2 Quantized Models with ExecuTorch and KleidiAI</title>
      <link href="https://pytorch.org/blog/unleashing-ai-mobile/" rel="alternate" type="text/html" title="Unleashing the Power of AI on Mobile: LLM Inference for Llama 3.2 Quantized Models with ExecuTorch and KleidiAI" />
      <published>2024-10-28T00:00:00-07:00</published>
      <updated>2024-10-28T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/unleashing-ai-mobile</id>
      <content type="html" xml:base="https://pytorch.org/blog/unleashing-ai-mobile/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;At the recent &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference/&quot;&gt;PyTorch Conference&lt;/a&gt;, Arm highlighted the widespread impact of its technology, spanning from cloud to edge, emphasizing its commitment to delivering its advanced AI computing capabilities seamlessly to millions of developers worldwide.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg1.png&quot; alt=&quot;key stats&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the presentation, it was emphasized that Arm bears the immense responsibility of equipping 20+ million developers and billions of users with advanced AI computing features without friction. Achieving this requires crucial software collaborations across a vast ecosystem of software and hardware partners.&lt;/p&gt;

&lt;p&gt;Just a few months ago, Arm launched Arm Kleidi, developer enablement technologies and resources to drive technical collaboration and innovation across the ML stack. This includes the KleidiAI software library providing optimized software routines, which when integrated into key frameworks such as XNNPACK enable automatic AI acceleration for developers on Arm Cortex-A CPUs.&lt;/p&gt;

&lt;p&gt;Today, we’re excited to announce a new milestone for the AI open-source community that brings Arm even closer to realizing this vision: the integration of  KleidiAI  into &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch&lt;/a&gt; via XNNPACK, boosting AI workload performance on Arm mobile CPUs!&lt;/p&gt;

&lt;p&gt;Thanks to the collaborative efforts of the engineering teams at Arm and Meta, AI developers can now deploy quantized Llama models which run up to 20% faster on Arm Cortex-A v9 CPUs with the i8mm ISA extension.&lt;/p&gt;

&lt;p&gt;And there’s more exciting news - the ExecuTorch team has officially launched the &lt;a href=&quot;https://pytorch.org/blog/executorch-beta/&quot;&gt;Beta release&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;This marks an important milestone in our partnership. In this blog, we are eager to share more details about ExecuTorch capabilities, the new Meta Llama 3.2 models, the integer 4-bit with per-block quantization, and the impressive performance recorded on certain Arm CPUs. Notably, we have achieved speeds of over 350 tokens per second on the prefill stage with the quantized Llama 3.2 1B model on Samsung S24+ device, as shown in the following screenshots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg2.png&quot; alt=&quot;mobile app screenshots&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, let’s dive into the key components that enabled the demo creation presented in the preceding images. First up: new Llama 3.2 models!&lt;/p&gt;

&lt;h2 id=&quot;meta-llama-32&quot;&gt;Meta Llama 3.2&lt;/h2&gt;

&lt;p&gt;Meta recently &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/&quot;&gt;announced&lt;/a&gt; the first lightweight quantized Llama models, which are designed to run on popular mobile devices. Meta used two techniques for quantizing Llama 3.2 1B and 3B models: Quantization-Aware Training (QAT) with LoRA adaptors (QLoRA), and SpinQuant, a state-of-the-art post-training quantization method. The quantized models were evaluated using PyTorch’s ExecuTorch framework as the inference engine, with the Arm CPU as a backend.&lt;/p&gt;

&lt;p&gt;These instruction-tuned models retain the quality and safety of the original 1B and 3B models while achieving a 2-4x speedup and reducing model size by 56% on average and memory footprint by 41% on average compared to the original BF16 format.&lt;/p&gt;

&lt;p&gt;In this blog post, we will demonstrate the performance improvements we observed in our experiments.&lt;/p&gt;

&lt;h2 id=&quot;executorch&quot;&gt;ExecuTorch&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch&lt;/a&gt; is a PyTorch-native framework specifically designed for deploying AI models on-device, enhancing privacy and reducing latency. It supports the deployment of cutting-edge open-source AI models, including the Llama family of models and vision and speech models like &lt;a href=&quot;https://segment-anything.com/&quot;&gt;Segment Anything&lt;/a&gt; and &lt;a href=&quot;https://ai.meta.com/research/seamless-communication/&quot;&gt;Seamless&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This unlocks new possibilities for edge devices such as mobile phones, smart glasses, VR headsets, and smart home cameras. Traditionally, deploying PyTorch-trained AI models to resource-limited edge devices has been challenging and time-consuming, often requiring conversion to other formats which could lead to errors and suboptimal performance. The varied toolchains across the hardware and edge ecosystem have also degraded the developer experience, making a universal solution impractical.&lt;/p&gt;

&lt;p&gt;ExecuTorch addresses these issues by providing composable components that include core runtime, operator library, and delegation interface that allows for portability as well extensibility. Models can be exported using torch.export(), producing a graph that is natively compatible with the ExecuTorch runtime, capable of running on most edge devices with CPUs, and extendable to specialized hardware like GPUs and NPUs for enhanced performance.&lt;/p&gt;

&lt;p&gt;Working with Arm, ExecuTorch now leverages the optimized low-bit matrix multiplication kernels from the Arm KleidiAI library to improve on-device Large Language Model (LLM) inference performance via XNNPACK. We also thank the XNNPACK team at Google for supporting this effort.&lt;/p&gt;

&lt;p&gt;In this post, we will focus on this integration available in &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/models/llama2/README.md&quot;&gt;ExecuTorch&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;evolving-the-architecture-for-ai-workloads&quot;&gt;Evolving the architecture for AI workloads&lt;/h2&gt;

&lt;p&gt;At Arm, we have been deeply committed to investing in open-source projects and advancing new technologies in our processors since the early days of the deep learning wave, focusing on making AI workloads high-performing and more power-efficient.&lt;/p&gt;

&lt;p&gt;For instance, Arm introduced the SDOT instruction, starting with the Armv8.2-A architecture, to accelerate dot product arithmetic between 8-bit integer vectors. This feature, now widely available in mobile devices, significantly speeds up the computation of quantized 8-bit models. After the SDOT instruction, Arm introduced the BF16 data type and the MMLA instruction to further enhance the floating-point and integer matrix multiplication performance on CPUs and, most recently, announced the Scalable Matrix Extension (SME), marking a significant leap forward in machine learning capabilities.&lt;/p&gt;

&lt;p&gt;The following image shows a few examples of Arm CPU’s continuous innovations in the AI space over the last decade:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg3.jpg&quot; alt=&quot;line chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the widespread use of Arm CPUs, AI frameworks need to take full advantage of these technologies in key operators to maximize performance. Recognizing this, we saw the need for an open-source library to share these optimized software routines. However, we were mindful of the challenges in integrating a new library into AI frameworks, such as concerns about library size, dependencies, and documentation and the need to avoid adding extra burdens for developers. So, we took extra steps to gather feedback from our partners and ensure a smooth integration process that does not require additional dependencies for AI developers. This effort led to KleidiAI, an open-source library that provides optimized performance-critical routines for artificial intelligence (AI) workloads tailored for Arm CPUs. You can learn more about KleidiAI &lt;a href=&quot;https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/kleidiai&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Working with the ExecuTorch team at Meta, Arm provided the software optimizations for their novel 4-bit with per-block quantization schema, which is used to accelerate the matrix multiplication kernel in the Transformer layer’s torch.nn.linear operator for Llama 3.2 quantized models. This flexible 4-bit quantization schema from ExecuTorch strikes a balance between model accuracy and low-bit matrix multiplication performance targeting on-device LLMs.&lt;/p&gt;

&lt;h2 id=&quot;the-integer-4-bit-with-per-block-quantization&quot;&gt;The integer 4-bit with per-block quantization&lt;/h2&gt;

&lt;p&gt;In KleidiAI, we introduced micro-kernels optimized for this new 4-bit integer quantization scheme (&lt;strong&gt;matmul_clamp_f32_qai8dxp_qsi4c32p&lt;/strong&gt;)&lt;/p&gt;

&lt;p&gt;As shown in the following image, this 4-bit quantization uses a per-block strategy for weight (RHS matrix) quantization and an 8-bit per-row quantization for activations (LHS matrix):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg4.png&quot; alt=&quot;arch diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see in the preceding image, each output feature map (OFM) in the weight matrix is divided into equally sized blocks (group size), with each block having a scale factor stored in BF16 format. BF16 is advantageous because it maintains the dynamic range of 32-bit floating-point (FP32) format with half the bit size, and it’s easy to convert to and from FP32 using a simple shift operation. This makes BF16 ideal for saving model space, preserving accuracy, and ensuring backward compatibility with devices that lack BF16 hardware acceleration. You can learn more about the BF16 format in &lt;a href=&quot;https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a&quot;&gt;this&lt;/a&gt; Arm Community blog post.&lt;/p&gt;

&lt;p&gt;For completeness, this 4-bit quantization scheme and our implementation in KleidiAI allow users to configure group size for the linear weights (RHS), allowing them to trade-off between model size, model accuracy, and model performance if the model is quantized by the user.&lt;/p&gt;

&lt;p&gt;At this point, we are ready to unveil the incredible performance recorded on Arm CPUs with ExecuTorch when running Llama 3.2 1B and Llama 3.2 3B. Let’s first go over metrics we will use to evaluate the performance of LLM inference.&lt;/p&gt;

&lt;h3 id=&quot;metrics-for-llm-inference&quot;&gt;Metrics for LLM Inference&lt;/h3&gt;

&lt;p&gt;Typically, performance metrics used to evaluate LLM performance during inference include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Time To First Token (TTFT)&lt;/strong&gt;: This measures the time it takes to produce the first output token after a prompt is provided by the user. This latency or response time is important for a good user experience, especially on a phone. TTFT is also a function of the length of the prompt or prompt tokens. To make this metric independent of the prompt length, we use Prefill tokens/second as a proxy here. The relationship between these is inverse: lower TTFT corresponds to higher Prefill tokens/second.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decode Performance&lt;/strong&gt;: This is the average number of output tokens generated per second, thus reported in Tokens/Second. It is independent of the total number of tokens generated. For on-device inference, it is important to keep this higher than a user’s average reading speed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Peak Runtime Memory&lt;/strong&gt;: This metric reflects the amount of RAM, typically reported in MegaBytes (MiB), needed to run the model with expected performance measured using the metrics above. Given the limited amount of RAM available on Android and iOS devices, this is one of the key metrics for on-device LLM deployment. It dictates the type of models that can be deployed on a device.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;p&gt;The quantized Llama 3.2 1B models, both SpinQuant and QLoRA, are designed to run efficiently on a wide range of phones with limited RAM. In this section, we demonstrate that the quantized Llama 3.2 1B models can achieve over 350 tokens per second in the prefill phase and over 40 tokens per second in the decode stage. This level of performance is sufficient to enable on-device text summarization with a reasonable user experience using only Arm CPUs. To put this into perspective, on average, 50 unread messages contain about 600 tokens. With this performance, the response time (the time it takes for the first generated word to appear on the screen) is approximately two seconds.&lt;/p&gt;

&lt;p&gt;We present measurements from a Samsung S24+ running vanilla Android. We used Llama 3.2 1B parameter models for these experiments. Although we only demonstrate using 1B models, similar performance gains can be expected for the 3B parameter models. The experiment setup involves doing a single warmup run, sequence length of 128, prompt length of 64, and using 6 out of 8 available CPUs, and measuring &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama#step-5-run-benchmark-on&quot;&gt;results&lt;/a&gt; over adb.&lt;/p&gt;

&lt;p&gt;Using the ExecuTorch main branch from GitHub, we first generated the ExecuTorch PTE binary files for each model using the published checkpoints. Then, using the same repository, we generated the ExecuTorch runtime binary for Armv8. In the rest of the section, we will compare the performance of different quantized 1B models against the BF16 model using the binary built with KleidiAI. We will also compare the performance gains for quantized models between the binary with KleidiAI and the one without KleidiAI to distill the impact from KleidiAI.&lt;/p&gt;

&lt;h4 id=&quot;quantized-model-performance&quot;&gt;Quantized Model Performance&lt;/h4&gt;

&lt;p&gt;Llama 3.2 quantized models both SpinQuant and QLoRA perform significantly better on prompt prefill and text generation (decode) compared to the baseline BF16. We observed a &amp;gt;2x improvement in decode and a &amp;gt;5x improvement in prefill performance.&lt;/p&gt;

&lt;p&gt;Furthermore, the quantized model size, PTE file size in bytes, is less than half that of the BF16 model, 2.3 GiB vs. 1.1 GiB. Although the size of int4 is a quarter of BF16, some layers in the model are quantized with int8, making the PTE file size ratio larger. We observed runtime peak memory footprint reduction of almost 40% from 3.1 GiB for the BF16 model to 1.9 GiB for the SpinQuant model, measured in Resident Set Size (RSS) for a maximum sequence length of 2048.&lt;/p&gt;

&lt;p&gt;With all-around improvements, the new quantized Llama 3.2 models are ideal for on-device deployment targeting Arm CPUs. For more information on accuracy, check out the Meta Llama 3.2 blog.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg5.png&quot; alt=&quot;bar graph&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;kleidiai-impact&quot;&gt;KleidiAI Impact&lt;/h4&gt;

&lt;p&gt;ExecuTorch relies on the Arm KleidiAI library to provide low-bit performant matrix multiplication kernels for the latest Arm CPUs with advanced Armv8/9 ISA features. These kernels are utilized for on-device quantized Llama 3.2 model inference in ExecuTorch. As depicted in the graph below, ExecuTorch achieves an average of &amp;gt;20% better prefill performance on S24+ with KleidiAI compared to non-KleidiAI kernels, while maintaining the same accuracy. This performance advantage is not limited to specific models or devices, and is expected to benefit all ExecuTorch models using low-bit quantized matrix multiplication on Arm CPUs.&lt;/p&gt;

&lt;p&gt;To assess the impact of Kleidi, we generated two ExecuTorch runtime binaries targeting Arm Cortex-A CPUs and compared their performance.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first ExecuTorch runtime binary built with the Arm KleidiAI library through the XNNPACK library.&lt;/li&gt;
  &lt;li&gt;The second binary was built without the Arm KleidiAI repository, using native kernels from the XNNPACK library.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/unleashing-ai-mobile/fg6.png&quot; alt=&quot;bar chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;try-it-yourself&quot;&gt;Try it yourself!&lt;/h2&gt;

&lt;p&gt;Ready to experience the performance improvements firsthand? Here’s how you can try out ExecuTorch with the optimizations provided by KleidiAI on your projects: Here is a &lt;a href=&quot;https://learn.arm.com/learning-paths/smartphones-and-mobile/build-llama3-chat-android-app-using-executorch-and-xnnpack/&quot;&gt;link to the learning path&lt;/a&gt; from Arm to start developing your own application using LLMs using ExecuTorch and KleidiAI.&lt;/p&gt;

&lt;p&gt;We look forward to hearing your feedback!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Gian Marco Iodice, Arm and Digant Desai, Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Intel GPU Support Now Available in PyTorch 2.5</title>
      <link href="https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/" rel="alternate" type="text/html" title="Intel GPU Support Now Available in PyTorch 2.5" />
      <published>2024-10-25T00:00:00-07:00</published>
      <updated>2024-10-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-gpu-support-pytorch-2-5</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/">&lt;p&gt;Support for Intel GPUs is now available in PyTorch® 2.5, providing improved functionality and performance for Intel GPUs which including &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Arc™ discrete graphics&lt;/a&gt;, &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra processors&lt;/a&gt; with built-in Intel® Arc™ graphics and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html&quot;&gt;Intel® Data Center GPU Max Series&lt;/a&gt;. This integration brings Intel GPUs and the SYCL* software stack into the official PyTorch stack, ensuring a consistent user experience and enabling more extensive AI application scenarios, particularly in the AI PC domain.&lt;/p&gt;

&lt;p&gt;Developers and customers building for and using Intel GPUs will have a better user experience by directly obtaining continuous software support from native PyTorch, unified software distribution, and consistent product release time.&lt;/p&gt;

&lt;p&gt;Furthermore, Intel GPU support provides more choices to users. Now PyTorch provides a consistent GPU programming paradigm on both front ends and back ends. Developers can now run and deploy workloads on Intel GPUs with minimal coding efforts.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-intel-gpu-support&quot;&gt;&lt;strong&gt;Overview of Intel GPU support&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Intel GPU support in PyTorch provides eager mode and graph mode support in the PyTorch built-in front end. Eager mode now has an implementation of commonly used Aten operators with the SYCL programming language. Graph mode (torch.compile) now has an enabled Intel GPU back end to implement the optimization for Intel GPUs and to integrate Triton. &lt;/p&gt;

&lt;p&gt;Essential components of Intel GPU support were added to PyTorch, including runtime, Aten operators, oneDNN, TorchInductor, Triton and Intel GPU tool chains integration. Meanwhile, quantization and distributed are being actively developed in preparation for the PyTorch 2.6 release.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In addition to providing key features for Intel® Client GPUs and Intel® Data Center GPU Max Series for inference and training, PyTorch keeps the same user experience as other hardware the PyTorch supports. If you migrate code from CUDA*, you can run the existing application code on an Intel GPU with minimal code changes for the device name (from cuda to xpu). For example:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;# CUDA Code&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;tensor&lt;/strong&gt; &lt;strong&gt;=&lt;/strong&gt; &lt;strong&gt;torch.tensor([&lt;/strong&gt;1.0&lt;strong&gt;,&lt;/strong&gt; 2.0&lt;strong&gt;]).to(&lt;/strong&gt;“cuda”&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;# Code for Intel GPU&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;tensor&lt;/strong&gt; &lt;strong&gt;=&lt;/strong&gt; &lt;strong&gt;torch.tensor([&lt;/strong&gt;1.0&lt;strong&gt;,&lt;/strong&gt; 2.0&lt;strong&gt;]).to(&lt;/strong&gt;“xpu”&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.5 features with an Intel GPU include: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inference and training workflows.&lt;/li&gt;
  &lt;li&gt;Enhance both torch.compile and eager mode functionalities (more Ops), together with performance improvement, and fully run three Dynamo Hugging Face*, TIMM* and TorchBench* benchmarks for eager and compile modes. &lt;/li&gt;
  &lt;li&gt;Data types such as FP32, BF16, FP16, and automatic mixed precision (AMP).&lt;/li&gt;
  &lt;li&gt;Runs on Intel® Client GPUs and Intel® Data Center GPU Max Series.&lt;/li&gt;
  &lt;li&gt;Supports Linux (Ubuntu, SUSE Linux and Red Hat Linux) and Windows 10/11.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;get-started&quot;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Get a tour of the environment setup, PIP wheels installation, and examples on Intel® Client GPUs and Intel® Data Center GPU Max Series from &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;. Support for Intel GPUs can be experienced through PyTorch PIP wheels installation by nightly and preview binary releases.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Try Intel® Client GPUs through Intel® Arc™ Graphics family (Codename DG2), Intel® Core™ Ultra processor family with Intel® Graphics (Codename Meteor Lake), and Intel® Core™ Ultra mobile processor family with Intel® Graphics (Codename Lunar Lake).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Try Intel Data Center GPU Max Series through &lt;a href=&quot;https://cloud.intel.com/&quot;&gt;Intel® Tiber™ AI Cloud&lt;/a&gt;.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;To learn how to create a free Standard account, see &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;Get Started&lt;/a&gt;. Then do the following:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Sign in to the &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;cloud console&lt;/a&gt;.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;From the &lt;a href=&quot;https://console.cloud.intel.com/training&quot;&gt;Training&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;section, open the  &lt;a href=&quot;https://console.cloud.intel.com/training/detail/7db2a900-e47d-4b70-8968-cefa08432c1d&quot;&gt;PyTorch on Intel® GPUs&lt;/a&gt;  notebook and click “Launch Jupyter Notebook.”&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Ensure that the &lt;strong&gt;PyTorch 2.5&lt;/strong&gt; kernel is selected for the notebook.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance&quot;&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The performance of Intel GPU on PyTorch was continuously optimized to achieve decent result on three Dynamo Hugging Face, TIMM and TorchBench benchmarks for eager and compile modes.&lt;/p&gt;

&lt;p&gt;The latest performance data measured on top of PyTorch Dynamo Benchmarking Suite using Intel® Data Center GPU Max Series 1100 single card showcase the FP16/BF16 significant speedup ratio over FP32 on eager mode in Figure 1, and Torch.compile mode speedup ratio over eager mode in Figure 2. Both inference and training reached the similar significant improvements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-gains-over-fp32-eager-2.png&quot; alt=&quot;Figure 2: FP16/BF16 Performance Gains Over FP32 Eager&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: FP16/BF16 Performance Gains Over FP32 Eager&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-gains-over-fp32-eager.png&quot; alt=&quot;Figure 3: Torch.compile Performance Gains Over Eager Mode&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3: Torch.compile Performance Gains Over Eager Mode&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Intel GPU on PyTorch 2.5 brings Intel® Client GPUs (Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Arc™ Graphics for dGPU parts) and Intel® Data Center GPU Max Series into the PyTorch ecosystem for AI workload acceleration. Especially, Client GPUs is added to the GPU-supported list for AI PC use scenarios on Windows and Linux environment.&lt;/p&gt;

&lt;p&gt;We warmly welcome the community to evaluate and provide feedback on these enhancements to  &lt;a href=&quot;https://github.com/pytorch/pytorch?tab=readme-ov-file#intel-gpu-support&quot;&gt;Intel GPU support on PyTorch&lt;/a&gt;. &lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;PyTorch Docs: Getting Started on Intel GPU&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.intel.com/&quot;&gt;Intel® Tiber™ AI Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We want thank PyTorch open source community for their technical discussions and insights: &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/seemethere&quot;&gt;Eli Uriegas&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;h2 id=&quot;performance-configuration&quot;&gt;&lt;strong&gt;Performance Configuration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The configurations in the table are collected with &lt;a href=&quot;https://github.com/intel/svr-info&quot;&gt;svr-info&lt;/a&gt;. Test by Intel on September 12, 2024.&lt;/p&gt;

&lt;h2 id=&quot;table-1&quot;&gt;Table 1&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Component&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Details&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel® Max Series GPU 1100 in Intel® Tiber™ Developer Cloud&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Time&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Thu Sep 12 08:21:27 UTC 2024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro SYS-521GE-TNRT&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Baseboard&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro X13DEG-OA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Chassis&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro Other&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;CPU Model&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel(R) Xeon(R) Platinum 8468V&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Microarchitecture&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SPR_XCC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Sockets&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Cores per Socket&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Hyperthreading&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;CPUs&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Intel Turbo Boost&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Base Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.4GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;All-core Maximum Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.4GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Maximum Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.9GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;NUMA Nodes&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Prefetchers&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled, AMP: Disabled, Homeless: Disabled, LLC: Disabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;PPINs&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5e3f862ef7ba9d50, 6c85812edfcc84b1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Accelerators&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DLB 2, DSA 2, IAA 2, QAT (on CPU) 2, QAT (on chipset) 0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Installed Memory&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024GB (16x64GB DDR5 4800 MT/s [4800 MT/s])&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Hugepagesize&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2048 kB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Transparent Huge Pages&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;madvise&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Automatic NUMA Balancing&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;NIC&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2 x Ethernet Controller X710 for 10GBASE-T, 4 x MT2892 Family [ConnectX-6 Dx]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Disk&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1 x 894.3G Micron_7450_MTFDKBG960TFR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;BIOS&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.4a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Microcode&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0x2b0004b1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;OS&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Ubuntu 22.04.2 LTS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Kernel&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5.15.0-73-generic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;TDP&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;330W&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Power &amp;amp; Perf Policy&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Normal (6)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Frequency Governor&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;performance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Frequency Driver&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;acpi-cpufreq&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Max C-State&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;table-2&quot;&gt;Table 2&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Component&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Details&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Single Card&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel® Max Series GPU 1100 series on 4th Gen Intel® Xeon® processors of Intel Tiber Developer Cloud&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Workload &amp;amp; version&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Timm ac34701, TorchBench 03cde49, Torchvision d23a6e1, Torchaudio b3f6f51, Transformers 243e186&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Software Stack&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;intel-for-pytorch-gpu-dev 0.5.3, intel-pti-dev 0.9.0, Intel xpu backend for Triton cc981fe&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Framework&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Pytorch 4a3dabd67f8ce63f2fc45f278421cca3cc532cfe&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;GPU driver&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;agama-ci-devel-803.61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;GFX FW Version&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PVC2_1.23374&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Notices &amp;amp; Disclaimers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AI disclaimer:&lt;/strong&gt;&lt;br /&gt;
AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at  &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>PyTorch Team at Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Support for Intel GPUs is now available in PyTorch® 2.5, providing improved functionality and performance for Intel GPUs which including Intel® Arc™ discrete graphics, Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Data Center GPU Max Series. This integration brings Intel GPUs and the SYCL* software stack into the official PyTorch stack, ensuring a consistent user experience and enabling more extensive AI application scenarios, particularly in the AI PC domain.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ExecuTorch Beta: On-Device AI and LLMs, Stability, and Acceleration with Partners</title>
      <link href="https://pytorch.org/blog/executorch-beta/" rel="alternate" type="text/html" title="ExecuTorch Beta: On-Device AI and LLMs, Stability, and Acceleration with Partners" />
      <published>2024-10-24T00:00:00-07:00</published>
      <updated>2024-10-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/executorch-beta</id>
      <content type="html" xml:base="https://pytorch.org/blog/executorch-beta/">&lt;ul&gt;
  &lt;li&gt;ExecuTorch has achieved Beta status with the release of v0.4, providing stable APIs and runtime, as well as extensive kernel coverage.&lt;/li&gt;
  &lt;li&gt;ExecuTorch is the recommended on-device inference engine for Llama 3.2 1B/3B models, offering enhanced performance and memory efficiency for both original and quantized models.&lt;/li&gt;
  &lt;li&gt;There has been a significant increase in adoption and ecosystem growth for ExecuTorch, and the focus is now on improving reliability, performance, and coverage for non-CPU backends as the next steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Current On-Device AI Market&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The on-device AI market has been rapidly expanding, and is revolutionizing the way we interact with technology. It is unlocking new experiences, enabling personalization, and reducing latency. Traditionally, computer vision and speech recognition have been the primary use-cases for on-device AI, particularly in IoT, industrial applications, and mobile devices. However, the emergence of Large Language Models (LLMs) has made Generative AI the fastest growing sector in AI, subsequently highlighting the importance of on-device Generative AI. IDC &lt;a href=&quot;https://www.idc.com/getdoc.jsp?containerId=prUS52478124&quot;&gt;forecasts&lt;/a&gt; by 2028, close to 1 billion GenAI capable smartphones being shipped worldwide.&lt;/p&gt;

&lt;p&gt;LLMs are not only getting smaller but more powerful. This has led to the creation of a new class of applications that leverage multiple models for intelligent agents and streamlined workflows. The community is rapidly adopting and contributing to these new models, with quantized versions being created within hours of model release. Several leading technology companies are investing heavily in small LLMs, even deploying Low-Rank Adaptation (LoRA) at scale on-device to transform user experiences.&lt;/p&gt;

&lt;p&gt;However, this rapid progress comes at a cost. The fragmentation of our on-device AI landscape creates complexity and inefficiency when going from model authoring to edge deployment. This is where PyTorch’s &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch&lt;/a&gt; comes in – our Beta announcement marks an important milestone in addressing these challenges and empowering developers to create innovative, AI-powered applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New Today&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s been exactly one year since we &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;first open sourced ExecuTorch&lt;/a&gt;, six months since &lt;a href=&quot;https://pytorch.org/blog/executorch-alpha/&quot;&gt;Alpha release&lt;/a&gt;, and today, we’re excited to announce three main developments:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Beta&lt;/strong&gt;. ExecuTorch has reached Beta status starting from v0.4! It is now widely adopted and used in production environments across Meta. Through this adoption process we’ve identified and addressed feature gaps, improved stability, and expanded kernel and accelerator coverage. These improvements make us confident to promote ExecuTorch from &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;Alpha&lt;/a&gt; to &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.4.0&quot;&gt;Beta&lt;/a&gt; status, and we are happy to welcome the community to adopt it in their own production settings. Here are three concrete enhancements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Developers can write application code and include the latest ExecuTorch as a dependency, updating when needed with a clean API contract. This is possible due to our API stabilization efforts, as well as our &lt;a href=&quot;https://pytorch.org/executorch/main/api-life-cycle.html&quot;&gt;explicit API lifecycle&lt;/a&gt; and backwards &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/runtime/COMPATIBILITY.md&quot;&gt;compatibility policy&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Running ExecuTorch on CPUs reached the necessary performance, portability and coverage. In particular, we have implemented more than 85% of all &lt;a href=&quot;https://pytorch.org/executorch/main/ir-ops-set-definition.html&quot;&gt;core ATen operators&lt;/a&gt; as part of our &lt;a href=&quot;https://pytorch.org/executorch/stable/kernel-library-overview.html&quot;&gt;portable CPU kernels library&lt;/a&gt; to ensure running a model on ExecuTorch just works in most cases and making missing ops an exception rather than the norm. Moreover, we integrated and extensively tested our &lt;a href=&quot;https://pytorch.org/executorch/main/native-delegates-executorch-xnnpack-delegate.html&quot;&gt;XNNPACK&lt;/a&gt; delegate for high performance on a wide range of CPU architectures. It is used in a number of production cases today.&lt;/li&gt;
  &lt;li&gt;In addition to the low-level ExecuTorch components for greater portability, we built extensions and higher-level abstractions to support more common use-cases such as &lt;a href=&quot;https://pytorch.org/executorch/main/devtools-overview.html&quot;&gt;developer tooling&lt;/a&gt; to support on-device debugging and profiling, and &lt;a href=&quot;https://pytorch.org/executorch/main/extension-module.html&quot;&gt;Module.h&lt;/a&gt; extension to simplify deployment for mobile devices.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2. On-Device Large-Language Models (LLMs).&lt;/strong&gt; There has been a growing interest in the community to deploy Large Language Models (LLMs) on edge devices, as it offers improved privacy and offline capabilities. However, these models are quite large, pushing the limits of what is possible. Fortunately, ExecuTorch can support these models, and we’ve enhanced the overall framework with numerous optimizations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ExecuTorch is the &lt;a href=&quot;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-inference-with-lightweight-models-&quot;&gt;recommended framework&lt;/a&gt; to run latest Llama models on-device with &lt;a href=&quot;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-inference-with-lightweight-models-&quot;&gt;excellent performance&lt;/a&gt; today. The Llama 3.2 1B/3B models are well-suited for mobile deployment, and it is especially true with the official &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/&quot;&gt;quantized 1B/3B model releases&lt;/a&gt; from Meta, as it provides a great balance between performance, accuracy, and size. When deploying Llama 3.2 1B/3B quantized models, decode latency improved by 2.5x and prefill latency improved by 4.2x on average, while model size decreased by 56% and memory usage reduced by 41% on average when benchmarked on Android OnePlus 12 device (we’ve also verified similar relative performance on Samsung S24+ for 1B and 3B, and Samsung S22 for 1B). For Llama 3.2 1B quantized model, for example, ExecuTorch is able to achieve 50.2 tokens/s for decoding and 260 tokens/s for prefill on the OnePlus 12, using the latest CPU kernels from XNNPACK and &lt;a href=&quot;https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/llm-inference-llama-quantized-models-executorch-kleidiai&quot;&gt;Kleidi libraries&lt;/a&gt;. These quantized models allow developers to integrate LLMs into memory and power-constrained devices while still maintaining quality and safety.&lt;/li&gt;
  &lt;li&gt;One of the value propositions of ExecuTorch is being able to use accelerators on mobile devices seamlessly. In fact, ExecuTorch also showcased accelerators to achieve even greater performance running Llama across &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/apple_ios/LLaMA/docs/delegates/mps_README.md&quot;&gt;Apple MPS backend&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/android/LlamaDemo/docs/delegates/qualcomm_README.md&quot;&gt;Qualcomm AI Accelerator&lt;/a&gt;, and &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/android/LlamaDemo/docs/delegates/mediatek_README.md&quot;&gt;MediaTek AI Accelerator&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;There has been growing community and industry interest in multimodal and beyond text-only LLMs, evidenced by Meta’s &lt;a href=&quot;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&quot;&gt;Llama 3.2 11B/90B vision models&lt;/a&gt; and open-source models like Llava. We have so far &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llava&quot;&gt;enabled Llava 1.5 7B model on phones via ExecuTorch&lt;/a&gt;, making many optimizations, notably reducing runtime memory from 11GB all the way down to 5GB.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Ecosystem and Community Adoption&lt;/strong&gt;&lt;br /&gt;
Now that ExecuTorch is in Beta, it is mature enough to be used in production. It is being increasingly used at Meta across various product surfaces. For instance, ExecuTorch already powers various ML inference use cases across Meta’s Ray-Ban Meta Smart Glasses and Quest 3 VR headsets as well as Instagram and WhatsApp.&lt;/p&gt;

&lt;p&gt;We also &lt;a href=&quot;https://huggingface.co/docs/transformers/main/en/main_classes/executorch&quot;&gt;partnered with Hugging Face&lt;/a&gt; to provide native ExecuTorch support for models being exported using torch.export. This collaboration ensures exported artifacts can directly be lowered and run efficiently on various mobile and edge devices. Models like gemma-2b and phi3-mini are already supported and more foundational models support is &lt;a href=&quot;https://github.com/huggingface/transformers/issues/32253&quot;&gt;in progress&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With stable APIs and Gen AI support, we’re excited to build and grow ExecuTorch with the community. The on-device AI community is growing rapidly and finding ways to adopt ExecuTorch across various fields. For instance, ExecuTorch is being used in a mobile app built by &lt;a href=&quot;https://digica.com/&quot;&gt;Digica&lt;/a&gt; to streamline inventory management in hospitals. As another example, Software Mansion developed an app, &lt;a href=&quot;https://blog.swmansion.com/eraserai-how-to-create-efficient-app-for-edge-device-04f09aa8072f&quot;&gt;EraserAI&lt;/a&gt;, to remove unwanted objects from a photo with EfficientSAM running on-device with ExecuTorch via Core ML delegate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Towards General Availability (GA):&lt;/strong&gt;&lt;br /&gt;
Since the original release of ExecuTorch alpha, we’ve seen a growing interest within the community in using ExecuTorch in various production environments. To that end, we have made great progress towards more stabilized and matured APIs and have made a significant investment in community support, adoption and contribution to ExecuTorch. As are are getting close to GA, we are investing our efforts in the following areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Non-CPU backends:&lt;/strong&gt; Bringing non-CPU backends to even greater robustness, coverage and performance is our next goal. From day one of our original launch, we have partnered with Apple (for Core ML and MPS), Arm (for EthosU NPU) and Qualcomm (for Hexagon NPU) on accelerator integration with ExecuTorch, and we’ve since then expanded our partnership to MediaTek (NPU) and Cadence (XTensa DSP). We’re also building &lt;a href=&quot;https://pytorch.org/executorch/stable/native-delegates-executorch-vulkan-delegate.html&quot;&gt;Vulkan GPU&lt;/a&gt; integration in-house. In terms of feature coverage, we’ve successfully implemented the core functionalities with our partners, ensured seamless integration with our developer tooling, and showcased successful LLM integration with many of the accelerators. Our next big step is to thoroughly validate the performance and reliability of the system in real-world, production use-cases. This stage will help us fine-tune the experience and ensure the stability needed for smooth operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Benchmarking infra&lt;/strong&gt;: As part of our ongoing testing efforts, we’ve developed a benchmarking infrastructure along with a &lt;a href=&quot;https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fexecutorch&quot;&gt;public dashboard&lt;/a&gt; to showcase our progress toward on-device model inference benchmarking. This allows us to transparently track and display model coverage across various backends, giving our community real-time insights into how we’re advancing towards our goals.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re excited to share these developments with you and look forward to continued improvements in collaboration with our partners and the community! We welcome community contribution to help us make ExecuTorch the clear choice for deploying AI and LLM models on-device. We invite you to start using ExecuTorch in your on-device projects, or even better consider &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; to it. You can also report any issues on our &lt;a href=&quot;https://github.com/pytorch/executorch/issues&quot;&gt;GitHub&lt;/a&gt; page.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">ExecuTorch has achieved Beta status with the release of v0.4, providing stable APIs and runtime, as well as extensive kernel coverage. ExecuTorch is the recommended on-device inference engine for Llama 3.2 1B/3B models, offering enhanced performance and memory efficiency for both original and quantized models. There has been a significant increase in adoption and ecosystem growth for ExecuTorch, and the focus is now on improving reliability, performance, and coverage for non-CPU backends as the next steps.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">TorchRec and FBGEMM 1.0 Stable Release</title>
      <link href="https://pytorch.org/blog/torchrec-fbgemm-1/" rel="alternate" type="text/html" title="TorchRec and FBGEMM 1.0 Stable Release" />
      <published>2024-10-23T00:00:00-07:00</published>
      <updated>2024-10-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchrec-fbgemm-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchrec-fbgemm-1/">&lt;p&gt;We are happy to announce the stable release, 1.0, for &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt;. TorchRec is the PyTorch native recommendation systems library, powered by FBGEMM’s (Facebook GEneral Matrix Multiplication) efficient, low-level kernels.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/introducing-torchrec/&quot;&gt;Initially open sourced in 2022&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; provides common primitives for creating state-of-the-art personalization models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simple, optimized APIs for distributed training across hundreds of GPUs&lt;/li&gt;
  &lt;li&gt;Advanced sharding techniques for embeddings&lt;/li&gt;
  &lt;li&gt;Modules common in authoring recommendation systems&lt;/li&gt;
  &lt;li&gt;Frictionless path to distributed inference with APIs for quantization and sharding of TorchRec models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since then, TorchRec has matured significantly, with wide internal adoption across many Meta production recommendation models for training and inference, alongside new features such as: &lt;a href=&quot;https://github.com/pytorch/torchrec/releases?page=1&quot;&gt;variable batched embeddings, embedding offloading, zero collision hashing, etc.&lt;/a&gt; Furthermore, TorchRec has a presence outside of Meta, such as &lt;a href=&quot;https://docs.databricks.com/en/machine-learning/train-recommender-models.html&quot;&gt;in recommendation models at Databricks&lt;/a&gt; and in the &lt;a href=&quot;https://github.com/twitter/the-algorithm-ml&quot;&gt;Twitter algorithm&lt;/a&gt;. As a result, standard TorchRec features have been marked as &lt;strong&gt;stable&lt;/strong&gt;, with PyTorch style BC guarantees, and can be seen on the &lt;a href=&quot;https://pytorch.org/torchrec/&quot;&gt;revamped TorchRec documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fbgemm&quot;&gt;FBGEMM&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/FBGEMM/&quot;&gt;FBGEMM is a library that provides high-performance kernels for CPUs and GPUs&lt;/a&gt;. Since 2018, FBGEMM has supported the efficient execution of Meta-internal and external AI/ML workloads by expanding its scope from &lt;a href=&quot;https://arxiv.org/abs/2101.05615&quot;&gt;performance-critical kernels for inference on CPUs&lt;/a&gt; to more complex sparse operators for both training and inference – and recently for Generative AI – on CPUs and GPUs.&lt;/p&gt;

&lt;p&gt;FBGEMM has been empowering TorchRec through its backend high-performance kernel implementations for recommendation workloads, ranging from embedding bag kernels to jagged tensor operations. Together with TorchRec, we released FBGEMM 1.0, which guarantees the functionality and backward-compatibility of several stable APIs serving its core features with &lt;a href=&quot;https://pytorch.org/FBGEMM/&quot;&gt;enhanced documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/&quot;&gt;DLRM (Deep Learning Recommendation Model)&lt;/a&gt; is the standard neural network architecture for powering recommendations at Meta, with categorical features being processed through embeddings, while continuous (dense) features are processed with a bottom multilayer perceptron. The following diagram depicts the basic architecture of DLRM, with a second order interaction layer between the dense and sparse features and a top MLP for generating the prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchrec-1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TorchRec provides standardized modules with significant optimizations in fusing embedding lookups. EBC is a traditional PyTorch embedding module implementation, containing a collection of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.EmbeddingBags.&lt;/code&gt; FusedEBC, powered by FBGEMM for high performance operations on embedding tables with a fused optimizer and UVM caching/management for alleviating memory constraints, is the optimized version present in sharded TorchRec modules for distributed training and inference. The below benchmark demonstrates the vast performance improvements of FusedEBC in comparison to a traditional PyTorch embedding module implementation (EBC) and the ability for FusedEBC to handle much larger embeddings than what is available on GPU memory with UVM caching.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchrec-2.png&quot; alt=&quot;performance chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;torchrec-data-types&quot;&gt;TorchRec Data Types&lt;/h2&gt;

&lt;p&gt;TorchRec provides standard &lt;a href=&quot;https://pytorch.org/torchrec/datatypes-api-reference.html&quot;&gt;data types&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/torchrec/modules-api-reference.html&quot;&gt;modules&lt;/a&gt; for easy handling of distributed embeddings. Here is a simple example setting up a collection of embedding tables through TorchRec:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec import EmbeddingBagCollection
from torchrec import KeyedJaggedTensor
from torchrec import JaggedTensor

ebc = torchrec.EmbeddingBagCollection(
    device=&quot;cpu&quot;,
    tables=[
        torchrec.EmbeddingBagConfig(
            name=&quot;product_table&quot;,
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=[&quot;product&quot;],
            pooling=torchrec.PoolingType.SUM,
        ),
        torchrec.EmbeddingBagConfig(
            name=&quot;user_table&quot;,
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=[&quot;user&quot;],
            pooling=torchrec.PoolingType.SUM,
        )
    ]
)

product_jt = JaggedTensor(
    values=torch.tensor([1, 2, 1, 5]), lengths=torch.tensor([3, 1])
)
user_jt = JaggedTensor(values=torch.tensor([2, 3, 4, 1]), lengths=torch.tensor([2, 2]))

kjt = KeyedJaggedTensor.from_jt_dict({&quot;product&quot;: product_jt, &quot;user&quot;: user_jt})

print(&quot;Call EmbeddingBagCollection Forward: &quot;, ebc(kjt))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;sharding&quot;&gt;Sharding&lt;/h2&gt;

&lt;p&gt;TorchRec provides a planner class that automatically generates an optimized sharding plan across many GPUs. Here we demonstrate generating a sharding plan across two GPUs:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology

planner = EmbeddingShardingPlanner(
    topology=Topology(
        world_size=2,
        compute_device=&quot;cuda&quot;,
    )
)

plan = planner.collective_plan(ebc, [sharder], pg)

print(f&quot;Sharding Plan generated: {plan}&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;model-parallel&quot;&gt;Model Parallel&lt;/h2&gt;

&lt;p&gt;TorchRec’s main distributed training API is &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module&quot;&gt;DistributedModelParallel&lt;/a&gt;, which calls the planner to generate a sharding plan (demonstrated above) and shards TorchRec modules according to that plan. We demonstrate using &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module&quot;&gt;DistributedModelParallel&lt;/a&gt; to our EmbeddingBagCollection for sharding embeddings distributed training:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(&quot;cuda&quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;TorchRec provides simple APIs for quantizing and sharding embeddings for a model for distributed inference. The usage is demonstrated below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec.inference.modules import (
    quantize_inference_model,
    shard_quant_model,
)
quant_model = quantize_inference_model(ebc)
sharded_model, _ = shard_quant_model(
    quant_model, compute_device=device, sharding_device=device
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;TorchRec and FBGEMM are now stable, with optimized features for large scale recommendation systems.&lt;/p&gt;

&lt;p&gt;For setting up TorchRec and FBGEMM, check out the &lt;a href=&quot;https://pytorch.org/torchrec/setup-torchrec.html&quot;&gt;getting started guide&lt;/a&gt;. &lt;br /&gt;
 &lt;br /&gt;
We also recommend the comprehensive, end-to-end &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchrec_intro_tutorial.html#&quot;&gt;tutorial for introducing the features in TorchRec and FBGEMM&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Paul Zhang, Zain Huda, Sarunya Pumma, Shintaro Iwasaki, Supadchaya Puangpontip, Benson Ma</name>
        
        
      </author>

      

      

      
        <summary type="html">We are happy to announce the stable release, 1.0, for TorchRec and FBGEMM. TorchRec is the PyTorch native recommendation systems library, powered by FBGEMM’s (Facebook GEneral Matrix Multiplication) efficient, low-level kernels.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.5 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-5/" rel="alternate" type="text/html" title="PyTorch 2.5 Release Blog" />
      <published>2024-10-17T00:00:00-07:00</published>
      <updated>2024-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-5</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-5/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.5 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.5.0&quot;&gt;release note&lt;/a&gt;)! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.&lt;/p&gt;

&lt;p&gt;This release is composed of 4095 commits from 504 contributors since PyTorch 2.4. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.5. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;As well, please check out our new ecosystem projects releases with &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch-labs/torchfix/releases/tag/v0.6.0&quot;&gt;TorchFix&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Beta
   &lt;/td&gt;
   &lt;td&gt;Prototype
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CuDNN backend for SDPA
   &lt;/td&gt;
   &lt;td&gt;FlexAttention
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.compile regional compilation without recompilations
   &lt;/td&gt;
   &lt;td&gt;Compiled Autograd
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchDynamo added support for exception handling &amp;amp; MutableMapping types
   &lt;/td&gt;
   &lt;td&gt;Flight Recorder
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchInductor CPU backend optimization
   &lt;/td&gt;
   &lt;td&gt;Max-autotune Support on CPU with GEMM Template
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;TorchInductor on Windows
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FP16 support on CPU path for both eager mode and TorchInductor CPP backend
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Autoload Device Extension
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Enhanced Intel GPU support
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-cudnn-backend-for-sdpa&quot;&gt;[Beta] CuDNN backend for SDPA&lt;/h3&gt;

&lt;p&gt;The cuDNN “Fused Flash Attention” backend  was landed for &lt;em&gt;torch.nn.functional.scaled_dot_product_attention&lt;/em&gt;. On NVIDIA H100 GPUs this can provide up to 75% speed-up over FlashAttentionV2. This speedup is enabled by default for all users of SDPA on H100 or newer GPUs.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchcompile-regional-compilation-without-recompilations&quot;&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; regional compilation without recompilations&lt;/h3&gt;

&lt;p&gt;Regional compilation without recompilations, via &lt;em&gt;torch._dynamo.config.inline_inbuilt_nn_modules&lt;/em&gt; which default to True in 2.5+. This option allows users to compile a repeated &lt;em&gt;nn.Module&lt;/em&gt; (e.g. a transformer layer in LLM) without recompilations. Compared to compiling the full model, this option can result in smaller compilation latencies with 1%-5% performance degradation compared to full model compilation.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/regional_compilation.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchinductor-cpu-backend-optimization&quot;&gt;[Beta] TorchInductor CPU backend optimization&lt;/h3&gt;

&lt;p&gt;This feature advances Inductor’s CPU backend optimization, including CPP backend code generation and FX fusions with customized CPU kernels. The Inductor CPU backend supports vectorization of common data types and all Inductor IR operations, along with the static and symbolic shapes. It is compatible with both Linux and Windows OS and supports the default Python wrapper, the CPP wrapper, and AOT-Inductor mode.&lt;/p&gt;

&lt;p&gt;Additionally, it extends the max-autotune mode of the GEMM template (prototyped in 2.5), offering further performance gains. The backend supports various FX fusions, lowering to customized kernels such as oneDNN for Linear/Conv operations and SDPA. The Inductor CPU backend consistently achieves performance speedups across three benchmark suites—TorchBench, Hugging Face, and timms—outperforming eager mode in 97.5% of the 193 models tested.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-flexattention&quot;&gt;[Prototype] FlexAttention&lt;/h3&gt;

&lt;p&gt;We’ve introduced a flexible API that enables implementing various attention mechanisms such as Sliding Window, Causal Mask, and PrefixLM with just a few lines of idiomatic PyTorch code. This API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations. Additionally, we automatically generate the backwards pass using PyTorch’s autograd machinery. Furthermore, our API can take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.&lt;/p&gt;

&lt;p&gt;For more information and examples, please refer to the &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;official blog post&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;Attention Gym&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-compiled-autograd&quot;&gt;[Prototype] Compiled Autograd&lt;/h3&gt;

&lt;p&gt;Compiled Autograd is an extension to the PT2 stack allowing the capture of the entire backward pass. Unlike the backward graph traced by AOT dispatcher, Compiled Autograd tracing is deferred until backward execution time, which makes it impervious to forward pass graph breaks, and allows it to record backward hooks into the graph.&lt;/p&gt;

&lt;p&gt;Please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/compiled_autograd_tutorial.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flight-recorder&quot;&gt;[Prototype] Flight Recorder&lt;/h3&gt;

&lt;p&gt;Flight recorder is a new debugging tool that helps debug stuck jobs. The tool works by continuously capturing information about collectives as they run. Upon detecting a stuck job, the information can be used to quickly identify misbehaving ranks/machines along with code stack traces.&lt;/p&gt;

&lt;p&gt;For more information please refer to the following &lt;a href=&quot;https://pytorch.org/tutorials/prototype/flight_recorder_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-max-autotune-support-on-cpu-with-gemm-template&quot;&gt;[Prototype] Max-autotune Support on CPU with GEMM Template&lt;/h3&gt;

&lt;p&gt;Max-autotune mode for the Inductor CPU backend in torch.compile profiles multiple implementations of operations at compile time and selects the best-performing one. This is particularly beneficial for GEMM-related operations, using a C++ template-based GEMM implementation as an alternative to the ATen-based approach with oneDNN and MKL libraries. We support FP32, BF16, FP16, and INT8 with epilogue fusions for x86 CPUs. We’ve seen up to 7% geomean speedup on the dynamo benchmark suites and up to 20% boost in next-token latency for LLM inference.&lt;/p&gt;

&lt;p&gt;For more information please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/max_autotune_on_CPU_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-torchinductor-cpu-on-windows&quot;&gt;[Prototype] TorchInductor CPU on Windows&lt;/h3&gt;

&lt;p&gt;Inductor CPU backend in torch.compile now works on Windows. We support MSVC (cl), clang (clang-cl) and Intel compiler (icx-cl) for Windows inductor currently.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows_cpu.html&quot;&gt;tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;prototype-fp16-support-on-cpu-path-for-both-eager-mode-and-torchinductor-cpp-backend&quot;&gt;[Prototype] FP16 support on CPU path for both eager mode and TorchInductor CPP backend&lt;/h3&gt;

&lt;p&gt;Float16 is a commonly used reduced floating point type for performance improvement in neural network inference/training. Since this release, float16 for both eager and TorchInductor is supported on the CPU path.&lt;/p&gt;

&lt;h3 id=&quot;prototype-autoload-device-extension&quot;&gt;[Prototype] Autoload Device Extension&lt;/h3&gt;

&lt;p&gt;PyTorch now supports autoloading for out-of-tree device extensions, streamlining integration by eliminating the need for manual imports. This feature, enabled through the torch.backends entrypoint, simplifies usage by ensuring seamless extension loading, while allowing users to disable it via an environment variable if needed.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/python_extension_autoload.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;prototype-enhanced-intel-gpu-support&quot;&gt;[Prototype] Enhanced Intel GPU support&lt;/h3&gt;

&lt;p&gt;Intel GPUs support enhancement is now available for both Intel® Data Center GPU Max Series and Intel® Client GPUs (Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Arc™ Graphics for dGPU parts), which is to make it easier to accelerate your Machine Learning workflows on Intel GPUs in PyTorch 2.5 release. We also enabled the initial support of PyTorch on Windows for Intel® Client GPUs in this release.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Expanded PyTorch hardware backend support matrix to include both Intel Data Center and Client GPUs.  &lt;/li&gt;
  &lt;li&gt;The implementation of SYCL* kernels to enhance coverage and execution of Aten operators on Intel GPUs to boost performance in PyTorch eager mode.&lt;/li&gt;
  &lt;li&gt;Enhanced Intel GPU backend of torch.compile to improve inference and training performance for a wide range of deep learning workloads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These features are available through PyTorch preview and nightly binary PIP wheels. For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.5 (release note)! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The Path to Achieve PyTorch Performance Boost on Windows CPU</title>
      <link href="https://pytorch.org/blog/performance-boost-windows/" rel="alternate" type="text/html" title="The Path to Achieve PyTorch Performance Boost on Windows CPU" />
      <published>2024-10-15T00:00:00-07:00</published>
      <updated>2024-10-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/performance-boost-windows</id>
      <content type="html" xml:base="https://pytorch.org/blog/performance-boost-windows/">&lt;p&gt;The challenge of PyTorch’s lower CPU performance on Windows compared to Linux has been a significant issue. There are multiple factors leading to this performance disparity. Through our investigation, we’ve identified several reasons for poor CPU performance on Windows, two primary issues have been pinpointed: the inefficiency of the Windows default malloc memory allocator and the absence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;&gt;SIMD&lt;/a&gt; for vectorization optimizations on the Windows platform. In this article, we show how PyTorch CPU performance on Windows has improved from the previous releases and where it stands as of PyTorch 2.4.1.&lt;/p&gt;

&lt;h2 id=&quot;memory-allocation-optimization-in-pytorch-212-and-later&quot;&gt;Memory Allocation Optimization in PyTorch 2.1.2 and later&lt;/h2&gt;

&lt;p&gt;In versions prior to PyTorch 2.1.2, PyTorch relied on the operating system’s default malloc function for memory allocation. The default malloc memory allocation on the Windows platform was less efficient compared to the malloc implementation mechanism on the Linux platform, leading to increased memory allocation times and reduced performance. To address this, we have substituted the default Windows malloc with mimalloc, a more efficient memory allocator developed by Microsoft. This update, included with the release of PyTorch 2.1.2 and later, has significantly enhanced the CPU performance of PyTorch on Windows, as shown in Figure 1.1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg1.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PyTorch CPU Performance Improvement on Windows with Memory Allocation Optimization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.1: Relative throughput improvement achieved by upgrading from Windows PyTorch version 2.0.1 to 2.1.2 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The graph illustrates that with the release of PyTorch 2.1.2, there has been a notable enhancement in CPU performance on the Windows platform. The degree of improvement varies across different models, which can be attributed to the diverse mix of operations they perform and their corresponding memory access patterns. While the BERT model shows a modest performance gain, models like ResNet50 and MobileNet-v3 Large benefit from more pronounced improvements.&lt;/p&gt;

&lt;p&gt;On a high-performance CPU, memory allocation becomes a performance bottleneck. This is also why addressing this issue has led to such significant performance improvements.&lt;/p&gt;

&lt;p&gt;As shown in the graphs below, we see that PyTorch CPU performance on Windows can significantly be improved. However, there is still a noticeable gap when compared to its performance on Linux. The absence of vectorization optimizations in the Windows variant of PyTorch CPU is a key factor to the remaining performance gap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg2.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.0.1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.2: Relative performance of Windows vs Linux with PyTorch version 2.0.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg3.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%; margin-top: 50px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.1.2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.3: Relative performance of Windows vs Linux with PyTorch version 2.1.2 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;vectorization-optimization-in-pytorch-241-and-later&quot;&gt;Vectorization Optimization in PyTorch 2.4.1 and later&lt;/h2&gt;

&lt;p&gt;Prior to PyTorch 2.4.1, the Windows build of PyTorch lacked &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;&gt;SIMD&lt;/a&gt; for vectorization optimizations, a feature that the Linux build leveraged for improved performance. This discrepancy was due to the &lt;a href=&quot;https://github.com/shibatch/sleef&quot;&gt;SLEEF&lt;/a&gt; Library’s integration issues on Windows, which is a SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT and is essential for efficient trigonometric calculations. Through a collaborative effort with engineers from ARM and Qualcomm, these challenges were resolved, enabling the integration of SIMD into PyTorch for Windows. The PyTorch 2.4.1 update has thus significantly enhanced PyTorch’s CPU performance on Windows, as shown in Figure 2.1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg4.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PyTorch CPU Performance Improvement on Windows with Vertorization Optimization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2.1: Relative throughput improvement achieved by upgrading from PyTorch CPU version 2.1.2 to 2.4.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As shown in the graph below, we see that PyTorch CPU performance on Windows ahieved the performance on Linux.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg5.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.4.1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2.2: Relative performance of Windows vs Linux with PyTorch version 2.4.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;CONCLUSION&lt;/h2&gt;

&lt;p&gt;From PyTorch 2.0.1 to PyTorch 2.4.1, the CPU performance gap between Windows and Linux has been continuously narrowing. We compared the ratio of CPU performance on Windows to CPU performance on Linux across different versions, and the results are shown in the following graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg6.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on different version of PyTorch&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Performance Ratio for Windows to Linux with different version of PyTorch (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The graph shows that with PyTorch 2.4.1, CPU performance on Windows has nearly converged with that on Linux, and on some models, it has even surpassed Linux. For example, in the case of DistillBERT and RoBERTa models, the CPU performance ratio of Windows to Linux has achieved a remarkable 102%. However, certain models, including MobileNet-v3, still show a performance discrepancy. Intel engineers will continue to collaborate with Meta engineers, to reduce the performance gap of PyTorch CPU between Windows and Linux.&lt;/p&gt;

&lt;h2 id=&quot;how-to-take-advantage-of-the-optimizations&quot;&gt;HOW TO TAKE ADVANTAGE OF THE OPTIMIZATIONS&lt;/h2&gt;

&lt;p&gt;Install PyTorch CPU 2.4.1 or later on Windows from the &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;official repository&lt;/a&gt;, and you may automatically experience a performance boost with memory allocation and vectorizations.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;ACKNOWLEDGMENTS&lt;/h2&gt;

&lt;p&gt;The results presented in this blog post was achieved through the collaborative effort of the Intel PyTorch team and Meta. We would like to express our sincere gratitude to &lt;a href=&quot;https://github.com/xuhancn&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;https://github.com/jgong5&quot;&gt;Jiong Gong&lt;/a&gt;, &lt;a href=&quot;https://github.com/zhuhaozhe&quot;&gt;Haozhe Zhu&lt;/a&gt;, &lt;a href=&quot;https://github.com/mingfeima&quot;&gt;Mingfei Ma&lt;/a&gt;, &lt;a href=&quot;https://github.com/chuanqi129&quot;&gt;Chuanqi Wang&lt;/a&gt;, &lt;a href=&quot;https://github.com/Guobing-Chen&quot;&gt;Guobing Chen&lt;/a&gt; and &lt;a href=&quot;https://github.com/EikanWang&quot;&gt;Eikan Wang&lt;/a&gt;. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here. Thanks to &lt;a href=&quot;https://github.com/peterjc123&quot;&gt;Jiachen Pu&lt;/a&gt; from community for his participation in the issue discussion and suggesting the use of &lt;a href=&quot;https://github.com/microsoft/mimalloc&quot;&gt;mimalloc&lt;/a&gt;. We’d also like to express our gratitude to Microsoft for providing such an easily integrated and performant mallocation library. Thanks to &lt;a href=&quot;https://github.com/blapie&quot;&gt;Pierre Blanchard&lt;/a&gt; , &lt;a href=&quot;https://github.com/nSircombe&quot;&gt;Nathan Sircombe&lt;/a&gt; from ARM and &lt;a href=&quot;https://github.com/alexreinking&quot;&gt;Alex Reinking&lt;/a&gt; from Adobe for their contribution in overcome the compatibility issues with the &lt;a href=&quot;https://github.com/shibatch/sleef&quot;&gt;sleef&lt;/a&gt; integrated to PyTorch Windows. Finally we want to thank &lt;a href=&quot;https://github.com/jingxu10&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;https://github.com/WeizhuoZhang-intel&quot;&gt;Weizhuo Zhang&lt;/a&gt; and &lt;a href=&quot;https://github.com/ZhaoqiongZ&quot;&gt;Zhaoqiong Zheng&lt;/a&gt; for their contributions to this blog.&lt;/p&gt;

&lt;h3 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h3&gt;

&lt;p&gt;The configurations in the table are collected with &lt;a href=&quot;https://github.com/intel/svr-info&quot;&gt;svr-info&lt;/a&gt;. Test by Intel on August 30, 2024.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Specification&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Configuration1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Configuration2&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Name
   &lt;/td&gt;
   &lt;td&gt;ThinkBook 14 G5+ IRH
   &lt;/td&gt;
   &lt;td&gt;ThinkBook 14 G5+ IRH
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Time
   &lt;/td&gt;
   &lt;td&gt;Fri Aug 30 02:43:02 PM UTC 2024
   &lt;/td&gt;
   &lt;td&gt;Fri Aug 30 02:43:02 PM UTC 2024
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;System
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseboard
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Chassis
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU Model
   &lt;/td&gt;
   &lt;td&gt;13th Gen Intel(R) Core(TM) i7-13700H
   &lt;/td&gt;
   &lt;td&gt;13th Gen Intel(R) Core(TM) i7-13700H
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Microarchitecture
   &lt;/td&gt;
   &lt;td&gt;Unknown Intel
   &lt;/td&gt;
   &lt;td&gt;Unknown Intel
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sockets
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Cores per Socket
   &lt;/td&gt;
   &lt;td&gt;14
   &lt;/td&gt;
   &lt;td&gt;14
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hyperthreading
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPUs
   &lt;/td&gt;
   &lt;td&gt;20
   &lt;/td&gt;
   &lt;td&gt;20
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Intel Turbo Boost
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Base Frequency
   &lt;/td&gt;
   &lt;td&gt;2.4GHz
   &lt;/td&gt;
   &lt;td&gt;2.4GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;All-core Maximum Frequency
   &lt;/td&gt;
   &lt;td&gt;4.7GHz
   &lt;/td&gt;
   &lt;td&gt;4.7GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Maximum Frequency
   &lt;/td&gt;
   &lt;td&gt;4.8GHz
   &lt;/td&gt;
   &lt;td&gt;4.8GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;NUMA Nodes
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Prefetchers
   &lt;/td&gt;
   &lt;td&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled
   &lt;/td&gt;
   &lt;td&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;PPINs
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Accelerators
   &lt;/td&gt;
   &lt;td&gt;DLB, DSA, IAA, QAT
   &lt;/td&gt;
   &lt;td&gt;DLB, DSA, IAA, QAT
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Installed Memory
   &lt;/td&gt;
   &lt;td&gt;32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s])
   &lt;/td&gt;
   &lt;td&gt;32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s])
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hugepagesize
   &lt;/td&gt;
   &lt;td&gt;2048kb
   &lt;/td&gt;
   &lt;td&gt;2048kb
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Transparent Huge Pages
   &lt;/td&gt;
   &lt;td&gt;madvise
   &lt;/td&gt;
   &lt;td&gt;madvise
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Automatic NUMA Balancing
   &lt;/td&gt;
   &lt;td&gt;Disabled
   &lt;/td&gt;
   &lt;td&gt;Disabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;NIC
   &lt;/td&gt;
   &lt;td&gt;“1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation”
   &lt;/td&gt;
   &lt;td&gt;“1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation”
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Disk
   &lt;/td&gt;
   &lt;td&gt;Micron MTFDKBA512TFH 500G
   &lt;/td&gt;
   &lt;td&gt;Micron MTFDKBA512TFH 500G
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BIOS
   &lt;/td&gt;
   &lt;td&gt;LBCN22WW
   &lt;/td&gt;
   &lt;td&gt;LBCN22WW
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Microcode
   &lt;/td&gt;
   &lt;td&gt;0x411c
   &lt;/td&gt;
   &lt;td&gt;0x411c
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;OS
   &lt;/td&gt;
   &lt;td&gt;Windows 11 Desktop
   &lt;/td&gt;
   &lt;td&gt;Ubuntu 23.10
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Kernel
   &lt;/td&gt;
   &lt;td&gt;OS Build 19045.4412
   &lt;/td&gt;
   &lt;td&gt;6.5.0-27-generic
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TDP
   &lt;/td&gt;
   &lt;td&gt;200 watts
   &lt;/td&gt;
   &lt;td&gt;200 watts
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Power &amp;amp; Perf Policy
   &lt;/td&gt;
   &lt;td&gt;Normal Powersave (7)
   &lt;/td&gt;
   &lt;td&gt;Normal Powersave (7)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Frequency Governor
   &lt;/td&gt;
   &lt;td&gt;performance
   &lt;/td&gt;
   &lt;td&gt;performance
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Frequency Driver
   &lt;/td&gt;
   &lt;td&gt;intel_pstate
   &lt;/td&gt;
   &lt;td&gt;intel_pstate
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Max C-State
   &lt;/td&gt;
   &lt;td&gt;9
   &lt;/td&gt;
   &lt;td&gt;9
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the &lt;a href=&quot;https://edc.intel.com/content/www/us/en/products/performance/benchmarks/overview/&quot;&gt;Performance Index site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Performance results are based on testing as of dates shown in &lt;a href=&quot;#product-and-performance-information&quot;&gt;configurations&lt;/a&gt; and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel Corporation</name>
        
        
      </author>

      

      

      
        <summary type="html">The challenge of PyTorch’s lower CPU performance on Windows compared to Linux has been a significant issue. There are multiple factors leading to this performance disparity. Through our investigation, we’ve identified several reasons for poor CPU performance on Windows, two primary issues have been pinpointed: the inefficiency of the Windows default malloc memory allocator and the absence of SIMD for vectorization optimizations on the Windows platform. In this article, we show how PyTorch CPU performance on Windows has improved from the previous releases and where it stands as of PyTorch 2.4.1. Memory Allocation Optimization in PyTorch 2.1.2 and later In versions prior to PyTorch 2.1.2, PyTorch relied on the operating system’s default malloc function for memory allocation. The default malloc memory allocation on the Windows platform was less efficient compared to the malloc implementation mechanism on the Linux platform, leading to increased memory allocation times and reduced performance. To address this, we have substituted the default Windows malloc with mimalloc, a more efficient memory allocator developed by Microsoft. This update, included with the release of PyTorch 2.1.2 and later, has significantly enhanced the CPU performance of PyTorch on Windows, as shown in Figure 1.1. PyTorch CPU Performance Improvement on Windows with Memory Allocation Optimization Figure 1.1: Relative throughput improvement achieved by upgrading from Windows PyTorch version 2.0.1 to 2.1.2 (higher is better). The graph illustrates that with the release of PyTorch 2.1.2, there has been a notable enhancement in CPU performance on the Windows platform. The degree of improvement varies across different models, which can be attributed to the diverse mix of operations they perform and their corresponding memory access patterns. While the BERT model shows a modest performance gain, models like ResNet50 and MobileNet-v3 Large benefit from more pronounced improvements. On a high-performance CPU, memory allocation becomes a performance bottleneck. This is also why addressing this issue has led to such significant performance improvements. As shown in the graphs below, we see that PyTorch CPU performance on Windows can significantly be improved. However, there is still a noticeable gap when compared to its performance on Linux. The absence of vectorization optimizations in the Windows variant of PyTorch CPU is a key factor to the remaining performance gap. Windows vs Linux Performance on PyTorch 2.0.1 Figure 1.2: Relative performance of Windows vs Linux with PyTorch version 2.0.1 (higher is better). Windows vs Linux Performance on PyTorch 2.1.2 Figure 1.3: Relative performance of Windows vs Linux with PyTorch version 2.1.2 (higher is better). Vectorization Optimization in PyTorch 2.4.1 and later Prior to PyTorch 2.4.1, the Windows build of PyTorch lacked SIMD for vectorization optimizations, a feature that the Linux build leveraged for improved performance. This discrepancy was due to the SLEEF Library’s integration issues on Windows, which is a SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT and is essential for efficient trigonometric calculations. Through a collaborative effort with engineers from ARM and Qualcomm, these challenges were resolved, enabling the integration of SIMD into PyTorch for Windows. The PyTorch 2.4.1 update has thus significantly enhanced PyTorch’s CPU performance on Windows, as shown in Figure 2.1. PyTorch CPU Performance Improvement on Windows with Vertorization Optimization Figure 2.1: Relative throughput improvement achieved by upgrading from PyTorch CPU version 2.1.2 to 2.4.1 (higher is better). As shown in the graph below, we see that PyTorch CPU performance on Windows ahieved the performance on Linux. Windows vs Linux Performance on PyTorch 2.4.1 Figure 2.2: Relative performance of Windows vs Linux with PyTorch version 2.4.1 (higher is better). CONCLUSION From PyTorch 2.0.1 to PyTorch 2.4.1, the CPU performance gap between Windows and Linux has been continuously narrowing. We compared the ratio of CPU performance on Windows to CPU performance on Linux across different versions, and the results are shown in the following graph. Windows vs Linux Performance on different version of PyTorch Figure 3: Performance Ratio for Windows to Linux with different version of PyTorch (higher is better). The graph shows that with PyTorch 2.4.1, CPU performance on Windows has nearly converged with that on Linux, and on some models, it has even surpassed Linux. For example, in the case of DistillBERT and RoBERTa models, the CPU performance ratio of Windows to Linux has achieved a remarkable 102%. However, certain models, including MobileNet-v3, still show a performance discrepancy. Intel engineers will continue to collaborate with Meta engineers, to reduce the performance gap of PyTorch CPU between Windows and Linux. HOW TO TAKE ADVANTAGE OF THE OPTIMIZATIONS Install PyTorch CPU 2.4.1 or later on Windows from the official repository, and you may automatically experience a performance boost with memory allocation and vectorizations. ACKNOWLEDGMENTS The results presented in this blog post was achieved through the collaborative effort of the Intel PyTorch team and Meta. We would like to express our sincere gratitude to Xu Han, Jiong Gong, Haozhe Zhu, Mingfei Ma, Chuanqi Wang, Guobing Chen and Eikan Wang. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here. Thanks to Jiachen Pu from community for his participation in the issue discussion and suggesting the use of mimalloc. We’d also like to express our gratitude to Microsoft for providing such an easily integrated and performant mallocation library. Thanks to Pierre Blanchard , Nathan Sircombe from ARM and Alex Reinking from Adobe for their contribution in overcome the compatibility issues with the sleef integrated to PyTorch Windows. Finally we want to thank Jing Xu, Weizhuo Zhang and Zhaoqiong Zheng for their contributions to this blog. Product and Performance Information The configurations in the table are collected with svr-info. Test by Intel on August 30, 2024. Specification Configuration1 Configuration2 Name ThinkBook 14 G5+ IRH ThinkBook 14 G5+ IRH Time Fri Aug 30 02:43:02 PM UTC 2024 Fri Aug 30 02:43:02 PM UTC 2024 System LENOVO LENOVO Baseboard LENOVO LENOVO Chassis LENOVO LENOVO CPU Model 13th Gen Intel(R) Core(TM) i7-13700H 13th Gen Intel(R) Core(TM) i7-13700H Microarchitecture Unknown Intel Unknown Intel Sockets 1 1 Cores per Socket 14 14 Hyperthreading Enabled Enabled CPUs 20 20 Intel Turbo Boost Enabled Enabled Base Frequency 2.4GHz 2.4GHz All-core Maximum Frequency 4.7GHz 4.7GHz Maximum Frequency 4.8GHz 4.8GHz NUMA Nodes 1 1 Prefetchers L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled PPINs - - Accelerators DLB, DSA, IAA, QAT DLB, DSA, IAA, QAT Installed Memory 32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s]) 32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s]) Hugepagesize 2048kb 2048kb Transparent Huge Pages madvise madvise Automatic NUMA Balancing Disabled Disabled NIC “1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation” “1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation” Disk Micron MTFDKBA512TFH 500G Micron MTFDKBA512TFH 500G BIOS LBCN22WW LBCN22WW Microcode 0x411c 0x411c OS Windows 11 Desktop Ubuntu 23.10 Kernel OS Build 19045.4412 6.5.0-27-generic TDP 200 watts 200 watts Power &amp;amp; Perf Policy Normal Powersave (7) Normal Powersave (7) Frequency Governor performance performance Frequency Driver intel_pstate intel_pstate Max C-State 9 9 Notices and Disclaimers Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation. Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Foundation Technical Advisory Council Elects New Leadership</title>
      <link href="https://pytorch.org/blog/tac-elects-new-leadership/" rel="alternate" type="text/html" title="PyTorch Foundation Technical Advisory Council Elects New Leadership" />
      <published>2024-10-08T00:00:00-07:00</published>
      <updated>2024-10-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/tac-elects-new-leadership</id>
      <content type="html" xml:base="https://pytorch.org/blog/tac-elects-new-leadership/">&lt;p&gt;We are pleased to announce the first-ever Chair and Vice Chair of the PyTorch Foundation’s Technical Advisory Council (TAC): &lt;strong&gt;Luca Antiga&lt;/strong&gt; as the Chair and &lt;strong&gt;Jiong Gong&lt;/strong&gt; as Vice Chair. Both leaders bring extensive experience and deep commitment to the PyTorch community, and they are set to guide the TAC in its mission to foster an open, diverse, and innovative PyTorch technical community.&lt;/p&gt;

&lt;h2 id=&quot;meet-the-new-leadership&quot;&gt;Meet the New Leadership&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tac-elects-new-leadership/luca-antiga.jpg&quot; alt=&quot;Luca Antiga&quot; style=&quot;max-width:350px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Luca Antiga is the CTO at Lightning AI since 2022. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings.&lt;/p&gt;

&lt;p&gt;“I am looking forward to taking on the role of the chair of the PyTorch TAC,” says Luca. “As the TAC chair, I will ensure effective, timely topic selection and enhance visibility of technical needs from the board members and from the ecosystem at large. I will strive for directional, cohesive messaging throughout the transition of PyTorch from Meta to the Linux Foundation.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tac-elects-new-leadership/jiong-gong.jpg&quot; alt=&quot;Jiong Gong&quot; style=&quot;max-width:350px; margin-top: 40px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Jiong Gong is a Principal Engineer and SW Architect for PyTorch Optimization from Intel. He serves as one of the PyTorch CPU module maintainers and is an active contributor to the TorchInductor CPU backend.&lt;/p&gt;

&lt;p&gt;“I plan to further strengthen the collaboration between PyTorch developers and hardware vendors, promoting innovation and performance optimization across various hardware platforms, enhancing PyTorch ecosystem and streamlining the decision-making process,” says Jiong. “I am honored to serve as the vice chair of the TAC.”&lt;/p&gt;

&lt;h2 id=&quot;what-does-the-tac-do&quot;&gt;What Does the TAC Do?&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation’s TAC provides a forum for technical communication, leadership,  and collaboration for the PyTorch Foundation. The committee members are members of the PyTorch Foundation.  The committee holds open meetings once a month that anyone in the community can attend.  The committee provides thought leadership on technical topics, knowledge sharing, and a forum to discuss issues with other technical experts in the community.&lt;/p&gt;

&lt;h2 id=&quot;new-tac-webpage&quot;&gt;New TAC Webpage&lt;/h2&gt;

&lt;p&gt;Stay connected with the PyTorch Foundation’s Technical Advisory Council (TAC) by visiting our new &lt;a href=&quot;/tac&quot;&gt;TAC webpage&lt;/a&gt;. Here you can find the TAC members, where to view upcoming meeting agendas, access presentations, attend public meetings, watch meeting recordings and participate in discussions on key technical topics.&lt;/p&gt;

&lt;p&gt;Plus stay tuned on our blog for regular updates from the PyTorch Foundation TAC leadership.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We are pleased to announce the first-ever Chair and Vice Chair of the PyTorch Foundation’s Technical Advisory Council (TAC): Luca Antiga as the Chair and Jiong Gong as Vice Chair. Both leaders bring extensive experience and deep commitment to the PyTorch community, and they are set to guide the TAC in its mission to foster an open, diverse, and innovative PyTorch technical community. Meet the New Leadership Luca Antiga is the CTO at Lightning AI since 2022. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings. “I am looking forward to taking on the role of the chair of the PyTorch TAC,” says Luca. “As the TAC chair, I will ensure effective, timely topic selection and enhance visibility of technical needs from the board members and from the ecosystem at large. I will strive for directional, cohesive messaging throughout the transition of PyTorch from Meta to the Linux Foundation.” Jiong Gong is a Principal Engineer and SW Architect for PyTorch Optimization from Intel. He serves as one of the PyTorch CPU module maintainers and is an active contributor to the TorchInductor CPU backend. “I plan to further strengthen the collaboration between PyTorch developers and hardware vendors, promoting innovation and performance optimization across various hardware platforms, enhancing PyTorch ecosystem and streamlining the decision-making process,” says Jiong. “I am honored to serve as the vice chair of the TAC.” What Does the TAC Do? The PyTorch Foundation’s TAC provides a forum for technical communication, leadership, and collaboration for the PyTorch Foundation. The committee members are members of the PyTorch Foundation. The committee holds open meetings once a month that anyone in the community can attend. The committee provides thought leadership on technical topics, knowledge sharing, and a forum to discuss issues with other technical experts in the community. New TAC Webpage Stay connected with the PyTorch Foundation’s Technical Advisory Council (TAC) by visiting our new TAC webpage. Here you can find the TAC members, where to view upcoming meeting agendas, access presentations, attend public meetings, watch meeting recordings and participate in discussions on key technical topics. Plus stay tuned on our blog for regular updates from the PyTorch Foundation TAC leadership.</summary>
      

      
      
    </entry>
  
</feed>



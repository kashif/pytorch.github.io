<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-05-02T09:14:13-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Announcing PyTorch Docathon June, 2024</title>
      <link href="https://pytorch.org/blog/docathon-june-2024/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon June, 2024" />
      <published>2024-05-02T00:00:00-07:00</published>
      <updated>2024-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/docathon-june-2024</id>
      <content type="html" xml:base="https://pytorch.org/blog/docathon-june-2024/">&lt;p&gt;We are thrilled to announce the upcoming PyTorch Docathon in June! The Docathon, akin to a hackathon, is an event dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Documentation is a vital component of any technology. By refining it, we can simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning. See our previous events &lt;a href=&quot;https://pytorch.org/blog/announcing-docathon/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/blog/announcing-docathon-h2-2023/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;Why Participate&lt;/h2&gt;

&lt;p&gt;The Docathon is an inclusive event designed to be accessible to newcomers, requiring only a basic understanding of Python, PyTorch, and Machine Learning, with some tasks not even requiring these skills. It offers a rewarding experience as participants can see the direct impact of their contributions on the project’s usability and accessibility. The Docathon promotes a collaborative environment, allowing participants to work with other contributors and PyTorch maintainers, fostering the exchange of ideas and networking. It also provides a rich learning experience, offering the opportunity to explore PyTorch modules, update docstrings, and test tutorials.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;June 4&lt;/strong&gt;: Kick-off&lt;br /&gt;
&lt;strong&gt;June 4-June 16&lt;/strong&gt;: Submissions and Feedback&lt;br /&gt;
&lt;strong&gt;June 17-18&lt;/strong&gt;: Final Reviews&lt;br /&gt;
&lt;strong&gt;June 20&lt;/strong&gt;: Winner Announcements&lt;/p&gt;

&lt;p&gt;Further details for the Docathon will be announced at the Kick-off call on June 4.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-pytorch-docathon-june-4-20th-2024/&quot;&gt;Please register to join this year’s event&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are thrilled to announce the upcoming PyTorch Docathon in June! The Docathon, akin to a hackathon, is an event dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Documentation is a vital component of any technology. By refining it, we can simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning. See our previous events here and here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Hitchhiker’s Guide to Speculative Decoding</title>
      <link href="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Speculative Decoding" />
      <published>2024-05-02T00:00:00-07:00</published>
      <updated>2024-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hitchhikers-guide-speculative-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/">&lt;p&gt;Speculative decoding is an optimization technique for inference that makes educated guesses about  future tokens while generating the current token, all within a single forward pass. It incorporates a verification mechanism to ensure the correctness of these speculated tokens, thereby guaranteeing that the overall output of speculative decoding is identical to that of vanilla decoding. Optimizing the cost of inference of large language models (LLMs) is arguably one of the most critical factors in reducing the cost of generative AI and increasing its adoption. Towards this goal, various inference optimization techniques are available, including custom kernels, dynamic batching of input requests, and quantization of large models.&lt;/p&gt;

&lt;p&gt;In this blog post, we provide a guide to speculative decoding and demonstrate how it can coexist with other optimizations. We are proud to open source the following, which includes the first speculator for Llama3 models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Speculator models for &lt;a href=&quot;https://huggingface.co/ibm-fms/llama3-8b-accelerator&quot;&gt;Meta Llama3 8B&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/ibm/granite-7b-lab-accelerator&quot;&gt;IBM Granite 7B lab&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/ibm-fms/codellama-13b-accelerator&quot;&gt;Meta Llama2 13B&lt;/a&gt;, and &lt;a href=&quot;https://huggingface.co/ibm-fms/codellama-13b-accelerator&quot;&gt;Meta Code Llama2 13B&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IBM/text-generation-inference/pull/79&quot;&gt;The code for inference via IBM’s fork of HF TGI.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp/pull/35&quot;&gt;The code for training your own speculators and corresponding recipes.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have deployed these speculators in an internal production-grade environment with thousands of daily users and observed 2x speedup on language models - Llama3 8B, Llama2 13B, and IBM Granite 7B and 3x speedup on IBM’s Granite 20B code models. We provide a detailed explanation of our approach in this &lt;a href=&quot;https://arxiv.org/abs/2404.19124&quot;&gt;technical report&lt;/a&gt; and are planning in-depth analysis in an upcoming ArXiv paper.&lt;/p&gt;

&lt;h2 id=&quot;speculative-decoding-inference&quot;&gt;Speculative decoding: Inference&lt;/h2&gt;

&lt;p&gt;We run IBM TGIS in our internal production environment that has optimizations such as continuous batching, fused kernels, and quantization kernels. To enable speculative decoding in TGIS, we modified the paged attention kernel from &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;. In what follows, we will describe the key changes to the inference engine to enable speculative decoding.&lt;/p&gt;

&lt;p&gt;Speculative decoding is based on the premise that the model is powerful enough to predict multiple tokens in a single forward pass. However, the current inference servers are optimized to predict only a single token at a time. In our approach, we attach multiple speculative heads (in addition to the usual one) to the LLM to predict &lt;em&gt;N+1-, N+2-, N+3-th …&lt;/em&gt; token. For example, 3 heads will predict 3 additional tokens. Details of the speculator architecture are explained in a later part of this blog. There are two challenges to achieve &lt;em&gt;efficiency&lt;/em&gt; and &lt;em&gt;correctness&lt;/em&gt; during inference - one is to predict without replicating KV-cache and the other is to verify that the predictions match the original model’s outcomes.&lt;/p&gt;

&lt;p&gt;In a typical generation loop, after the prompt is processed in a single forward step, a sequence length of 1 (next token predicted) is fed into the forward pass of the model along with the kv-cache. In a naive speculative decoding implementation, each speculative head would have its own kv-cache, but instead we  modify the paged attention kernel developed in the vLLM project to enable efficient kv-cache maintenance. This  ensures that throughput does not reduce at larger batch sizes. Further, we modify the attention masks to enable verification of the &lt;em&gt;N+1’th&lt;/em&gt; token and thus enable speculative decoding without deviating from the original model’s output. The details of this implementation are captured &lt;a href=&quot;https://github.com/foundation-model-stack/fms-extras&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We illustrate the speedup obtained with the Meta’s chat versions of Llama2 13B using a simple prompt.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig1.gif&quot; alt=&quot;Visual illustration of the non-speculative generation (left) compared to speculative generation (right)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: Visual illustration of the non-speculative generation (left) compared to speculative generation (right)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We deployed the above solution in an internal production environment. The figure below reports two metrics – time to first token (TTFT) and inter-token latency (ITL) with different numbers of concurrent users (which is captured in the numbers on the graph lines). We observe that the speculative decoding version is nearly twice as fast for the Llama2 13B chat model and nearly thrice as fast for the Granite 20B code model compared to the non-speculative version for all batch sizes. We observe similar behavior for the smaller models - IBM’s Granite 7B and Meta Llama3 8B models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig2.png&quot; alt=&quot;Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Llama 13B with number of concurrent users indicated on the graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Llama 13B with number of concurrent users indicated on the graph&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig3.png&quot; alt=&quot;Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Granite 20B Code with number of concurrent users indicated on the graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Granite 20B Code with number of concurrent users indicated on the graph&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;note-on-efficiency&quot;&gt;Note on efficiency&lt;/h3&gt;

&lt;p&gt;We performed numerous experiments to determine the right configuration for speculator training. These are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Speculator architecture&lt;/strong&gt;: The current approach allows for the number of heads to be modified, which maps to the number of tokens that we can look ahead. Increasing the number of heads also increases the amount of extra compute needed and complexity of  training. In practice, for language models, we find 3-4 heads works well in practice, whereas we found that code models can reap benefits from 6-8 heads.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compute&lt;/strong&gt;: Increasing the number of heads results in increased compute in two dimensions, one is that of increased latency for a single forward pass as well as the compute needed for multiple tokens. If the speculator is not accurate with more heads, it will result in wasted compute increasing the latency and reducing the throughput.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: The increased compute is offset by the roundtrips to HBM that need to be done for each forward pass. Note that if we get 3 tokens lookahead correct, we have saved three round trip times on HBM.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We settled on 3-4 heads for the language models and 6-8 heads for the code models and across different model sizes ranging from 7B to 20B, we observed significant latency improvements without throughput loss compared to non-speculative decoding. We begin to observe throughput reduction beyond a batch size of 64, which happens rarely in practice.&lt;/p&gt;

&lt;h2 id=&quot;speculative-decoding-training&quot;&gt;Speculative decoding: Training&lt;/h2&gt;

&lt;p&gt;There are  two broad approaches for speculative decoding, one is to leverage a smaller model (e.g., Llama 7B as a speculator for Llama 70B) and the other is to attach speculator heads (and train them). In our experiments, we find the approach of attaching speculator heads to be more   effective both in model quality and latency gains.&lt;/p&gt;

&lt;h3 id=&quot;speculator-architecture&quot;&gt;Speculator architecture&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10774&quot;&gt;Medusa&lt;/a&gt; made speculative decoding popular; their approach is to add a head to the existing model which is then trained to do speculation. We modify the Medusa architecture by making the “heads” hierarchical, where each head stage predicts a single token and then feeds it to the next head stage. These multi-stage heads are depicted in the below figure. We are exploring ways of minimizing the embeddings table by sharing these across the multiple stages and base model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig4.png&quot; alt=&quot;A simple architecture diagram for a 3-headed multi-stage  speculator. Z is the state from the base model.&quot; style=&quot;width:100%;display:block;max-width:300px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: A simple architecture diagram for a 3-headed multi-stage  speculator. Z is the state from the base model.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;speculator-training&quot;&gt;Speculator training&lt;/h4&gt;

&lt;p&gt;We have a two-phase approach to training a speculator for efficiency reasons. In the first phase, we train on small batches with long sequence lengths (4k tokens) and use the standard causal LM approach for training. In phase 2, we use large batches with short sequence lengths (256 tokens) generated from the base model. In this training phase, we tune the heads to match the output of the base model. Through numerous experiments, we find that a 5:2 ratio of steps for phase 1 vs phase 2 works well. We depict the progress of these phases in the below figure. We use PyTorch FSDP and &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;IBM FMS&lt;/a&gt; for the training of speculators.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig5.jpg&quot; alt=&quot;Per-head training loss curves for Llama2-13B speculator training, phase 1 and 2&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: Per-head training loss curves for Llama2-13B speculator training, phase 1 and 2&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;Through this blog, we are releasing  a new approach for speculative decoding and the following assets:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Models for improving the inter-token latencies for a range of models - Llama3 8B, Llama2 13B, Granite 7B, and CodeLlama 13B&lt;/li&gt;
  &lt;li&gt;Production quality code for inference&lt;/li&gt;
  &lt;li&gt;Recipes for training speculators&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We are working on training speculators for Llama3 70B and Mistral models and invite the community to contribute as well as help improve on our framework. We would also love to work with major open source serving frameworks such as &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt; and &lt;a href=&quot;https://github.com/huggingface/text-generation-inference&quot;&gt;TGI&lt;/a&gt; to contribute back our speculative decoding approach to benefit the community.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;

&lt;p&gt;There are several teams that helped us get to these latency improvements for inference. We would like to thank the vLLM team for creating the paged attention kernel in a clean and reusable manner. We extend our gratitude to the Team PyTorch at Meta that helped provide feedback on this blog as well as continued efforts on optimal usage of PyTorch. Special thanks to our internal production teams at IBM Research who took this prototype to production and hardened it. A shout out to Stas Bekman for providing insightful comments on the blog resulting in an improved explanation of the tradeoffs between compute, memory, and speculator effectiveness.&lt;/p&gt;

&lt;p&gt;The paged attention kernel was integrated into IBM FMS by Josh Rosenkranz and Antoni Viros i Martin. The speculator architecture and training was done by Davis Wertheimer, Pavithra Ranganathan, and Sahil Suneja. The integration of the modeling code with the inference server was done by Thomas Parnell, Nick Hill, and Prashant Gupta.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at IBM</name>
        
        
      </author>

      

      

      
        <summary type="html">Speculative decoding is an optimization technique for inference that makes educated guesses about future tokens while generating the current token, all within a single forward pass. It incorporates a verification mechanism to ensure the correctness of these speculated tokens, thereby guaranteeing that the overall output of speculative decoding is identical to that of vanilla decoding. Optimizing the cost of inference of large language models (LLMs) is arguably one of the most critical factors in reducing the cost of generative AI and increasing its adoption. Towards this goal, various inference optimization techniques are available, including custom kernels, dynamic batching of input requests, and quantization of large models.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Llama3 FP8 Inference with Triton Kernels</title>
      <link href="https://pytorch.org/blog/accelerating-llama3/" rel="alternate" type="text/html" title="Accelerating Llama3 FP8 Inference with Triton Kernels" />
      <published>2024-05-01T00:00:00-07:00</published>
      <updated>2024-05-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-llama3</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-llama3/">&lt;h2 id=&quot;10-summary&quot;&gt;1.0 Summary&lt;/h2&gt;

&lt;p&gt;We present an optimized Triton FP8 GEMM (General Matrix-Matrix Multiply) kernel TK-GEMM, which leverages SplitK parallelization. For small batch size inference, TK-GEMM delivers up to &lt;strong&gt;1.94x&lt;/strong&gt; over the base Triton matmul implementation, &lt;strong&gt;1.87x&lt;/strong&gt; speedup over cuBLAS FP8 and &lt;strong&gt;1.71x&lt;/strong&gt; over cuBLAS FP16 for Llama3-70B inference problem sizes on NVIDIA H100 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig1.png&quot; alt=&quot;TK-GEMM Speedup over PyTorch (calling cuBLAS) for Llama3-70B Attention Layer Matrix Shapes (N=K=8192)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; TK-GEMM Speedup over PyTorch (calling cuBLAS) for Llama3-70B Attention Layer Matrix Shapes (N=K=8192)&lt;/p&gt;

&lt;p&gt;In this blog, we will cover how we designed an optimized kernel using &lt;a href=&quot;https://github.com/openai/triton&quot;&gt;Triton&lt;/a&gt; for FP8 inference and tuned it for Lama3-70B inference. We will cover FP8 (8-bit floating point), a new datatype supported by Hopper generation GPUs (SM90), the key SM90 features that Triton supports, and how we modified the parallelization to be able to maximize memory throughput for memory-bound (inference) problem sizes.&lt;/p&gt;

&lt;p&gt;We also dedicate a section on CUDA graphs, an important technology that will help materialize kernel level speedups and enable developers who want to use Triton kernels in production settings to get additional performance gain.&lt;/p&gt;

&lt;p&gt;Repo and code available at: &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai&quot;&gt;https://github.com/pytorch-labs/applied-ai&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;20-fp8-datatype&quot;&gt;2.0 FP8 Datatype&lt;/h2&gt;

&lt;p&gt;The FP8 datatype was &lt;a href=&quot;https://arxiv.org/pdf/2209.05433.pdf&quot;&gt;introduced&lt;/a&gt; jointly by Nvidia, Arm and Intel and serves as a successor to 16-bit floating point types.  With half the bit count, it has the potential to provide significant throughput improvements over its predecessors for Transformer networks. The FP8 datatype consists of 2 formats:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E4M3&lt;/strong&gt; (4-bit exponent and 3-bit mantissa).  Able to store +/ 448 and nan.&lt;br /&gt;
&lt;strong&gt;E5M2&lt;/strong&gt; (5-bit exponent and 2-bit mantissa).  Able to store +/- 57,334, nan and inf.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig2.png&quot; alt=&quot;BF16, FP16, FP8 E4M3 and FP8 E5M2&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Above:&lt;/strong&gt; &lt;em&gt;BF16, FP16, FP8 E4M3 and FP8 E5M2.&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;To show precision differences, the closest representation to 0.3952 is shown in each format.&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html&quot;&gt;Nvidia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We use E4M3 in inference and forward pass training due its higher precision and E5M2 in training backward pass due to its higher dynamic range. Nvidia has designed their H100 FP8 Tensor Core to provide a peak of 3958 TFLOPS, &lt;strong&gt;2x&lt;/strong&gt; the FLOPS of the FP16 Tensor Core.&lt;/p&gt;

&lt;p&gt;We designed our Triton kernel with these hardware innovations in mind and in the rest of the blog we will discuss methods to leverage and verify that these features are indeed being utilized by the Triton compiler.&lt;/p&gt;

&lt;h2 id=&quot;30-triton-hopper-support-and-fp8-tensor-core-instruction&quot;&gt;3.0 Triton Hopper Support and FP8 Tensor Core Instruction&lt;/h2&gt;

&lt;p&gt;The Hopper GPU architecture has added the following &lt;a href=&quot;https://arxiv.org/abs/2402.13499&quot;&gt;new features&lt;/a&gt; that we can expect will accelerate FP8 GEMM.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TMA (Tensor Memory Accelerator) Hardware Unit&lt;/li&gt;
  &lt;li&gt;WGMMA (Warp Group Matrix Multiply-Accumulate Instruction)&lt;/li&gt;
  &lt;li&gt;Threadblock Clusters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Triton currently takes advantage of one of these features, the &lt;em&gt;wgmma&lt;/em&gt; instruction, whereas PyTorch (calling cuBLAS) leverages all 3 which makes these speedups even more impressive. To fully take advantage of the Hopper FP8 Tensor Core, the wgmma is necessary even though the older mma.sync instruction is still supported.&lt;/p&gt;

&lt;p&gt;The key difference between the mma and wgmma instructions is that instead of 1 CUDA warp being responsible for an output shard, an entire warp group, 4 CUDA warps, &lt;em&gt;asynchronously&lt;/em&gt; contributes to an output shard.&lt;/p&gt;

&lt;p&gt;To see what this instruction looks like in practice, and to verify that our Triton Kernel is indeed utilizing this feature we analyzed the PTX and SASS assembly using &lt;a href=&quot;https://developer.nvidia.com/nsight-compute&quot;&gt;nsight compute&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig3.png&quot; alt=&quot;PTX Assembly&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; PTX Assembly&lt;/p&gt;

&lt;p&gt;This instruction is further lowered into a QGMMA instruction in SASS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig4.png&quot; alt=&quot;SASS Assembly&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; SASS Assembly&lt;/p&gt;

&lt;p&gt;Both instructions tell us that we are multiplying two FP8 E4M3 input tensors and accumulating in F32, which confirms that the TK-GEMM Kernel is utilizing the FP8 Tensor Core and the lowering is being done correctly.&lt;/p&gt;

&lt;h2 id=&quot;40-splitk-work-decomposition&quot;&gt;4.0 SplitK Work Decomposition&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig5.png&quot; alt=&quot;TK-GEMM vs Base Triton GEMM TFLOPS for M = 1-64&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; TK-GEMM vs Base Triton GEMM TFLOPS for M = 1-64&lt;/p&gt;

&lt;p&gt;The base Triton FP8 GEMM implementation does &lt;a href=&quot;https://github.com/openai/triton/issues/3104&quot;&gt;not perform&lt;/a&gt; well for the small M regime, where for a matrix multiplication of A (&lt;em&gt;MxN&lt;/em&gt;) x B (&lt;em&gt;NxK&lt;/em&gt;), &lt;em&gt;M&lt;/em&gt; &amp;lt; &lt;em&gt;N&lt;/em&gt;, &lt;em&gt;K&lt;/em&gt;. To optimize for this type matrix profile we applied a SplitK work decomposition instead of the Data Parallel decomposition found in the base Triton kernel. This greatly improved latencies for the small M regime.&lt;/p&gt;

&lt;p&gt;For background, SplitK launches additional thread blocks along the k dimension to calculate partial output sums. The partial results from each thread block are then summed using an atomic reduction.  This allows for finer grained work decomposition with resultant performance improvements.  More details on SplitK are available in our &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;arxiv paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After carefully tuning the other relevant hyperparameters for our kernel such as tile sizes, number of warps and the number of pipeline stages to Llama3-70B problem sizes we were able to produce up to &lt;strong&gt;1.94x&lt;/strong&gt; speedup over the Triton &lt;a href=&quot;https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html&quot;&gt;base implementation&lt;/a&gt;. For a more comprehensive introduction to hyperparameter tuning, see our &lt;a href=&quot;https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig6.png&quot; alt=&quot;NCU profiler times for TK-GEMM under varying batch sizes, and compared with PyTorch (calling cuBLAS) FP8 and FP16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Above&lt;/strong&gt;: &lt;em&gt;NCU profiler times for TK-GEMM under varying batch sizes, and compared with PyTorch (calling cuBLAS) FP8 and FP16.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that starting at M=32, the cuBLAS FP8 kernel starts to outperform TK-GEMM. For M &amp;gt;= 32, we suspect that hyperparameters we found are not optimal, and thus another set of experiments is required to determine the optimal parameters for the mid-sized M regime.&lt;/p&gt;

&lt;h2 id=&quot;50-cuda-graphs-to-enable-end-to-end-speedup&quot;&gt;5.0 CUDA Graphs to Enable End-to-End Speedup&lt;/h2&gt;

&lt;p&gt;To be able to realize these speedups in an end-to-end setting, we must take into account both the kernel execution time (GPU duration) as well as the wall time (CPU+GPU) duration. Triton kernels, which are handwritten (as opposed to torch compile generated) are known to suffer from high-kernel launch latencies. If we use &lt;a href=&quot;https://pytorch.org/docs/stable/profiler.html&quot;&gt;torch profiler&lt;/a&gt; to trace the TK-GEMM kernel we can see the call stack on the CPU side to pinpoint exactly what is causing the slowdown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig7.png&quot; alt=&quot;CPU Launch Overhead: 2.413ms&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; CPU Launch Overhead: 2.413ms&lt;/p&gt;

&lt;p&gt;From above, we see that the majority of the wall time of our optimized kernel is dominated by JIT (Just-in-Time) compilation overhead. To combat this we can use CUDA graphs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig8.png&quot; alt=&quot;CUDA Graphs Visualization&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; CUDA Graphs Visualization&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/&quot;&gt;PyTorch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key idea is instead of multiple kernel launches, we instead can create and instantiate a graph (1 time cost) and then submit that instance of the graph for execution. To illustrate this point we simulate a Llama3-70B Attention layer, As shown in the below figure generated using &lt;a href=&quot;https://developer.nvidia.com/nsight-systems&quot;&gt;nsight systems&lt;/a&gt;, the time between each GEMM is &lt;strong&gt;&lt;em&gt;165us&lt;/em&gt;&lt;/strong&gt; compared to the &lt;strong&gt;&lt;em&gt;12us&lt;/em&gt;&lt;/strong&gt; spent on the actual matmul due the CPU kernel launch overhead. This means that &lt;strong&gt;&lt;em&gt;92%&lt;/em&gt;&lt;/strong&gt; of the time of the time in an Attention layer the GPU is idle and not doing any work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig9.png&quot; alt=&quot;Simulated Llama3-70B Attention Layer with TK-GEMM&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Simulated Llama3-70B Attention Layer with TK-GEMM&lt;/p&gt;

&lt;p&gt;To show the impact of CUDA graphs, we then created a graph of the TK-GEMM kernel in the toy Attention layer and replayed the graph. Below, we can see that the gaps between kernel executions are reduced to 6.65us.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig10.png&quot; alt=&quot;Simulated Llama3-70B Attention Layer with TK-GEMM and CUDA Graphs&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Simulated Llama3-70B Attention Layer with TK-GEMM and CUDA Graphs&lt;/p&gt;

&lt;p&gt;In practice, this optimization would result in a &lt;strong&gt;6.4x&lt;/strong&gt; speedup of a single attention layer in Llama3-70B, over naively using TK-GEMM in a model without CUDA graphs.&lt;/p&gt;

&lt;h2 id=&quot;60-potential-future-optimization-paths&quot;&gt;6.0 Potential Future Optimization Paths&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig11.png&quot; alt=&quot;TMA Hardware Unit&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; TMA Hardware Unit&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/&quot;&gt;Nvidia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Nvidia H100 features a TMA hardware unit. The dedicated TMA unit frees up registers and threads to do other work, as address generation is completely handled by the TMA. For memory bound problem sizes, this can provide even further gain when Triton enables support for this feature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig12.png&quot; alt=&quot;Tensor Core Utilization (Arrows Indicate Degrees of Freedom)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; Tensor Core Utilization (Arrows Indicate Degrees of Freedom)&lt;/p&gt;

&lt;p&gt;To identify how well we are utilizing the Tensor Core, we can analyze the roofline chart. Notice that we are in the memory-bound region as expected for small M. To improve kernel latency we can either increase the arithmetic intensity, which with a fixed problem size can only be achieved through exploiting data locality and other loop &lt;a href=&quot;https://www.codee.com/is-your-algorithm-running-at-peak-performance-the-roofline-model/&quot;&gt;optimizations&lt;/a&gt; or increasing the memory throughput. This requires either a more optimal parallel algorithm specialized for the FP8 datatype as well as the type of problem size characteristics we expect to see in FP8 inference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig13.png&quot; alt=&quot;DRAM Throughput Circled, 1.65TB/s vs Peak 3.35TB/s on H100 (M=16, N=8192, K=8192)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; DRAM Throughput Circled, 1.65TB/s vs Peak 3.35TB/s on H100 (M=16, N=8192, K=8192)&lt;/p&gt;

&lt;p&gt;Lastly, we can see that we are only achieving around &lt;strong&gt;50%&lt;/strong&gt; of peak DRAM throughput on the NVIDIA H100. High performance GEMM kernels typically achieve around &lt;strong&gt;70-80%&lt;/strong&gt; of peak throughput. This means that there is still a lot of room to improve and the techniques mentioned above (loop unrolling, optimized parallelization) are needed for additional gain.&lt;/p&gt;

&lt;h2 id=&quot;70-future-work&quot;&gt;7.0 Future Work&lt;/h2&gt;

&lt;p&gt;For future research, we would like to explore &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main&quot;&gt;CUTLASS&lt;/a&gt; 3.x and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/media/docs/cute&quot;&gt;CuTe&lt;/a&gt; to leverage more direct control over Hopper features especially in terms of obtaining direct TMA control and exploring pingpong architectures, which have shown promising results for FP8 GEMM.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Chih Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">1.0 Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ExecuTorch Alpha: Taking LLMs and AI to the Edge with Our Community and Partners</title>
      <link href="https://pytorch.org/blog/executorch-alpha/" rel="alternate" type="text/html" title="ExecuTorch Alpha: Taking LLMs and AI to the Edge with Our Community and Partners" />
      <published>2024-04-30T00:00:00-07:00</published>
      <updated>2024-04-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/executorch-alpha</id>
      <content type="html" xml:base="https://pytorch.org/blog/executorch-alpha/">&lt;p&gt;We are excited to announce the release of &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch alpha&lt;/a&gt;, focused on deploying large language models (LLMs) and large ML models to the edge, stabilizing the API surface, and improving our installation processes. It has been an exciting few months &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;from our 0.1 (preview) release&lt;/a&gt; in collaboration with our partners at Arm, Apple, and Qualcomm Technologies, Inc.&lt;/p&gt;

&lt;p&gt;In this post we’ll discuss our full support for Meta’s Llama 2, early support for Meta’s Llama 3, broad model support in ExecuTorch, and highlight the important work our partners have done to move us forward.&lt;/p&gt;

&lt;h2 id=&quot;large-language-models-on-mobile&quot;&gt;Large Language Models on Mobile&lt;/h2&gt;

&lt;p&gt;Mobile devices are highly constrained for compute, memory, and power. To bring LLMs to these devices, we heavily leverage quantization and other techniques to pack these models appropriately.&lt;/p&gt;

&lt;p&gt;ExecuTorch alpha supports 4-bit post-training quantization using GPTQ. We’ve provided broad device support on CPU by landing dynamic shape support and new dtypes in XNNPack. We’ve also made significant improvements in export and lowering, reduced memory overhead and improved runtime performance. This enables running Llama 2 7B efficiently on iPhone 15 Pro, iPhone 15 Pro Max, Samsung Galaxy S22, S23, and S24 phones and other edge devices. &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;Early support&lt;/a&gt; for &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3/&quot;&gt;Llama 3 8B&lt;/a&gt; is also included. We are always improving the token/sec on various edge devices and you can visit GitHub for the &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/models/llama2/README.md&quot;&gt;latest performance numbers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’re working closely with our partners at Apple, Arm, and Qualcomm Technologies to delegate to GPU and NPU for performance through Core ML, MPS, TOSA, and Qualcomm AI Stack backends respectively.&lt;/p&gt;

&lt;h2 id=&quot;supported-models&quot;&gt;Supported Models&lt;/h2&gt;

&lt;p&gt;We remain committed to supporting an ever-expanding list of models with ExecuTorch. Since preview, we have significantly expanded our tested models across NLP, vision and speech, with full details &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;in our release notes&lt;/a&gt;. Although support for on-device LLMs is early, we anticipate most traditional models to function seamlessly out of the box, with delegation to XNNPACK, Core ML, MPS, TOSA, and HTP for performance. If you encounter any problems please open &lt;a href=&quot;https://github.com/pytorch/executorch/issues&quot;&gt;a GitHub issue&lt;/a&gt; with us.&lt;/p&gt;

&lt;h2 id=&quot;productivity&quot;&gt;Productivity&lt;/h2&gt;

&lt;p&gt;Deploying performant models tuned for specific platforms often require deep visualization into the on-device runtime data to determine the right changes to make in the original PyTorch model. With ExecuTorch alpha, we provide a powerful SDK with observability throughout the process from model authoring to deployment, including delegate and hardware-level information.&lt;/p&gt;

&lt;p&gt;The ExecuTorch SDK was enhanced to include better debugging and profiling tools. Because ExecuTorch is built on PyTorch, the debugging capabilities include the ability to map from operator nodes back to original Python source code for more efficient anomaly resolution and performance tuning for both delegated and non-delegated model instances. You can learn more about the ExecuTorch SDK &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/sdk/README.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;partnerships&quot;&gt;Partnerships&lt;/h2&gt;

&lt;p&gt;ExecuTorch has only been possible because of strong collaborations across Arm, Apple, and  Qualcomm Technologies. The collaboration for the initial launch of ExecuTorch continues as we support LLMs and large AI models on the edge for PyTorch. As we’ve seen with this early work for ExecuTorch alpha, there are unique challenges with these larger models and we’re excited to develop in the open.&lt;/p&gt;

&lt;p&gt;We also want to highlight the great partnership with Google on &lt;a href=&quot;https://github.com/google/XNNPACK&quot;&gt;XNNPACK&lt;/a&gt; for CPU performance. The teams continue to work together upstreaming our changes and across the TensorFlow and PyTorch teams to make sure we can all support generative AI models on the edge with SOTA performance.&lt;/p&gt;

&lt;p&gt;Lastly, our hardware partner MediaTek has been doing work enabling the Llama collection of models with ExecuTorch on their SoCs. We’ll have more to share in the future.&lt;/p&gt;

&lt;h2 id=&quot;alpha-and-production-usage&quot;&gt;Alpha and Production Usage&lt;/h2&gt;

&lt;p&gt;With our alpha release, we have production-tested ExecuTorch. Meta is using ExecuTorch for hand tracking on Meta Quest 3 and a variety of models on Ray-Ban Meta Smart Glasses. In addition, we have begun the rollout of ExecuTorch with Instagram and are integrating with other Meta products. We are excited to see how ExecuTorch can be used for other edge experiences.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;We are excited to see various efforts in the community to adopt or contribute to ExecuTorch. For instance, Unity recently &lt;a href=&quot;https://schedule.gdconf.com/session/unity-developer-summit-drive-better-gameplay-experiences-on-user-devices-with-ai-presented-by-unity/903634&quot;&gt;shared their work&lt;/a&gt; at the Game Developers Conference (&lt;a href=&quot;https://gdconf.com/&quot;&gt;GDC&lt;/a&gt;) on leveraging ExecuTorch and Edge IR to run PyTorch models with their neural network inference library Sentis. Leveraging ExecuTorch’s hackability and extensibility, Unity introduced their own custom backend that serializes ExecuTorch’s Edge Dialect IR into Sentis’ native serialized format enabling developers to begin using PyTorch models easily in their games and apps.&lt;/p&gt;

&lt;p&gt;We’ve been building and innovating with ExecuTorch in the open. Our north star is to empower the community to deploy any ML model on edge devices painlessly and efficiently. Whether you are a hobbyist or this is your day job, we’d love for you to &lt;a href=&quot;https://pytorch.org/executorch/stable/getting-started-setup.html&quot;&gt;jump in to bring your ML models to the edge&lt;/a&gt;. We are looking for your help to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use ExecuTorch to &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/docs/source/llm/getting-started.md&quot;&gt;run your LLM models locally&lt;/a&gt; on various deployment targets and share your feedback&lt;/li&gt;
  &lt;li&gt;Expand our supported models, including bug reports&lt;/li&gt;
  &lt;li&gt;Expand our quantization schemes&lt;/li&gt;
  &lt;li&gt;Help us build out delegates to GPU and NPU&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To all individual contributors and early adopters of ExecuTorch, a big thank you as well. We can’t wait to have more of you &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;join us&lt;/a&gt;!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of ExecuTorch alpha, focused on deploying large language models (LLMs) and large ML models to the edge, stabilizing the API surface, and improving our installation processes. It has been an exciting few months from our 0.1 (preview) release in collaboration with our partners at Arm, Apple, and Qualcomm Technologies, Inc.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.3 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-3/" rel="alternate" type="text/html" title="PyTorch 2.3 Release Blog" />
      <published>2024-04-24T00:00:00-07:00</published>
      <updated>2024-04-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-3</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-3/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.3 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.3.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;

&lt;p&gt;This release is composed of 3393 commits and 426 contributors since PyTorch 2.2. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.3. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;User-defined Triton kernels in torch.compile
   &lt;/td&gt;
   &lt;td&gt;torch.export adds new API to specify dynamic_shapes
   &lt;/td&gt;
   &lt;td&gt;Weight-Only-Quantization introduced into Inductor CPU backend
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Tensor parallelism within PyTorch Distributed
   &lt;/td&gt;
   &lt;td&gt;Asynchronous checkpoint generation
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Support for semi-structured sparsity
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-support-for-user-defined-triton-kernels-in-torchcompile&quot;&gt;[Beta] Support for User-defined Triton kernels in &lt;em&gt;torch.compile&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Allows for PyTorch code that contains triton kernels to be executed natively using torch.compile. This enables users to migrate code containing triton kernels from eager PyTorch to &lt;em&gt;torch.compile&lt;/em&gt; without running into performance regressions or graph breaks. Native support also creates an opportunity for Torch Inductor to precompile the user-defined Triton kernel as well as better organize code around the Triton kernel allowing for further optimizations.&lt;/p&gt;

&lt;p&gt;You can find more information about how to utilize user defined Triton kernels in torch.compile within &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensor-parallelism-introduces-more-efficient-ways-to-train-llms&quot;&gt;[Beta] Tensor Parallelism introduces more efficient ways to train LLMs&lt;/h3&gt;

&lt;p&gt;The Tensor Parallel API facilitates various tensor manipulations across GPUs/hosts and integrates with FSDP for 2D Parallelism (Tensor parallelism across devices + Data Parallelism across hosts). It also offers a low-level API for constructing higher-level Tensor parallel APIs. This API has been validated to support the training of transformer models with over 100 billion parameters.&lt;/p&gt;

&lt;p&gt;You can find more information on how to utilize this within your workflows within &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TP_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-semi-structured-sparsity-provides-users-with-a-way-to-take-advantage-of-accelerated-sparse-inference-and-memory-savings&quot;&gt;[Beta] Semi-structured sparsity provides users with a way to take advantage of accelerated sparse inference and memory savings&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;torch.sparse.SparseSemiStructuredTensor&lt;/em&gt; implements semi-structured sparsity as a Tensor subclass, which have observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;

&lt;p&gt;In particular it adds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Additional support for quantization composability (mixed dtype, dequant fusion)&lt;/li&gt;
  &lt;li&gt;Updated cuSPARSELt and CUTLASS kernels&lt;/li&gt;
  &lt;li&gt;torch.compile support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find more information on how to take advantage of semi-structured sparsity &lt;a href=&quot;https://pytorch.org/tutorials/advanced/semi_structured_sparse.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;prototype-torchexport-adds-new-api-to-specify-dynamic_shapes&quot;&gt;[PROTOTYPE] &lt;em&gt;torch.export&lt;/em&gt; adds new API to specify &lt;em&gt;dynamic_shapes&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;You can now use &lt;em&gt;torch.export.Dim&lt;/em&gt; to better represent dynamic shapes by enabling developers to specify ranges (min and max values) that can be reused across different input dimensions that are constrained to be equal.&lt;/p&gt;

&lt;p&gt;To learn more about &lt;em&gt;torch.export.Dim&lt;/em&gt; as well as how it can be used to express more interesting relationships (such as linear arithmetic expressions) check out the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-asynchronous-checkpoint-generation&quot;&gt;[PROTOTYPE] Asynchronous checkpoint generation&lt;/h3&gt;

&lt;p&gt;Asynchronous checkpoint generation allows users to continue their training loops while checkpoints are being generated, essentially offloading much of the checkpointing cost.&lt;/p&gt;

&lt;p&gt;You can find out how to utilize this within your own workflows with this &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/2.3/torch/distributed/checkpoint/examples/async_checkpointing_example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;prototype-weight-only-quantization-introduced-into-inductor-cpu-backend&quot;&gt;[PROTOTYPE] Weight-Only-Quantization introduced into Inductor CPU backend&lt;/h3&gt;

&lt;p&gt;PyTorch 2.3 enhances LLM inference performance on torch inductor CPU backend. The project &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;gpt-fast&lt;/a&gt; offers a simple and efficient PyTorch native acceleration for transformer text generation with &lt;em&gt;torch.compile&lt;/em&gt;. Prior to 2.3 only CUDA devices were supported and this feature enables the CPU counterpart by providing highly optimized kernels for the int4 and int8 weight only quantization Linear.&lt;/p&gt;

&lt;p&gt;For more information / how to utilize this feature please refer to the &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast#quantization&quot;&gt;gpt-fast README&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.3 (release note)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">torchtune: Easily fine-tune LLMs using PyTorch</title>
      <link href="https://pytorch.org/blog/torchtune-fine-tune-llms/" rel="alternate" type="text/html" title="torchtune: Easily fine-tune LLMs using PyTorch" />
      <published>2024-04-16T00:00:00-07:00</published>
      <updated>2024-04-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchtune-fine-tune-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchtune-fine-tune-llms/">&lt;p&gt;We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.&lt;/p&gt;

&lt;p&gt;Staying true to PyTorch’s design principles, torchtune provides composable and modular building blocks along with easy-to-extend training recipes to fine-tune popular LLMs on a variety of consumer-grade and professional GPUs.&lt;/p&gt;

&lt;p&gt;torchtune supports the full fine-tuning workflow from start to finish, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Downloading and preparing datasets and model checkpoints.&lt;/li&gt;
  &lt;li&gt;Customizing the training with composable building blocks that support different model architectures, parameter-efficient fine-tuning (PEFT) techniques, and more.&lt;/li&gt;
  &lt;li&gt;Logging progress and metrics to gain insight into the training process.&lt;/li&gt;
  &lt;li&gt;Quantizing the model post-tuning.&lt;/li&gt;
  &lt;li&gt;Evaluating the fine-tuned model on popular benchmarks.&lt;/li&gt;
  &lt;li&gt;Running local inference for testing fine-tuned models.&lt;/li&gt;
  &lt;li&gt;Checkpoint compatibility with popular production inference systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started, jump right into the &lt;a href=&quot;https://www.github.com/pytorch/torchtune&quot;&gt;code&lt;/a&gt; or walk through our many &lt;a href=&quot;https://pytorch.org/torchtune/main/&quot;&gt;tutorials&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;why-torchtune&quot;&gt;Why torchtune?&lt;/h2&gt;

&lt;p&gt;Over the past year there has been an explosion of interest in open LLMs. Fine-tuning these state of the art models has emerged as a critical technique for adapting them to specific use cases. This adaptation can require extensive customization from dataset and model selection all the way through to quantization, evaluation and inference. Moreover, the size of these models poses a significant challenge when trying to fine-tune them on consumer-level GPUs with limited memory.&lt;/p&gt;

&lt;p&gt;Existing solutions make it hard to add these customizations or optimizations by hiding the necessary pieces behind layers of abstractions. It’s unclear how different components interact with each other and which of these need to be updated to add new functionality. torchtune empowers developers to adapt LLMs to their specific needs and constraints with full control and visibility.&lt;/p&gt;

&lt;h2 id=&quot;torchtunes-design&quot;&gt;torchtune’s Design&lt;/h2&gt;

&lt;p&gt;torchtune was built with the following principles in mind&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Easy extensibility&lt;/strong&gt; - New techniques emerge all the time and everyone’s fine-tuning use case is different. torchtune’s recipes are designed around easily composable components and hackable training loops, with minimal abstraction getting in the way of fine-tuning your fine-tuning. Each &lt;a href=&quot;https://github.com/pytorch/torchtune/tree/main/recipes&quot;&gt;recipe&lt;/a&gt; is self-contained - no trainers or frameworks, and is designed to be easy to read - less than 600 lines of code!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Democratize fine-tuning&lt;/strong&gt; - Users, regardless of their level of expertise, should be able to use torchtune. Clone and modify configs, or get your hands dirty with some code! You also don’t need beefy data center GPUs. Our memory efficient recipes have been tested on machines with a single 24GB gaming GPU.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability with the OSS LLM ecosystem&lt;/strong&gt; - The open source LLM ecosystem is absolutely thriving, and torchtune takes advantage of this to provide interoperability with a wide range of offerings. This flexibility puts you firmly in control of how you train and use your fine-tuned models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Over the next year, open LLMs will become even more powerful, with support for more languages (multilingual), more modalities (multimodal) and more tasks. As the complexity of these models increases, we need to pay the same attention to “how” we design our libraries as we do to the features provided or performance of a training run. Flexibility will be key to ensuring the community can maintain the current pace of innovation, and many libraries/tools will need to play well with each other to power the full spectrum of use cases. torchtune is built from the ground up with this future in mind.&lt;/p&gt;

&lt;p&gt;In the true PyTorch spirit, torchtune makes it easy to get started by providing integrations with some of the most popular tools for working with LLMs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/en/index&quot;&gt;Hugging Face Hub&lt;/a&gt;&lt;/strong&gt; - Hugging Face provides an expansive repository of open source models and datasets for fine-tuning. torchtune seamlessly integrates through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune download&lt;/code&gt; CLI command so you can get started right away with fine-tuning your first model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;PyTorch FSDP&lt;/a&gt;&lt;/strong&gt; - Scale your training using PyTorch FSDP. It is very common for people to invest in machines with multiple consumer level cards like the 3090/4090 by NVidia. torchtune allows you to take advantage of these setups by providing distributed recipes powered by FSDP.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/strong&gt; - torchtune uses the Weights &amp;amp; Biases AI platform to log metrics and model checkpoints during training. Track your configs, metrics and models from your fine-tuning runs all in one place!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;EleutherAI’s LM Evaluation Harness&lt;/a&gt;&lt;/strong&gt; - Evaluating fine-tuned models is critical to understanding whether fine-tuning is giving you the results you need. torchtune includes a simple evaluation recipe powered by EleutherAI’s LM Evaluation Harness to provide easy access to a comprehensive suite of standard LLM benchmarks. Given the importance of evaluation, we will be working with EleutherAI very closely in the next few months to build an even deeper and more “native” integration.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/executorch-overview&quot;&gt;ExecuTorch&lt;/a&gt;&lt;/strong&gt; - Models fine-tuned with torchtune can be &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&quot;&gt;easily exported&lt;/a&gt; to ExecuTorch, enabling efficient inference to be run on a wide variety of mobile and edge devices.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao&lt;/a&gt;&lt;/strong&gt; - Easily and efficiently quantize your fine-tuned models into 4-bit or 8-bit using a simple &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py&quot;&gt;post-training recipe&lt;/a&gt; powered by the quantization APIs from torchao.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;This is just the beginning and we’re really excited to put this alpha version in front of a vibrant and energetic community. In the coming weeks, we’ll continue to augment the library with more models, features and fine-tuning techniques. We’d love to hear any feedback, comments or feature requests in the form of GitHub issues on our repository, or on our &lt;a href=&quot;https://discord.com/invite/4Xsdn8Rr9Q&quot;&gt;Discord channel&lt;/a&gt;. As always, we’d love any contributions from this awesome community. Happy Tuning!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating MoE model inference with Locality-Aware Kernel Design</title>
      <link href="https://pytorch.org/blog/accelerating-moe-model/" rel="alternate" type="text/html" title="Accelerating MoE model inference with Locality-Aware Kernel Design" />
      <published>2024-04-04T00:00:00-07:00</published>
      <updated>2024-04-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-moe-model</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-moe-model/">&lt;h2 id=&quot;10-summary&quot;&gt;1.0 Summary&lt;/h2&gt;

&lt;p&gt;We show that by implementing column-major scheduling to improve data locality, we can accelerate the core Triton GEMM (General Matrix-Matrix Multiply) kernel for MoEs (Mixture of Experts) up to 4x on A100, and up to 4.4x on H100 Nvidia GPUs. This post demonstrates several different work decomposition and scheduling algorithms for MoE GEMMs and shows, at the hardware level, why column-major scheduling produces the highest speedup.&lt;/p&gt;

&lt;p&gt;Repo and code available at: &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/triton/inference/col_major_moe_gemm&quot;&gt;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/triton/inference/col_major_moe_gemm&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-7.png&quot; alt=&quot;Figure 1A. Optimized Fused MoE GEMM Kernel TFLOPs on A100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1A. Optimized Fused MoE GEMM Kernel TFLOPs on &lt;strong&gt;A100&lt;/strong&gt; for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-8.png&quot; alt=&quot;Figure 1B. Optimized Fused MoE GEMM Kernel TFLOPs on H100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto; margin-top: 40px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1B. Optimized Fused MoE GEMM Kernel TFLOPs on &lt;strong&gt;H100&lt;/strong&gt; for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;20-background&quot;&gt;2.0 Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/openai/triton&quot;&gt;OpenAI’s Triton&lt;/a&gt; is a hardware-agnostic language and compiler that as our prior &lt;a href=&quot;https://pytorch.org/blog/accelerating-triton/&quot;&gt;blog post&lt;/a&gt; has shown can be used to accelerate quantization workflows. We also showed that in terms of kernel development, much of the same learnings and performance analysis tools from CUDA can be leveraged to provide similar insights into how Triton kernels work under-the-hood and subsequent measures to speedup these kernels in latency sensitive environments. As Triton becomes increasingly adopted in production settings, it is important that developers understand the common tips and tricks to developing performant kernels as well as the generality of these methods to various different architectures and workflows. Thus, this post will explore how we optimized the Triton kernel developed by &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM &lt;/a&gt;for the popular Mixture of Experts (MoE) Mixtral model using classical techniques and how these techniques can be implemented in Triton to achieve performance gain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04088&quot;&gt;Mixtral 8x7B&lt;/a&gt; is a sparse Mixture of Experts Language Model. Unlike the classical dense transformer architecture, each transformer block houses 8 MLP layers where each MLP is an ‘expert’. As a token flows through, a router network selects which 2 of the 8 experts should process that token and the results are then combined. The selected experts for the same token vary at each layer.  As a result, while Mixtral 8x7B has a total of 47B params, during inference only 13B params are active.&lt;/p&gt;

&lt;p&gt;The MoE GEMM (General Matrix-Matrix Multiply) kernel receives a stacked weight matrix containing all the experts, and must subsequently route each token to the TopK (2 for Mixtral) experts by utilizing a mapping array produced by the resultant scores of the router network. In this post, we provide methods to efficiently parallelize this computation during inference time, specifically during autoregression (or decoding stages).&lt;/p&gt;

&lt;h2 id=&quot;30-work-decomposition---splitk&quot;&gt;3.0 Work Decomposition - SplitK&lt;/h2&gt;

&lt;p&gt;We have previously shown that for the matrix problem sizes found in LLM inference, specifically in the context of W4A16 quantized inference, GEMM kernels can be accelerated by applying a &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;SplitK work decomposition&lt;/a&gt;. Thus, we started our MoE acceleration research by implementing SplitK in the &lt;a href=&quot;https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py&quot;&gt;vLLM MoE Kernel&lt;/a&gt;, which produced speedups of approximately 18-20% over the Data Parallel approach.&lt;/p&gt;

&lt;p&gt;This result shows that the SplitK optimization can be used as a part of a more formulaic approach to improving/developing Triton kernels in inference settings. To build intuition about these different work decompositions, let’s consider a simple example for the multiplication of two 4x4 matrices and SplitK=2.&lt;/p&gt;

&lt;p&gt;In the data parallel GEMM kernel shown below, the computation for a single block of the output matrix will be handled by 1 threadblock, TB0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-1.gif&quot; alt=&quot;Figure 2. Data Parallel GEMM&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. Data Parallel GEMM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In contrast, in the SplitK kernel, the work required to compute 1 block in the output matrix, is “split” or shared amongst 2 thread blocks TB0 and TB1. This provides better load balancing and increased parallelism.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig.gif&quot; alt=&quot;Figure 3. SplitK GEMM&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3. SplitK GEMM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key idea is that we’ve increased our parallelism from M&lt;em&gt;N to M&lt;/em&gt;N*SplitK. This approach does incur some costs such as adding inter-threadblock communication via atomic operations. However, these costs are minimal compared to the savings of other constrained GPU resources like shared memory and registers. Most importantly, the SplitK strategy provides superior load balancing characteristics for skinny matrices, (as is the case in MoE inference) and is the common matrix profile during decoding and inference.&lt;/p&gt;

&lt;h2 id=&quot;40-gemm-hardware-scheduling---column-major&quot;&gt;4.0 GEMM Hardware Scheduling - Column Major&lt;/h2&gt;

&lt;p&gt;To improve upon the ~20% speedup with SplitK we focused our investigation on the logic that controls the hardware scheduling of the GEMM in Triton Kernels. Our profiling of the vLLM MoE kernel showed a low L2 cache hit rate, thus we investigated three scheduling options - column-major, row-major and grouped launch.  Due to some intrinsic properties of MoE models, such as large expert matrices, and having to dynamically load TopK (2 for Mixtral) matrices during the duration of the kernel, cache reuse/hit rate becomes a bottleneck that this optimization will target.&lt;/p&gt;

&lt;p&gt;For background, in our previous &lt;a href=&quot;https://pytorch.org/blog/accelerating-triton/&quot;&gt;blog&lt;/a&gt;, we touched on the concept of “tile swizzling”, a method to achieve greater L2 cache hit rate. This concept relates to how the software &lt;em&gt;schedules&lt;/em&gt; the GEMM onto the SMs of a GPU. In Triton, this schedule is determined by the pid_m and pid_n calculations. Our key insight is that for skinny matrix multiplications, a column-major ordering ensures optimal reuse of the columns of the weight matrix, B. To illustrate this, let’s take a look at a snippet of what a column major computation of pid_m, and pid_n would look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-6.png&quot; alt=&quot;Figure 4. Column Major ordering in PyTorch&quot; style=&quot;width:100%;display: block; max-width: 500px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4. Column Major ordering in PyTorch&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From above, we note that with this mapping, we schedule the GEMM such that we calculate the output blocks of C in the following order: C(0, 0), C(1, 0), C(2, 0),… etc. To understand the implications we provide the following illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-5.png&quot; alt=&quot;Activation matrix / Weight matrix&quot; style=&quot;width:100%;display: block; max-width: 500px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-4.png&quot; alt=&quot;L1/L2 Cache&quot; style=&quot;width:100%;display: block; max-width: 300px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-3.png&quot; alt=&quot;C - Output Matrix&quot; style=&quot;width:100%;display: block; max-width: 300px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5. Cache Reuse Pattern for a Column-Major GEMM Schedule&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the above simplified view of a column-major schedule, let’s assume for a GEMM with skinny activation matrix A, that the entire matrix can fit in the GPU cache which is a reasonable assumption to make for the type of problem sizes we encounter in MoE inference. This allows for maximal &lt;em&gt;reuse&lt;/em&gt; of the columns of the weight matrix B, due to the fact that the B column can be re-used for the corresponding output tile calculations, C(0,0), C(1, 0) and C(2, 0). Consider instead, a row-major schedule, C(0,0), C(0,1), C(0, 2) etc. We would have to evict the column of B, and issue multiple load instructions to DRAM to calculate the same amount of output blocks.&lt;/p&gt;

&lt;p&gt;An important design consideration when optimizing kernels is a memory access pattern that results in the least amount of global load instructions. This optimal memory access pattern is achieved with the column-major schedule. The results below showcase the performance of the three schedules we investigated:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-2.png&quot; alt=&quot;Figure 6. Comparison of GEMM Schedules on A100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6. Comparison of GEMM Schedules on A100 for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The column-major schedule provides up to a 4x speedup over the other patterns, and as we’ll show in the next section, provides an optimal memory access pattern due to greatly improved data locality.&lt;/p&gt;

&lt;h2 id=&quot;50-nsight-compute-analysis---throughput-and-memory-access-pattern&quot;&gt;5.0 Nsight Compute Analysis - Throughput and Memory Access Pattern&lt;/h2&gt;

&lt;p&gt;For performance analysis, we focus on the &lt;strong&gt;M = 2&lt;/strong&gt; case for the H100.  A similar study can be done for the A100 as many of the same observations carry over.  We note the following salient results, that showcase the impact of our optimizations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-1.png&quot; alt=&quot;Figure 7. H100 Memory Throughput Chart for M = 2.  Note the very large increase in the cache hit rates L1 cache hit rate (+2696%) and L2 cache hit rate (+254%).&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 7. H100 Memory Throughput Chart for M = 2.  Note the very large increase in the cache hit rates L1 cache hit rate (+2696%) and L2 cache hit rate (+254%).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig.png&quot; alt=&quot;Figure 8. H100 Memory Instruction Statistics M = 2. Note the 49% reduction in global memory loads.&quot; style=&quot;width:100%;margin-top: 40px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 8. H100 Memory Instruction Statistics M = 2. Note the 49% reduction in global memory loads.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These statistics show that our optimizations had the intended effect, which can be seen in the reduced cache misses, reduced memory accesses and the resultant 2.7x speedup. More concretely, the trace shows us a 2.54x increase in L2 hit rate (Figure 7), and a  ~50% reduction in DRAM accesses (Figure 8).&lt;/p&gt;

&lt;p&gt;These improvements ultimately yield the reduced latency, with the optimized kernel being 2.7x faster for bs=2 and 4.4x for bs=512.&lt;/p&gt;

&lt;h2 id=&quot;60-future-work&quot;&gt;6.0 Future Work&lt;/h2&gt;

&lt;p&gt;Our kernel was tested in FP16, which showcases the numerics and performance of the column major scheduling for MoE, but most production models are using BFloat16.  We encountered a limitation in Triton such that tl.atomic_add does not support Bfloat16 and hit launch latency concerns which would require cuda graph support for column major production use. In initial testing this translated to a 70% end-to-end speedup but, we encountered some expert mapping inconsistencies in an end to end environment that are not reflected in the test environment, so further work is needed to fully realize these speedups. \&lt;/p&gt;

&lt;p&gt;For future work, we intend to move this into a CUDA kernel which will ensure full BFloat16 support and reduced launch latency relative to Triton, and potentially resolve the expert routing inconsistency. We’ve also previously &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;published work&lt;/a&gt; on enabling GPTQ W4A16 with Triton GEMM kernels, so natural follow-on work would include fusing dequantization into this kernel to allow for a GPTQ quantized inference path.&lt;/p&gt;

&lt;h2 id=&quot;70-reproducibility&quot;&gt;7.0 Reproducibility&lt;/h2&gt;

&lt;p&gt;We have &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/triton/inference/col_major_moe_gemm&quot;&gt;open sourced&lt;/a&gt; the Triton kernel code along with an easy to run performance benchmark for readers interested in comparing or verifying the performance on their own GPU.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We want to thank Daniel Han, Raghu Ganti, Mudhakar Srivatsa, Bert Maher, Gregory Chanan, Eli Uriegas, and Geeta Chauhan for their review of the presented material and Woosuk from the vLLM team as we built on his implementation of the Fused MoE kernel.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Antoni Virós Martin, Chih-Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">1.0 Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Maximizing training throughput using PyTorch FSDP</title>
      <link href="https://pytorch.org/blog/maximizing-training/" rel="alternate" type="text/html" title="Maximizing training throughput using PyTorch FSDP" />
      <published>2024-03-13T00:00:00-07:00</published>
      <updated>2024-03-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/maximizing-training</id>
      <content type="html" xml:base="https://pytorch.org/blog/maximizing-training/">&lt;p&gt;In this blog, we demonstrate the scalability of FSDP with a pre-training exemplar, a 7B model trained for 2T tokens, and share various techniques we used to achieve a rapid training speed of 3,700 tokens/sec/GPU, or 40B tokens/day on 128 A100 GPUs. This translates to a model FLOPS utilization (MFU) and hardware FLOPS utilization (HFU) of 57%. Additionally, we have observed near linear scaling of FSDP to 512 GPUs, implying that training a 7B model on 512 GPUs to 2T tokens using this method would take just under two weeks.&lt;/p&gt;

&lt;p&gt;IBM researchers trained a Meta Llama 2 7B architecture to 2T tokens, which we will refer to as LlamaT(est). This model demonstrates comparable model quality as Llama 2 on various academic benchmarks. All of the &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;training code&lt;/a&gt;, along with our methodology to achieve this throughput, can be found in this blog. We also share the configuration knobs that work well for the Llama 2 models – 7B, 13B, 34B, and 70B for A100s and H100s.&lt;/p&gt;

&lt;p&gt;In this process, we also propose a _new _selective activation checkpointing mechanism that applies to FSDP which gives us a 10% boost beyond out-of-the box FSDP. We have open sourced the &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;training code base&lt;/a&gt; and an associated scalable data loader as the methodology to achieve this throughput.&lt;/p&gt;

&lt;p&gt;One key benefit of a PyTorch native pathway for training is the ability  to seamlessly train on multiple hardware backends. For example, the recent end-to-end stack for training that was released by AllenAI through OLMo also leverages PyTorch FSDP for training on AMD and NVIDIA GPUs. There are three main components that we leverage from FSDP to achieve our throughput:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;SDPA Flash attention&lt;/a&gt;, that enables fused attention kernels and efficient attention computation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;Overlap&lt;/a&gt; in computation and communication allows for better utilization of the GPU&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;Selective activation checkpointing&lt;/a&gt; enables us to tradeoff between GPU memory and compute&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;IBM has been working closely with Team PyTorch at Meta on &lt;a href=&quot;https://arxiv.org/abs/2304.11277&quot;&gt;PyTorch FSDP&lt;/a&gt; for nearly two years: introducing the &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/&quot;&gt;rate limiter&lt;/a&gt; for achieving better throughput on Ethernet interconnects, &lt;a href=&quot;https://pytorch.org/blog/performant-distributed-checkpointing/&quot;&gt;distributed checkpointing&lt;/a&gt; to improve the checkpoint times by an order of magnitude, and implementing the early version of checkpointing for the hybrid sharding mode of FSDP. Late last year, we used FSDP to train a model end-to-end.&lt;/p&gt;

&lt;h2 id=&quot;training-details&quot;&gt;Training Details&lt;/h2&gt;

&lt;p&gt;The 7B model is trained on 128 A100 GPUs with 400Gbps network connectivity and GPU direct RDMA. We use SDPA FlashAttention v2 for attention computation, and for this model we turned off activation checkpointing that limits the batch size, but provides the highest throughput – batch size is 1 million tokens per batch for 128 GPUs and improves throughput by about 10% when compared to activation checkpointing. With these parameters, we have an almost full overlap in computation and communication. We use the AdamW optimizer in 32-bit with beta1 of 0.9 and beta2 of 0.95, weight decay of 0.1, and a learning rate ending at 3e-5 with a warmup to max learning rate of 3e-4 and a cosine schedule to reduce to 3e-5 over 2T tokens. The training was performed using mixed precision bf16 on an internal dataset. The training stack is using IBM’s &lt;a href=&quot;https://github.com/foundation-model-stack/foundation-model-stack/blob/main/fms/models/llama.py&quot;&gt;Foundation Model Stack&lt;/a&gt; for model architecture and PyTorch nightlies post-2.2 release for FSDP and SDPA. We tried a few different nightlies during the time period of Nov 2023 through Feb 2024 and we observed an improvement in the throughput.&lt;/p&gt;

&lt;h3 id=&quot;selective-activation-checkpointing&quot;&gt;Selective activation checkpointing&lt;/h3&gt;

&lt;p&gt;We jointly implemented a simple and effective mechanism of selective activation checkpointing (AC). In FSDP, the common practice is to checkpoint each transformer block. A simple extension is to checkpoint every _n _blocks and reduce the amount of recomputation, while increasing the memory needed. This is quite effective for the 13B model size, increasing the throughput by 10%. For the 7B model size, we did not need activation checkpointing at all. Future versions of FSDP will provide selective activation checkpointing at an operator level, enabling an optimal compute-memory tradeoff. The code for the above is implemented &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp/blob/main/fms_fsdp/policies/ac_handler.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;throughput-and-mfu-hfu-computation&quot;&gt;Throughput and MFU, HFU computation&lt;/h3&gt;

&lt;p&gt;While we only trained the 7B model to 2T tokens, we performed numerous experiments on the other model sizes to provide the best configuration options. This is summarized in the table below for two types of infrastructure —  an A100 cluster with 128 GPUs and 400Gbps inter-node interconnect, and an H100 cluster with 96 GPUs and 800Gbps inter-node interconnect.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Model size&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Batch size&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Activation checkpoint&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Throughput tokens/sec/GPU (A100 80GB and 400Gbps interconnect)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;MFU % (A100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;HFU % (A100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Throughput tokens/sec/GPU (H100 80GB and 800Gbps interconnect)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;MFU % (H100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;HFU % (H100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
7B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
No


   &lt;/td&gt;
   &lt;td&gt;
3700


   &lt;/td&gt;
   &lt;td&gt;
0.57


   &lt;/td&gt;
   &lt;td&gt;
0.57


   &lt;/td&gt;
   &lt;td&gt;
7500


   &lt;/td&gt;
   &lt;td&gt;
0.37


   &lt;/td&gt;
   &lt;td&gt;
0.37


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
13B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Selective


   &lt;/td&gt;
   &lt;td&gt;
1800


   &lt;/td&gt;
   &lt;td&gt;
0.51


   &lt;/td&gt;
   &lt;td&gt;
0.59


   &lt;/td&gt;
   &lt;td&gt;
3800


   &lt;/td&gt;
   &lt;td&gt;
0.35


   &lt;/td&gt;
   &lt;td&gt;
0.40


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
34B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Yes


   &lt;/td&gt;
   &lt;td&gt;
700


   &lt;/td&gt;
   &lt;td&gt;
0.47


   &lt;/td&gt;
   &lt;td&gt;
0.64


   &lt;/td&gt;
   &lt;td&gt;
1550


   &lt;/td&gt;
   &lt;td&gt;
0.32


   &lt;/td&gt;
   &lt;td&gt;
0.44


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
70B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Yes


   &lt;/td&gt;
   &lt;td&gt;
370


   &lt;/td&gt;
   &lt;td&gt;
0.50


   &lt;/td&gt;
   &lt;td&gt;
0.67


   &lt;/td&gt;
   &lt;td&gt;
800


   &lt;/td&gt;
   &lt;td&gt;
0.34


   &lt;/td&gt;
   &lt;td&gt;
0.45


   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: Model and Hardware FLOPS utilization of various model sizes on A100 and H100 GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;HFU numbers are computed using the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/2240018c03744ee34ea14ad53481db934c37e384/torch/utils/flop_counter.py#L336&quot;&gt;PyTorch FLOP counter&lt;/a&gt; and the theoretical bf16 performance of A100 and H100 GPUs, whereas MFU numbers are computed using the methodology outlined in &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/2240018c03744ee34ea14ad53481db934c37e384/torch/utils/flop_counter.py#L336&quot;&gt;PaLM paper&lt;/a&gt;. We also note that the batch sizes we use for the larger models are intentionally kept at 2 per GPU to mimic choices made in training models of 4k sequence length and achieve this up to 512 GPUs without exceeding the 4M tokens popular batch size. Beyond that, we would need tensor parallelism or sequence parallelism.&lt;/p&gt;

&lt;p&gt;We note in the table above that for A100s, that activation recomputation causes the MFU to reduce, while HFU increases! With the introduction of better activation checkpointing schemes, we expect MFU to increase and catch up with HFU. However, we observe that for H100s, both MFU and HFU are relatively low. We analyze the PyTorch profile traces on H100 and observe that there is a 10% gap due to network “peeking” out. In addition, we  hypothesize that the HBM bandwidth of H100s is the cause for the reduced HFU/MFU on H100s and not being able to obtain the 3x improvement (H100s are theoretically 3x faster than A100s - &lt;a href=&quot;https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#tflops-comparison-table&quot;&gt;312 vs 989TFLOPS&lt;/a&gt;, but only have &amp;lt;2x the HBM bandwidth than A100s - &lt;a href=&quot;https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#accelerator-memory-size-and-speed&quot;&gt;2.0 vs 3.35TBps&lt;/a&gt;). We plan to try out other configuration options like Tensor Parallel to improve the knobs for the 70B model on H100s.&lt;/p&gt;

&lt;h3 id=&quot;model-details&quot;&gt;Model details&lt;/h3&gt;

&lt;p&gt;The loss curve for training is shown in the below figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/loss_curve.png&quot; alt=&quot;loss curve for training&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: LlamaT training loss curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The 2T checkpoint is converted to Hugging Face format by a script that is provided in the repository and we then use &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;lm-evaluation-harness&lt;/a&gt; to compute key academic benchmarks and compare that by running it on Llama2-7B. These results are captured in the below table.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Evaluation metric&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama2-7B (baseline)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;LlamaT-7B&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (zero shot)
   &lt;/td&gt;
   &lt;td&gt;0.41
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.43&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (5-shot weighted avg)
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.50&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Arc challenge
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.46&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.44
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Arc easy
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.74&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.71
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Boolq
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.78&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Copa
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.87&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.83
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hellaswag
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.76&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.74
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Openbookqa
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.44&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.42
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Piqa
   &lt;/td&gt;
   &lt;td&gt;0.79
   &lt;/td&gt;
   &lt;td&gt;0.79
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sciq
   &lt;/td&gt;
   &lt;td&gt;0.91
   &lt;/td&gt;
   &lt;td&gt;0.91
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Winogrande
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.69&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.67
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Truthfulqa
   &lt;/td&gt;
   &lt;td&gt;0.39
   &lt;/td&gt;
   &lt;td&gt;0.39
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GSM8k (8-shot)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.13&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.11
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: LM eval harness scores&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe that the model performs competitively with Llama2 (bolder is better).&lt;/p&gt;

&lt;h3 id=&quot;training-chronicles&quot;&gt;Training chronicles&lt;/h3&gt;

&lt;p&gt;Training was stable with no crashes, though we did observe a few hiccups:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0-200B tokens&lt;/strong&gt;: We observed a slowdown in the iteration time (time taken to execute one training step). We stopped the job to ensure that the data loader was not causing any slowdowns and the checkpointing was performant and accurate. We did not find any issues. By this time, HSDP checkpointing code was available in PyTorch, and we took this opportunity to make the switch to PyTorch checkpointing code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;200B tokens-1.9T&lt;/strong&gt;: We did not do any manual intervention in the job in late December. When we came back early January, disk space had exceeded and checkpoints were failing to be written, although the training job continued. The last known checkpoint was 1.5T.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.5T-1.7T&lt;/strong&gt;: We evaluated the 1.5T checkpoint with lm-evaluation-harness and discovered that model has been trained with an extra special token between two documents due to the Hugging Face tokenizer introducing a separator token and our dataloader also appending its own document separator. We modified the dataloader to eliminate the extra special token, and continued training with the modified dataloader from 1.7T token onwards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.7T-2T&lt;/strong&gt;: The loss initially spiked due to the change in the special tokens which was quickly recovered in a few billion tokens. The training finished without any other manual intervention!&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways-and-even-more-speed&quot;&gt;Key takeaways and even more speed&lt;/h3&gt;

&lt;p&gt;We demonstrated how one can use FSDP to train a model to 2T tokens with an excellent performance of 3700 tokens/sec/GPU and that generates a good quality model. As part of this exercise, we open sourced all our code for training and the knobs to achieve this throughput. These knobs can be leveraged by not only large-scale runs, but also smaller scale tuning runs. You can find the code &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FSDP APIs implement the &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html&quot;&gt;ZeRO&lt;/a&gt; algorithms in a PyTorch native manner and allow for tuning and training of large models. In the past, we have seen FSDP proof points (&lt;a href=&quot;https://github.com/tatsu-lab/stanford_alpaca&quot;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/blog/ram-efficient-pytorch-fsdp&quot;&gt;Hugging Face&lt;/a&gt;, &lt;a href=&quot;https://github.com/facebookresearch/llama-recipes&quot;&gt;Llama 2 recipes&lt;/a&gt;) on tuning a variety of LLMs (such as Meta Llama 2  7B to 70B Llama) using simple training loops and achieving good throughputs and training times.&lt;/p&gt;

&lt;p&gt;Finally, we note that there are several levers for speeding up training:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Node optimizations that can speedup specific operations (e.g., attention computation using Flash Attention V2)&lt;/li&gt;
  &lt;li&gt;Graph optimizations (e.g., fusing kernels, torch.compile)&lt;/li&gt;
  &lt;li&gt;Overlap in compute-communications&lt;/li&gt;
  &lt;li&gt;Activation recomputation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have leveraged 1, 3, and a variation of 4 in this blog and are working closely with Team PyTorch at Meta to get torch.compile (2) as well as a more advanced version of 4 with per-operator selective activation recomputation. We plan to share a simple formatting code and example data to ingest into our data loader to enable others to use the code base for training of models.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;There are several teams that have been involved in reaching this proof point and we would like to thank the teams across Meta and IBM. Specifically, we extend our gratitude to the PyTorch distributed team, Facebook Research and Applied AI teams that built the &lt;a href=&quot;https://arxiv.org/abs/2304.11277&quot;&gt;FSDP APIs&lt;/a&gt; and made enhancements based on our feedback. We also wish to thank the data team at IBM Research that curated the data corpus used in this exercise and the infrastructure team at IBM Research (especially, Claudia Misale, Shweta Salaria, and Seetharami Seelam) that optimized NCCL and network configurations. By building and leveraging all of these components, we have successfully demonstrated the LlamaT proof point.&lt;/p&gt;

&lt;p&gt;The selective activation checkpointing was conceptualized at IBM by Linsong Chu, Davis Wertheimer, Mudhakar Srivatsa, and Raghu Ganti and implemented by Less Wright at Meta.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/stasbekman/?originalSubdomain=ca&quot;&gt;Stas Bekman&lt;/a&gt; and &lt;a href=&quot;https://minjiazhang.github.io/&quot;&gt;Minjia Zhang&lt;/a&gt;, who provided extensive feedback and helped improve the blog. Their insights have been invaluable in highlighting key aspects of optimizing the training and exploring further enhancements.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;communication-computation-overlap&quot;&gt;Communication computation overlap&lt;/h3&gt;

&lt;p&gt;Another key aspect of training in a multi-node setting is the ability to overlap communication and computation. In FSDP, there are multiple opportunities for overlapping – during the FSDP unit gathering phase at forward pass as well as the backward pass computation. Overlapping the gather during forward pass while the computation of the previous unit and overlapping backward computation with the next unit gathering and gradient scattering help improve GPU utilization by nearly 2x. We illustrate this on the 400Gbps network interconnect with A100 80GB GPUs. In the case of HSDP, there is no inter-node traffic during the pre-fetch stage for forward pass and the overlap is only for the backward gradient computation phase. Of course, HSDP is feasible only when the model can be sharded within a single node, limiting the size of models to around 30B parameters.&lt;/p&gt;

&lt;p&gt;The below figure shows three steps in FSDP with the communication between nodes at the bottom and the compute stream at the top of the second half of the image. For the 7B model with no activation recomputation, we observe the overlap to be complete. In practice, the overlap percentage possible is 90% since the first block during forward pass and the last block during backward pass are not able to overlap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/overlap_zoomed_out.png&quot; alt=&quot;three steps in FSDP with the communication between nodes at the bottom and the compute stream at the top of the second half&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A zoomed in view of the above three-step process is shown below for a single step. We can clearly see the granularity of the computation and communication and how they overlap in an interleaved manner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/overlap_zoomed_in.png&quot; alt=&quot;zoomed in view of the above three-step process&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at IBM and Team PyTorch at Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we demonstrate the scalability of FSDP with a pre-training exemplar, a 7B model trained for 2T tokens, and share various techniques we used to achieve a rapid training speed of 3,700 tokens/sec/GPU, or 40B tokens/day on 128 A100 GPUs. This translates to a model FLOPS utilization (MFU) and hardware FLOPS utilization (HFU) of 57%. Additionally, we have observed near linear scaling of FSDP to 512 GPUs, implying that training a 7B model on 512 GPUs to 2T tokens using this method would take just under two weeks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2 paper and tutorial @ ASPLOS 2024</title>
      <link href="https://pytorch.org/blog/pytorch-2-paper-tutorial/" rel="alternate" type="text/html" title="PyTorch 2 paper and tutorial @ ASPLOS 2024" />
      <published>2024-02-06T00:00:00-08:00</published>
      <updated>2024-02-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-paper-tutorial</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-paper-tutorial/">&lt;p&gt;The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.&lt;/p&gt;

&lt;p&gt;The paper delves into the implementation of torch.compile and highlights the key technologies driving it, including TorchDynamo (graph capture), TorchInductor (backend compiler), and Dynamic Shape support.&lt;/p&gt;

&lt;p&gt;During the ASPLOS conference, we’ll be conducting a tutorial on Saturday, April 27, focusing on the inner workings of PyTorch 2 and how systems researchers can leverage and build upon it. Stay tuned for more details as the event approaches – we look forward to your participation!&lt;/p&gt;

&lt;p&gt;A preview of the paper is attached below:&lt;/p&gt;

&lt;p&gt;Title: &lt;strong&gt;PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.&lt;/strong&gt; &lt;a href=&quot;/assets/pytorch2-2.pdf&quot;&gt;&lt;strong&gt;Full Paper PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27x inference and 1.41x training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.&lt;/p&gt;

&lt;h3 id=&quot;authors&quot;&gt;Authors&lt;/h3&gt;

&lt;p&gt;Jason Ansel (Meta); Edward Yang (Meta); Horace He (Meta); Natalia Gimelshein (OpenAI); Animesh Jain (Meta); Michael Voznesensky (Meta); Bin Bao (Meta); Peter Bell (Quansight); David Berard (Meta); Evgeni Burovski Quansight; Geeta Chauhan (Meta); Anjali Chourdia (Meta); Will Constable (Meta); Alban Desmaison (Meta); Zachary DeVito (Meta); Elias Ellison (Meta); Will Feng (Meta); Jiong Gong (Intel); Michael Gschwind (Meta); Brian Hirsh (Meta); Sherlock Huang (Meta); Kshiteej Kalambarkar (Quansight); Laurent Kirsch (Meta); Michael Lazos (Meta); Mario Lezcano (Quansight); Yanbo Liang (Meta); Jason Liang (Meta); Yinghai Lu (Meta); CK Luk (Meta); Bert Maher (Meta); Yunjie Pan (University of Michigan); Christian Puhrsch (Meta); Matthias Reso (Meta); Mark Saroufim (Meta); Marcos Yukio Siraichi (Quansight); Helen Suk (Meta); Michael Suo (Meta); Phil Tillet (OpenAI); Eikan Wang (Intel); Xiaodong Wang (Meta); William Wen (Meta); Shunting Zhang (Meta); Xu Zhao (Meta); Keren Zhou (OpenAI &amp;amp; George Mason University); Richard Zou (Meta); Ajit Mathews (Meta); Gregory Chanan (Meta); Peng Wu (Meta); Soumith Chintala (Meta)&lt;/p&gt;

&lt;h3 id=&quot;asplos24---full-day-tutorial-schedule&quot;&gt;ASPLOS’24 - Full Day Tutorial Schedule&lt;/h3&gt;

&lt;p&gt;Full schedule for the ASPLOS’24 PyTorch 2 Tutoral on Saturday, April 27th is available &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/ASPLOS_2024&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">What’s New in PyTorch Documentation</title>
      <link href="https://pytorch.org/blog/new-in-docs/" rel="alternate" type="text/html" title="What's New in PyTorch Documentation" />
      <published>2024-02-01T00:00:00-08:00</published>
      <updated>2024-02-01T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/new-in-docs</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-in-docs/">&lt;p&gt;Greetings to the PyTorch community! Here is a quick update on PyTorch docs.&lt;/p&gt;

&lt;p&gt;In November 2023, we successfully conducted a &lt;a href=&quot;https://pytorch.org/blog/pytorch-docathon-h2-2023-wrap/&quot;&gt;PyTorch Docathon&lt;/a&gt;, a community event where PyTorch community members gathered together to improve PyTorch documentation and tutorials. This event saw a global participation of contributors who dedicated their time and effort to enhance our docs. We extend our sincere gratitude to everyone involved.&lt;/p&gt;

&lt;p&gt;A key accomplishment of the Docathon was the comprehensive work carried out on docstrings. Our community contributors meticulously reviewed and improved the docstrings based on the provided tasks.&lt;/p&gt;

&lt;p&gt;In addition to that, we’ve added three new tutorials that showcase real-world applications of PyTorch. We are particularly proud that two of these tutorials were contributed by PyTorch ecosystem partners.&lt;/p&gt;

&lt;p&gt;Here is the new tutorials for you to explore:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/tiatoolbox_tutorial.html&quot;&gt;Whole Slide Image Classification Using PyTorch and TIAToolbox&lt;/a&gt; —This tutorial demonstrates how to classify Whole Slide Images (WSIs) using PyTorch deep learning models with TIAToolbox, which are images of human tissue samples used by pathologists and researchers to study diseases like cancer at the microscopic level.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/usb_semisup_learn.html&quot;&gt;Semi-Supervised Learning using USB built upon PyTorch&lt;/a&gt; – This tutorial introduces USB, a flexible and modular semi-supervised learning framework based on PyTorch, demonstrating its ease of use in training a FreeMatch/SoftMatch model on CIFAR-10 using pre-trained ViT and its adaptability to various algorithms and imbalanced datasets.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/torchserve_vertexai_tutorial.html&quot;&gt;Deploying a PyTorch Stable Diffusion model as a Vertex AI Endpoint&lt;/a&gt; – This tutorial provides a step-by-step guide on how to streamline the deployment of a PyTorch Stable Diffusion model (v1.5) using Vertex AI, a fully-managed machine learning platform, by creating a custom TorchServe handler, uploading model artifacts to Google Cloud Storage, creating a Vertex AI model with the model artifacts and a prebuilt PyTorch container image, and finally deploying the model onto an endpoint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re planning more community events this year, so stay tuned!&lt;/p&gt;

&lt;p&gt;And finally, we just published new 2.2 PyTorch &lt;a href=&quot;https://pytorch.org/docs/&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;tutorials&lt;/a&gt;. Check it out!&lt;/p&gt;

&lt;p&gt;Best regards,&lt;br /&gt;
The PyTorch Team&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Greetings to the PyTorch community! Here is a quick update on PyTorch docs.</summary>
      

      
      
    </entry>
  
</feed>



<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-07-30T19:32:03-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile</title>
      <link href="https://pytorch.org/blog/torchchat-local-llm-inference/" rel="alternate" type="text/html" title="Introducing torchchat: Accelerating Local LLM Inference on Laptop, Desktop and Mobile" />
      <published>2024-07-30T00:00:00-07:00</published>
      <updated>2024-07-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchchat-local-llm-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchchat-local-llm-inference/">&lt;p&gt;Today, we’re releasing &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt;, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.&lt;/p&gt;

&lt;p&gt;In our previous blog posts, we &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;showed&lt;/a&gt; how to use native PyTorch 2 to run LLMs with great performance using CUDA. Torchchat expands on this with more target environments, models and execution modes. Additionally it provides important functions such as export, quantization and eval in a way that’s easy to understand providing an E2E story for those who want to build a local inference solution.&lt;/p&gt;

&lt;p&gt;You will find the project organized into three areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Python: Torchchat provides a &lt;a href=&quot;https://github.com/pytorch/torchchat?tab=readme-ov-file#server&quot;&gt;REST API&lt;/a&gt; that is called via a Python CLI or can be accessed via the browser&lt;/li&gt;
  &lt;li&gt;C++: Torchchat produces a desktop-friendly binary using PyTorch’s &lt;a href=&quot;https://pytorch-dev-podcast.simplecast.com/episodes/aotinductor&quot;&gt;AOTInductor&lt;/a&gt; backend&lt;/li&gt;
  &lt;li&gt;Mobile devices: Torchchat uses &lt;a href=&quot;https://pytorch.org/executorch/stable/index.html&quot;&gt;ExecuTorch&lt;/a&gt; to export a .pte binary file for on-device inference&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchchat.png&quot; alt=&quot;torchchat schema&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;The following table tracks the performance of torchchat for Llama 3 for a variety of configurations.&lt;br /&gt;
&lt;em&gt;Numbers for Llama 3.1 are coming soon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Llama 3 8B Instruct on Apple MacBook Pro M1 Max 64GB Laptop&lt;/strong&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm Compile
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;5.84
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.99
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;Arm AOTI
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;4.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;1.05
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;3.28
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;MPS Eager
   &lt;/td&gt;
   &lt;td&gt;float16
   &lt;/td&gt;
   &lt;td&gt;12.63
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;16.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;17.15
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Llama 3 8B Instruct on Linux x86 and CUDA&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Intel(R) Xeon(R) Platinum 8339HC CPU @ 1.80GHz with 180GB Ram + A100 (80GB)&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Mode&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;DType&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama 3 8B Tokens/Sec&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;x86 Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;2.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;3.15
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;5.33
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td rowspan=&quot;3&quot;&gt;CUDA Compile
   &lt;/td&gt;
   &lt;td&gt;bfloat16
   &lt;/td&gt;
   &lt;td&gt;83.23
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int8
   &lt;/td&gt;
   &lt;td&gt;118.17
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;int4
   &lt;/td&gt;
   &lt;td&gt;135.16
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Llama3 8B Instruct on Mobile&lt;/strong&gt;&lt;br /&gt;
Torchchat achieves &amp;gt; 8T/s on the Samsung Galaxy S23 and iPhone using 4-bit GPTQ via ExecuTorch.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We encourage you to &lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;clone the torchchat repo and give it a spin&lt;/a&gt;&lt;/strong&gt;, explore its capabilities, and share your feedback as we continue to empower the PyTorch community to run LLMs locally and on constrained devices. Together, let’s unlock the full potential of generative AI and LLMs on any device. Please submit &lt;a href=&quot;https://github.com/pytorch/torchat/issues&quot;&gt;issues&lt;/a&gt; as you see them, since we are still iterating quickly. We’re also inviting community contributions across a broad range of areas, from additional models, target hardware support, new quantization schemes, or performance improvements.  Happy experimenting!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Today, we’re releasing torchchat, a library showcasing how to seamlessly and performantly run Llama 3, 3.1, and other large language models across laptop, desktop, and mobile.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.4 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-4/" rel="alternate" type="text/html" title="PyTorch 2.4 Release Blog" />
      <published>2024-07-24T00:00:00-07:00</published>
      <updated>2024-07-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-4</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-4/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.4 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.4.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.4 adds support for the latest version of Python (3.12) for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. AOTInductor freezing gives developers running AOTInductor more performance-based optimizations by allowing the serialization of MKLDNN weights. As well, a new default TCPStore server backend utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libuv&lt;/code&gt; has been introduced which should significantly reduce initialization times for users running large-scale jobs. Finally, a new Python Custom Operator API makes it easier than before to integrate custom kernels into PyTorch, especially for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This release is composed of 3661 commits and 475 contributors since PyTorch 2.3. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.4. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Python 3.12 support for torch.compile
   &lt;/td&gt;
   &lt;td&gt;FSDP2: DTensor-based per-parameter-sharding FSDP
   &lt;/td&gt;
   &lt;td&gt;torch.compile optimizations for AWS Graviton (aarch64-linux) processors
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;AOTInductor Freezing for CPU
   &lt;/td&gt;
   &lt;td&gt;torch.distributed.pipelining, simplified pipeline parallelism
   &lt;/td&gt;
   &lt;td&gt;BF16 symbolic shape optimization in TorchInductor
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;New Higher-level Python Custom Operator API
   &lt;/td&gt;
   &lt;td&gt;Intel GPU is available through source build
   &lt;/td&gt;
   &lt;td&gt;Performance optimizations for GenAI projects utilizing CPU devices
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Switching TCPStore’s default server backend to libuv
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-python-312-support-for-torchcompile&quot;&gt;[Beta] Python 3.12 support for &lt;em&gt;torch.compile&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; previously only supported Python &lt;strong&gt;3.8-3.11&lt;/strong&gt;. Users can now optimize models with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; with Python &lt;strong&gt;3.12&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-aotinductor-freezing-for-cpu&quot;&gt;[Beta] AOTInductor Freezing for CPU&lt;/h3&gt;

&lt;p&gt;This feature enables users to turn on the freezing flag when using AOTInductor on CPU. With this feature, AOTInductor can cover the same set of op scenarios and reach on-par performance as Inductor CPP backend. Before this support, when models contain MKLDNN operators (when computation-intensive operators are involved, such as Convolution, Linear, ConvTranspose, and so on) and freezing is on, those models will fail to run since AOTInductor didn’t support serializing the MKLDNN weights which have an opaque format.&lt;/p&gt;

&lt;p&gt;The workflow is as explained in the AOTInductor &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt;tutorial&lt;/a&gt;, in addition to that users could now add the freezing flag to get better performance:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export TORCHINDUCTOR_FREEZING=1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;beta-new-higher-level-python-custom-operator-api&quot;&gt;[Beta] New Higher-level Python Custom Operator API&lt;/h3&gt;

&lt;p&gt;We’ve added a new higher-level Python Custom Operator API that makes it easier than before to extend PyTorch with custom operators that behave like PyTorch’s built-in operators. Operators registered using the &lt;a href=&quot;https://pytorch.org/docs/2.4/library.html#module-torch.library&quot;&gt;new high-level torch.library APIs&lt;/a&gt; are guaranteed to be compatible with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and other PyTorch subsystems; authoring a custom operator in Python using the previous &lt;a href=&quot;https://pytorch.org/docs/2.4/library.html#low-level-apis&quot;&gt;low-level torch.library APIs&lt;/a&gt; required deep understanding of PyTorch internals and has many footguns.&lt;/p&gt;

&lt;p&gt;Please see the &lt;a href=&quot;https://pytorch.org/tutorials/advanced/python_custom_ops.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-switching-tcpstores-default-server-backend-to-libuv&quot;&gt;[Beta] Switching TCPStore’s default server backend to &lt;em&gt;libuv&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Introduced a new default server backend for TCPStore built with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libuv&lt;/code&gt; which should introduce significantly lower initialization times and better scalability. This should ideally benefit users with a much shorter startup time when accounting for large-scale jobs.&lt;/p&gt;

&lt;p&gt;For more information on the motivation + fallback instructions please refer to this &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TCPStore_libuv_backend.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;prototype-fsdp2-dtensor-based-per-parameter-sharding-fsdp&quot;&gt;[PROTOTYPE] FSDP2: DTensor-based per-parameter-sharding FSDP&lt;/h3&gt;

&lt;p&gt;FSDP2 is a new fully sharded data parallelism implementation that uses dim-0 per-parameter sharding to resolve fundamental composability challenges with FSDP1’s flat-parameter sharding.&lt;/p&gt;

&lt;p&gt;For more information regarding the motivation / design for FSDP2 please refer to the &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/114299&quot;&gt;RFC on Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-torchdistributedpipelining-simplified-pipeline-parallelism&quot;&gt;[PROTOTYPE] &lt;em&gt;torch.distributed.pipelining&lt;/em&gt;, simplified pipeline parallelism&lt;/h3&gt;

&lt;p&gt;Pipeline Parallelism is one of the primitive parallelism techniques for deep learning. It allows the execution of a model to be partitioned such that multiple micro-batches can execute different parts of the model code concurrently.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.distributed.pipelining&lt;/code&gt; provides a toolkit that allows for easy implementation of pipeline parallelism on general models while also offering composability with other common PyTorch distributed features like DDP, FSDP, or tensor parallel.&lt;/p&gt;

&lt;p&gt;For more information on this please refer to our &lt;a href=&quot;https://pytorch.org/docs/main/distributed.pipelining.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/pipelining_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-intel-gpu-is-available-through-source-build&quot;&gt;[PROTOTYPE] Intel GPU is available through source build&lt;/h3&gt;

&lt;p&gt;Intel GPU in PyTorch on Linux systems offers fundamental functionalities on Intel® Data Center GPU Max Series: eager mode and torch.compile.&lt;/p&gt;

&lt;p&gt;For eager mode, the commonly used Aten operators are implemented by using SYCL programming language. The most performance-critical graphs and operators are highly optimized by using oneAPI Deep Neural Network (oneDNN). For torch.compile mode, Intel GPU backend is integrated to Inductor on top of Triton.&lt;/p&gt;

&lt;p&gt;For more information for Intel GPU source build please refer to our &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/articles/technical/pytorch-2-4-supports-gpus-accelerate-ai-workloads.html&quot;&gt;blog post&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;torchcompile-optimizations-for-aws-graviton-aarch64-linux-processors&quot;&gt;&lt;em&gt;torch.compile&lt;/em&gt; optimizations for AWS Graviton (aarch64-linux) processors&lt;/h3&gt;

&lt;p&gt;AWS optimized the PyTorch torch.compile feature for AWS Graviton3 processors. This optimization results in up to 2x better performance for Hugging Face model inference (based on geomean of performance improvement for 33 models) and up to 1.35x better performance for TorchBench model inference (geomean of performance improvement for 45 models) compared to the default eager mode inference across several natural language processing (NLP), computer vision (CV), and recommendation models on AWS Graviton3-based Amazon EC2 instances.&lt;/p&gt;

&lt;p&gt;For more information regarding specific technical details please refer to the &lt;a href=&quot;https://pytorch.org/blog/accelerated-pytorch-inference/&quot;&gt;blog post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;bf16-symbolic-shape-optimization-in-torchinductor&quot;&gt;BF16 symbolic shape optimization in TorchInductor&lt;/h3&gt;

&lt;p&gt;Pytorch users can now experience improved quality and performance gains with the beta BF16 symbolic shape support. While static shape may afford additional optimization opportunities compared to symbolic shape, it is insufficient for scenarios such as inference services with varying batch size and sequence length, or detection models with data-dependent output shape.&lt;/p&gt;

&lt;p&gt;Verification using TorchBench, Huggingface, and timms_model shows a similar pass rate and comparable speedup with the BF16 static shape scenario. Combining the benefits of symbolic shape with BF16 AMX instructions hardware acceleration provided by Intel CPUs and general Inductor CPU backend optimizations applicable to both static and symbolic shape in PyTorch 2.4, the performance for BF16 symbolic shape has significantly improved compared to PyTorch 2.3.&lt;/p&gt;

&lt;p&gt;The API to use this feature:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;…&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;”&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bfloat16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;compiled_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dynamic&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;performance-optimizations-for-genai-projects-utilizing-cpu-devices&quot;&gt;Performance optimizations for GenAI projects utilizing CPU devices&lt;/h3&gt;

&lt;p&gt;Highlighting the enhanced performance of PyTorch on CPU, as demonstrated through the optimizations made for the &lt;a href=&quot;https://github.com/pytorch-labs/segment-anything-fast&quot;&gt;“Segment Anything Fast”&lt;/a&gt; and &lt;a href=&quot;https://github.com/huggingface/diffusion-fast&quot;&gt;“Diffusion Fast”&lt;/a&gt; project. However, only CUDA devices are supported in the model. We have incorporated CPU support into the projects, enabling users to leverage the increased power of CPU for running the project’s experiments. Meanwhile, we have employed a &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/126961&quot;&gt;block-wise attention mask for SDPA&lt;/a&gt; as well, which can significantly reduce peak memory usage and improve performance. We have also optimized a series of &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/126961&quot;&gt;layout propagation rules in Inductor CPU&lt;/a&gt; to improve performance.&lt;/p&gt;

&lt;p&gt;To facilitate this, we have updated the README file. The API to use this feature is given below, simply providing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--device cpu&lt;/code&gt; in the command lines:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;For Segment Anything Fast:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;SEGMENT_ANYTHING_FAST_USE_FLASH_4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
python run_experiments.py 16 vit_b &amp;lt;pytorch_github&amp;gt; &amp;lt;segment-anything_github&amp;gt;
&amp;lt;path_to_experiments_data&amp;gt; &lt;span class=&quot;nt&quot;&gt;--run-experiments&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--num-workers&lt;/span&gt; 32 &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt; cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For Diffusion Fast:&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python run_benchmark.py &lt;span class=&quot;nt&quot;&gt;--compile_unet&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--compile_vae&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--enable_fused_projections&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cpu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Users can follow the guidelines to run the experiments and observe the performance improvements firsthand, as well as explore the performance improvement trends across FP32 and BF16 data types.&lt;/p&gt;

&lt;p&gt;Additionally, users can achieve good performance using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and SDPA. By observing the performance trends across these different factors, users can gain a deeper understanding of how various optimizations enhance PyTorch’s performance on CPU.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.4 (release note)! PyTorch 2.4 adds support for the latest version of Python (3.12) for torch.compile. AOTInductor freezing gives developers running AOTInductor more performance-based optimizations by allowing the serialization of MKLDNN weights. As well, a new default TCPStore server backend utilizing libuv has been introduced which should significantly reduce initialization times for users running large-scale jobs. Finally, a new Python Custom Operator API makes it easier than before to integrate custom kernels into PyTorch, especially for torch.compile.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Dive on the Hopper TMA Unit for FP8 GEMMs</title>
      <link href="https://pytorch.org/blog/hopper-tma-unit/" rel="alternate" type="text/html" title="Deep Dive on the Hopper TMA Unit for FP8 GEMMs" />
      <published>2024-07-22T00:00:00-07:00</published>
      <updated>2024-07-22T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hopper-tma-unit</id>
      <content type="html" xml:base="https://pytorch.org/blog/hopper-tma-unit/">&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;The Hopper (H100) GPU architecture, billed as the “first truly asynchronous GPU”, includes a new, fully asynchronous hardware copy engine for bulk data movement between global and shared memory called Tensor Memory Accelerator (TMA).  While CUTLASS has &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/56b46e2d13875b46b8f6a03f9f5ac91e2bfdc01a/include/cute/arch/copy_sm90_tma.hpp&quot;&gt;built-in&lt;/a&gt; support for TMA via its asynchronous pipeline paradigm, Triton exposes TMA support via an &lt;a href=&quot;https://github.com/triton-lang/triton/blob/538556a66ee49630e1cb0b239f93e63b968b2478/python/triton/tools/experimental_descriptor.py#L25&quot;&gt;experimental API&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we provide a deeper dive into the details of how TMA works, for developers to understand the new async copy engine.  We also show the importance of leveraging TMA for H100 kernels by building a TMA enabled FP8 GEMM kernel in Triton, which delivers from 1.4-2.2x performance gains over cuBLAS FP16 for small-to-medium problem sizes.  Finally, we showcase key implementation differences between Triton and CUTLASS that may account for reports of performance regressions with TMA in Triton.  We open source our implementation for reproducibility and review at &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels&quot;&gt;https://github.com/pytorch-labs/applied-ai/tree/main/kernels&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg1.png&quot; alt=&quot;The throughput in TFLOPs of various Triton and cuBLAS FP8 and FP16 kernels, for M=M, N=4096, K=4096. The red line is the Triton TMA, which showcases the advantages of leveraging TMA.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; The throughput in TFLOPs of various Triton and cuBLAS FP8 and FP16 kernels, for M=M, N=4096, K=4096. The red line is the Triton TMA, which showcases the advantages of leveraging TMA.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;tma-background&quot;&gt;TMA Background&lt;/h2&gt;

&lt;p&gt;TMA is an H100 hardware addition that allows applications to asynchronously and bi-directionally transfer 1D-5D tensors between GPU global and shared memory.  In addition, TMA can also transfer the same data to not just the calling SM’s shared memory, but to other SM’s shared memory if they are part of the same Thread Block Cluster.  This is termed ‘multicast’.&lt;/p&gt;

&lt;p&gt;TMA is very lightweight as only a single thread is needed to kick off a TMA transfer.  By moving data directly from GMEM (global) to SMEM (shared), this avoids earlier GPU requirements of using registers for moving data between different memory spaces.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg2.png&quot; alt=&quot;A100-style data movement vs H100 with TMA.  TMA hardware eliminates the need for a large amount of threads and registers participating in bulk data transfers.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; A100-style data movement vs H100 with TMA.  TMA hardware eliminates the need for a large amount of threads and registers participating in bulk data transfers.  (Image credit Nvidia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A single thread can issue large data movement instructions, allowing the majority of a given thread block to continue working on other instructions while data is in-flight. Combined with asynchronous pipelining, this allows memory transfers to be easily hidden and ensure the majority of any given thread block cluster can focus on computational task.&lt;/p&gt;

&lt;p&gt;This lightweight invocation for data movement enables the creation of warp-group specialized kernels, where warp-groups take on different roles, namely producers and consumers. Producers elect a leader thread that fires off TMA requests, which are then asynchronously coordinated with the consumer (MMA) warp-groups via an arrival barrier.  Consumers then process the data using warp-group MMA, and signal back to the producers when they have finished reading from the SMEM buffer and the cycle repeats.&lt;/p&gt;

&lt;p&gt;Further, within threadblock clusters, producers can lower their max register requirements since they are only issuing TMA calls, and effectively transfer additional registers to MMA consumers, which helps to alleviate register pressure for consumers.&lt;/p&gt;

&lt;p&gt;In addition, TMA handles the address computation for the shared memory destination where the data requested should be placed. This is why calling threads (producers) can be so lightweight.&lt;/p&gt;

&lt;p&gt;To ensure maximum read access speed, TMA can lay out the arriving data based on swizzling instructions, to ensure the arriving data can be read as fast as possible by consumers, as the swizzling pattern helps avoid shared memory bank conflicts.&lt;/p&gt;

&lt;p&gt;Finally for TMA instructions that are outgoing, or moving data from SMEM to GMEM, TMA can also include reduction operations (add/min/max) and bitwise (and/or) operations.&lt;/p&gt;

&lt;h2 id=&quot;tma-usage-in-triton&quot;&gt;TMA usage in Triton&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Pre-Hopper Load:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;offs_m = pid_m*block_m + tl.arange(0, block_m)
offs_n = pid_n*block_n + tl.arange(0, block_n)
offs_k = tl.arange(0, block_k)

a_ptrs = a_ptr + (offs_am[:, None]*stride_am + offs_k[None, :]*stride_ak)
b_ptrs = b_ptr + (offs_k[:, None]*stride_bk + offs_bn[None, :]*stride_bn)

a = tl.load(a_ptrs)
b = tl.load(b_ptrs)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; Traditional style bulk load from global to shared memory in Triton&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the above Triton example showing a pre-Hopper load, we see how the data for tensors a and b are loaded by each thread block computing  global offsets (a_ptrs, b_ptrs) from their relevant program_id (pid_m, pid_n, k) and then making a request to move blocks of memory into shared memory for a and b.&lt;/p&gt;

&lt;p&gt;Now let’s examine how to perform a load using TMA in Triton.&lt;/p&gt;

&lt;p&gt;The TMA instruction requires a special data structure called a tensor map, in contrast to the above where we directly pass pointers to global memory. To build the tensor map, we first create a TMA descriptor on the CPU. The descriptor handles the creation of the tensor map by using the &lt;a href=&quot;https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html#group__CUDA__TENSOR__MEMORY&quot;&gt;cuTensorMapEncode API&lt;/a&gt;. The tensor map holds metadata such as the global and shared memory layout of the tensor and serves as a compressed representation of the structure of the multi-dimensional tensor stored in global memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg3.png&quot; alt=&quot;TMA address generation via a copy descriptor&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; TMA address generation via a copy descriptor (Image credit: Nvidia)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The TMA descriptor holds the tensor’s key properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Base Pointer&lt;/li&gt;
  &lt;li&gt;Shape and Block Size&lt;/li&gt;
  &lt;li&gt;Datatype&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The TMA descriptor is created on the host before the kernel, and then moved to device by passing the descriptor to a torch tensor. Thus, in Triton, the GEMM kernel receives a global pointer to the tensor map.&lt;/p&gt;

&lt;h2 id=&quot;triton-host-code&quot;&gt;Triton Host Code&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   desc_a = np.empty(TMA_SIZE, dtype=np.int8)
   desc_b = np.empty(TMA_SIZE, dtype=np.int8)
   desc_c = np.empty(TMA_SIZE, dtype=np.int8)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(a.data_ptr(), m, k, block_m, block_k, a.element_size(), desc_a)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(b.data_ptr(), n, k, block_n, block_k, b.element_size(), desc_b)

   triton.runtime.driver.active.utils.fill_2d_tma_descriptor(c.data_ptr(), m, n, block_m, block_n, c.element_size(), desc_c)
  
   desc_a = torch.tensor(desc_a, device='cuda')
   desc_b = torch.tensor(desc_b, device='cuda')
   desc_c = torch.tensor(desc_c, device='cuda')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the code that is used to set up the descriptors in the kernel invoke function.&lt;/p&gt;

&lt;h2 id=&quot;triton-device-code&quot;&gt;Triton Device Code&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Offsets/Pointer Arithmetic:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   offs_am = pid_m * block_m
   offs_bn = pid_n * block_n
   offs_k = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Load:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [block_m, block_k], tl.float8e4nv)
  b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [block_n, block_k], tl.float8e4nv)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Store:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; tl._experimental_descriptor_store(c_desc_ptr, accumulator, [offs_am, offs_bn])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We no longer need to calculate a pointer array for both load and store functions in the kernel. Instead, we pass a single descriptor pointer, the offsets, block size and the input datatype. This simplifies address calculation and reduces register pressure, as we no longer have to do complex pointer arithmetic in software and dedicate CUDA cores for address computation.&lt;/p&gt;

&lt;h2 id=&quot;tma-performance-analysis&quot;&gt;TMA Performance Analysis&lt;/h2&gt;

&lt;p&gt;Below, we discuss the PTX instructions for different load mechanisms on Hopper.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PTX for Loading Tile (cp.async) - H100 no TMA&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add.s32 	%r27, %r100, %r8;
add.s32 	%r29, %r100, %r9;
selp.b32 	%r30, %r102, 0, %p18;


@%p1 cp.async.cg.shared.global [ %r27 + 0 ], [ %rd20 + 0 ], 0x10, %r30;
@%p1 cp.async.cg.shared.global [ %r29 + 0 ], [ %rd21 + 0 ], 0x10, %r30;


cp.async.commit_group ;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we observe the older cp.async instruction responsible for global memory copies. From the traces below we can see that both loads bypass the L1 cache. A major difference in the newer TMA load is that before tiles from A and B were ready to be consumed by the Tensor Core we would need to execute an ldmatrix instruction that operated on data contained in register files. On Hopper, the data can now be directly reused from shared memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg4.png&quot; alt=&quot;H100 Memory Chart showing GMEM Throughput = 910.22 GB/s&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; H100 Memory Chart showing GMEM Throughput = 910.22 GB/s (Triton GEMM &lt;strong&gt;without&lt;/strong&gt; TMA) for M=128, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By leveraging TMA through the Triton API changes we mentioned above, we can investigate the PTX that Triton generates for a single 2D tile load with TMA.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PTX for Loading Tile (cp.async.bulk.tensor) - H100 using TMA&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bar.sync 	0;
shr.u32 	%r5, %r4, 5;
shfl.sync.idx.b32	%r66, %r5, 0, 31, -1;

elect.sync _|%p7, 0xffffffff;


add.s32 	%r24, %r65, %r67;
shl.b32 	%r25, %r66, 7;

@%p8
cp.async.bulk.tensor.2d.shared::cluster.global.mbarrier::complete_tx::bytes [%r24], [%rd26, {%r25,%r152}], [%r19];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The cp.async.bulk.tensor.2d.shared TMA instruction is passed the destination address in shared memory, a pointer to the tensor map, the tensor map coordinates and a pointer to the mbarrier object, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg5.png&quot; alt=&quot;H100 Memory Chart GMEM Throughput =1.45 TB/s&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; H100 Memory Chart GMEM Throughput =1.45 TB/s (Triton GEMM &lt;strong&gt;with&lt;/strong&gt; TMA) for M=128, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For optimal performance we tuned the TMA GEMM kernel extensively. Amongst other parameters such as tile sizes, number of warps and number of pipeline stages, the biggest increase in memory throughput  was observed when we increased the TMA_SIZE (descriptor size) from 128 to 512. From the above NCU profiles, we can see that the final tuned kernel has increased global memory transfer throughput from 910 GB/s to 1.45 TB/s, a &lt;strong&gt;59%&lt;/strong&gt; increase in GMEM throughput, over the non-TMA Triton GEMM kernel.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Comparison of CUTLASS and Triton FP8 GEMM and TMA Implementation - Kernel Architecture&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg6.png&quot; alt=&quot;Triton vs CUTLASS Ping-Pong FP8 GEMM TFLOPs, M=M, N=4096, K=4096&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Triton vs CUTLASS Ping-Pong FP8 GEMM TFLOPs, M=M, N=4096, K=4096&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The above chart shows the performance of a CUTLASS &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/637b15906358191cb4238af419d408a65819d7ec/include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp&quot;&gt;Ping-Pong GEMM kernel&lt;/a&gt; against Triton. The Ping-Pong kernel leverages TMA differently than Triton. It makes use of all of its HW and SW software capabilities, while Triton currently does not. Specifically, CUTLASS supports the below TMA features that help explain the performance gaps in pure GEMM performance:.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;TMA Multicast&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables copy of data from GMEM to multiple SMs&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Warp Specialization&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables warp groups within a threadblock to take on different roles&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tensor Map (TMA Descriptor) Prefetch&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Enables prefetching the Tensor Map object from GMEM, which allows pipelining of TMA loads&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To put the performance numbers in perspective, below we show a ‘speed-up’ chart highlighting the latency differences on a percentage basis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg7.png&quot; alt=&quot;% Speedup of CUTLASS Ping-Pong vs Triton FP8 with TMA.&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; % Speedup of CUTLASS Ping-Pong vs Triton FP8 with TMA.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This speedup is purely kernel throughput, not including E2E launch overhead which we will discuss below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TMA Descriptor movement - a key difference between Triton and CUTLASS with E2E performance implications&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As noted previously, creation of a 2D+ dimensional TMA descriptor takes place on the host and is then transferred to the device.  However, this transfer process takes place very differently depending on the implementation.&lt;/p&gt;

&lt;p&gt;Here we showcase the differences between how Triton transfers TMA descriptors compared with CUTLASS.&lt;/p&gt;

&lt;p&gt;Recall, TMA transfers require a special data structure, a tensor map to be created on CPU through the cuTensorMap API, which for an FP8 GEMM Kernel means creating three descriptors, one for each A, B and C. We see below that for both the Triton and CUTLASS Kernels the same CPU procedures are invoked.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg8.png&quot; alt=&quot;Calls to cuTensorMapEncodeTiled (Both Triton and CUTLASS use this path)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Calls to cuTensorMapEncodeTiled (Both Triton and CUTLASS use this path)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However, for Triton, each descriptor is transferred in its own distinct copy kernel, which adds a significant amount of overhead and serves as a barrier to use this kernel in an end-to-end use inference scenario.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg9.png&quot; alt=&quot;Three H2D Copy Kernels are launched before the kernel execution, for A, B and C&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Three H2D Copy Kernels are launched before the kernel execution, for A, B and C&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These copies are not observed in the CUTLASS implementation, due to the way that TMA descriptors are passed to the kernel. We can see from the PTX below that with Cutlass, tensor maps are passed-by-value to the kernel.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.entry _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_6half_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEENS7_ILi128EEES9_EEENS6_IJNS7_ILi2EEENS7_ILi1EEESC_EEENS_4gemm32KernelTmaWarpSpecializedPingpongENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE(

.param .align 64 .b8 _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_6half_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEENS7_ILi128EEES9_EEENS6_IJNS7_ILi2EEENS7_ILi1EEESC_EEENS_4gemm32KernelTmaWarpSpecializedPingpongENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE_param_0[1024]


mov.b64 	%rd110, _ZN7cutlass13device_kernelIN49_GLOBAL__N__8bf0e19b_16_scaled_mm_c3x_cu_2bec3df915cutlass_3x_gemmIaNS_10bfloat16_tENS1_14ScaledEpilogueEN4cute5tupleIJNS5_1CILi64EEES8_NS7_ILi256EEEEEENS6_IJNS7_ILi1EEESB_SB_EEENS_4gemm24KernelTmaWarpSpecializedENS_8epilogue18TmaWarpSpecializedEE10GemmKernelEEEvNT_6ParamsE_param_0;

add.s64 	%rd70, %rd110, 704;
cvta.param.u64 	%rd69, %rd70;

cp.async.bulk.tensor.2d.global.shared::cta.bulk_group [%rd69, {%r284, %r283}], [%r1880];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; CUTLASS kernel PTX showing pass-by-value&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By directly passing the TMA Descriptor as opposed to passing a global memory pointer, the CUTLASS kernel avoids the three extra H2D copy kernels and instead these copies are included in the single device kernel launch for the GEMM.&lt;/p&gt;

&lt;p&gt;Because of the difference in how descriptors are moved to the device, the kernel latencies including the time to prepare the tensors to be consumed by the TMA is drastically different.  For M=1-128, N=4096, K=4096 the CUTLASS pingpong kernel has an average latency of 10us Triton TMA kernels complete in an average of 4ms.  This is a factor of ~3330x slower and appears to be directly linked to the 3 independent kernel launches for TMA descriptor transfer by Triton.&lt;/p&gt;

&lt;p&gt;Cuda graphs may be one way to reduce this, but given the overhead created by the H2D copies the current Triton implementation when measured end to end is not competitive.  A rework of how the Triton compiler manages TMA descriptors would likely resolve this gap.  We thus focused on comparing the actual compute kernel throughput and not E2E in our data above.&lt;/p&gt;

&lt;h2 id=&quot;results-summary&quot;&gt;Results Summary&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hopper-tma-unit/fg10.png&quot; alt=&quot;Triton FP8 TMA GEMM TFLOPs Comparison&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; Triton FP8 TMA GEMM TFLOPs Comparison&lt;/em&gt;&lt;/p&gt;

&lt;table class=&quot;mt-5 table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;M
   &lt;/td&gt;
   &lt;td&gt;Triton TMA
   &lt;/td&gt;
   &lt;td&gt;Triton Tutorial
   &lt;/td&gt;
   &lt;td&gt;Triton SplitK 
   &lt;/td&gt;
   &lt;td&gt;cuBLAS FP8 
   &lt;/td&gt;
   &lt;td&gt;cuBLAS FP16 
   &lt;/td&gt;
   &lt;td&gt;CUTLASS Ping-Pong FP8
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2.5
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;2.4
   &lt;/td&gt;
   &lt;td&gt;1.5
   &lt;/td&gt;
   &lt;td&gt;1.8
   &lt;/td&gt;
   &lt;td&gt;3.57
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;5.1
   &lt;/td&gt;
   &lt;td&gt;2.5
   &lt;/td&gt;
   &lt;td&gt;4.8
   &lt;/td&gt;
   &lt;td&gt;3.1
   &lt;/td&gt;
   &lt;td&gt;3.6
   &lt;/td&gt;
   &lt;td&gt;5.9
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;4
   &lt;/td&gt;
   &lt;td&gt;10.3
   &lt;/td&gt;
   &lt;td&gt;7.21
   &lt;/td&gt;
   &lt;td&gt;9.6
   &lt;/td&gt;
   &lt;td&gt;6.1
   &lt;/td&gt;
   &lt;td&gt;7.2
   &lt;/td&gt;
   &lt;td&gt;14.3
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;8
   &lt;/td&gt;
   &lt;td&gt;21.0
   &lt;/td&gt;
   &lt;td&gt;16.5
   &lt;/td&gt;
   &lt;td&gt;19.2
   &lt;/td&gt;
   &lt;td&gt;12.3
   &lt;/td&gt;
   &lt;td&gt;14.4
   &lt;/td&gt;
   &lt;td&gt;28.6
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;16
   &lt;/td&gt;
   &lt;td&gt;44.5
   &lt;/td&gt;
   &lt;td&gt;41.0
   &lt;/td&gt;
   &lt;td&gt;37.2
   &lt;/td&gt;
   &lt;td&gt;24.5
   &lt;/td&gt;
   &lt;td&gt;27.7
   &lt;/td&gt;
   &lt;td&gt;55.1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;89.7
   &lt;/td&gt;
   &lt;td&gt;81.2
   &lt;/td&gt;
   &lt;td&gt;72.2
   &lt;/td&gt;
   &lt;td&gt;71.6
   &lt;/td&gt;
   &lt;td&gt;56.8
   &lt;/td&gt;
   &lt;td&gt;114.4
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;64
   &lt;/td&gt;
   &lt;td&gt;178.5
   &lt;/td&gt;
   &lt;td&gt;163.7
   &lt;/td&gt;
   &lt;td&gt;130.8
   &lt;/td&gt;
   &lt;td&gt;144.6
   &lt;/td&gt;
   &lt;td&gt;105.3
   &lt;/td&gt;
   &lt;td&gt;228.7
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;128
   &lt;/td&gt;
   &lt;td&gt;359.7
   &lt;/td&gt;
   &lt;td&gt;225.9
   &lt;/td&gt;
   &lt;td&gt;160.1
   &lt;/td&gt;
   &lt;td&gt;244.0
   &lt;/td&gt;
   &lt;td&gt;189.2
   &lt;/td&gt;
   &lt;td&gt;377.7
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; Triton FP8 TMA GEMM TFLOPs Comparison Table&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The above chart and table summarize the gain we’ve been able to achieve on a single NVIDIA H100 for FP8 GEMM, by leveraging the TMA Hardware Unit, over non-TMA Triton kernels and high performance CUDA (cuBLAS) kernels. The key point to note is this kernel’s superior scaling (with the batch size) properties over the competition. The problem sizes we benchmarked on are representative of the matrix shapes found in small-to-medium batch size LLM inference. Thus, TMA GEMM kernel performance in the mid-M regime (M=32 to M=128) will be critical for those interested in leveraging this kernel for FP8 LLM deployment use cases, as the FP8 compressed data type can allow larger matrices to fit in GPUs memory.&lt;/p&gt;

&lt;p&gt;To summarize our analysis, the TMA implementation in Triton and CUTLASS differ in terms of full featureset support (multicast, prefetch etc.) and how the TMA Descriptor is passed to the GPU kernel. If this descriptor is passed in a manner that more closely matches the CUTLASS kernel (pass-by-value), the extraneous H2D copies could be avoided and thus the E2E performance would be greatly improved.&lt;/p&gt;

&lt;h2 id=&quot;future-work&quot;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;For future research, we plan to improve upon these results, by working with the community to incorporate the CUTLASS architecture of TMA loads into Triton as well as investigating the Cooperative Kernel for FP8 GEMM, a modified strategy to the Ping-Pong Kernel.&lt;/p&gt;

&lt;p&gt;In addition, once features like thread block clusters and TMA atomic operations are enabled in Triton, we may be able to get further speedups by leveraging the SplitK strategy in the TMA GEMM Kernel, as atomic operations on Hopper can be performed in Distributed Shared Memory (DSMEM) as opposed to L2 Cache.  We also note the similarities of NVIDIA Hopper GPUs with other AI hardware accelerators like Google’s &lt;a href=&quot;https://people.csail.mit.edu/suvinay/pubs/2023.tpu.isca.pdf&quot;&gt;TPU&lt;/a&gt; and IBM’s &lt;a href=&quot;https://ieeexplore.ieee.org/document/9499865&quot;&gt;AIU&lt;/a&gt; which are dataflow architectures. On Hopper, data can now “flow” from GMEM to a network of connected SMs due to the additions of TMA, which we discussed extensively in this blog, and DSMEM, which we plan to cover in a future post.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Chih-Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">Abstract</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</title>
      <link href="https://pytorch.org/blog/flashattention-3/" rel="alternate" type="text/html" title="FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision" />
      <published>2024-07-11T00:00:00-07:00</published>
      <updated>2024-07-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/flashattention-3</id>
      <content type="html" xml:base="https://pytorch.org/blog/flashattention-3/">&lt;p&gt;Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;libraries&lt;/a&gt; to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (&lt;a href=&quot;https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k&quot;&gt;Llama 3&lt;/a&gt;). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.&lt;/p&gt;

&lt;p&gt;We’re excited to release FlashAttention-3 that incorporates these techniques. It’s 1.5-2.0x faster than FlashAttention-2 with FP16, up to 740 TFLOPS, i.e., 75% utilization of H100 theoretical max FLOPS. With FP8, FlashAttention-3 reaches close to 1.2 PFLOPS, with 2.6x smaller error than baseline FP8 attention.&lt;/p&gt;

&lt;p&gt;FlashAttention-3 is available at: &lt;a href=&quot;https://github.com/Dao-AILab/flash-attention&quot;&gt;https://github.com/Dao-AILab/flash-attention&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;https://tridao.me/publications/flash3/flash3.pdf&quot;&gt;Paper&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;flashattention-recap&quot;&gt;FlashAttention Recap&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; is an algorithm that reorders the attention computation and leverages tiling and recomputation to significantly speed it up and reduce memory usage from quadratic to linear in sequence length. We use tiling to load blocks of inputs from HBM (GPU memory) to SRAM (fast cache), perform attention with respect to that block, and update the output in HBM. By not writing the large intermediate attention matrices to HBM, we reduce the amount of memory reads/writes, which brings 2-4x wallclock time speedup.&lt;/p&gt;

&lt;p&gt;Here we show a diagram of FlashAttention forward pass: with tiling and softmax rescaling, we operate by blocks and avoid having to read/write from HBM, while obtaining the correct output with no approximation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg1.png&quot; alt=&quot;math equations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;new-hardware-features-on-hopper-gpus---wgmma-tma-fp8&quot;&gt;New hardware features on Hopper GPUs - WGMMA, TMA, FP8&lt;/h2&gt;

&lt;p&gt;While FlashAttention-2 can achieve up to 70% theoretical max FLOPS on Ampere (A100) GPUs, it does not yet take advantage of new features on Hopper GPUs to maximize performance. We describe some of the new Hopper-specific features here, and why they are important.&lt;/p&gt;

&lt;p&gt;1. WGMMA (Warpgroup Matrix Multiply-Accumulate). This new feature makes use of the new Tensor Cores on Hopper, with much higher throughput&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; than the older mma.sync instruction in Ampere (image from the &lt;a href=&quot;https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper?ncid=no-ncid&quot;&gt;H100 white paper)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg2.png&quot; alt=&quot;image from the H100 white paper&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2. TMA (Tensor Memory Accelerator). This is a special hardware unit that accelerates the transfer of data between global memory and shared memory, taking care of all index calculation and out-of-bound predication. This frees up registers, which is a valuable resource to increase tile size and efficiency.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg3.png&quot; alt=&quot;block diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3. Low-precision with FP8. This doubles the Tensor Core throughput (e.g. 989 TFLOPS with FP16 and 1978 TFLOPS with FP8), but trades off accuracy by using fewer bits to represent floating point numbers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg4.png&quot; alt=&quot;6x throughput&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FlashAttention-3 makes use of all of these new features of Hopper, using powerful abstractions from &lt;a href=&quot;https://github.com/NVIDIA/cutlass&quot;&gt;NVIDIA’s CUTLASS&lt;/a&gt; library. &lt;br /&gt;
 &lt;br /&gt;
By rewriting FlashAttention to use these new features, we can already significantly speed it up (e.g., from 350 TFLOPS in FlashAttention-2 FP16 forward pass to around 540-570 TFLOPS). However, the asynchronous nature of the new instructions on Hopper (WGMMA and TMA) opens up additional algorithmic opportunities to overlap operations and thereby extract even greater performance. For this blogpost, we’ll explain two such techniques specific to attention. The generic technique of warp specialization, with separate producer and consumer warps doing TMA and WGMMA, is &lt;a href=&quot;https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md#warp-specialization&quot;&gt;well-covered elsewhere&lt;/a&gt; in the context of GEMM and works the same here.&lt;/p&gt;

&lt;h2 id=&quot;asynchrony-overlapping-gemm-and-softmax&quot;&gt;Asynchrony: Overlapping GEMM and Softmax&lt;/h2&gt;

&lt;p&gt;Why overlap?&lt;/p&gt;

&lt;p&gt;Attention has GEMMs (those matmuls between Q and K and between attention probability P and V) and softmax as its two main operations. Why do we need to overlap them? Isn’t most of the FLOPS in the GEMMs anyway? As long as the GEMMs are fast (e.g., computed using WGMMA instructions), shouldn’t the &lt;a href=&quot;https://horace.io/brrr_intro.html&quot;&gt;GPU be going brrrr&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;The problem is that non-matmul operations are much slower than matmul operations on modern accelerators. Special functions such as exponential (for the softmax) have even lower throughput than floating point multiply-add; they are evaluated by the multi-function unit, a unit separate from floating point multiply-add or matrix multiply-add. As an example, the H100 GPU SXM5 has 989 TFLOPS of FP16 matrix multiply, but only 3.9 TFLOPS (256x less throughput) for special functions&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;! For head dimension 128, there are 512x more matmul FLOPS than exponential, which means that exponential can take 50% of the time compared to matmul. The situation is even worse for FP8, where the matmul FLOPS are twice as fast yet exponential FLOPS stay the same speed. Ideally we want matmul and softmax to operate in parallel. While the Tensor Cores are busy with matmul, the multi-function units should be calculating exponential!&lt;/p&gt;

&lt;h3 id=&quot;inter-warpgroup-overlapping-with-pingpong-scheduling&quot;&gt;Inter-warpgroup overlapping with pingpong scheduling&lt;/h3&gt;

&lt;p&gt;The first and easiest way to overlap GEMM and softmax is to do nothing at all! The warp schedulers already try to schedule warps so that if some warps are blocked (e.g., waiting for GEMM results), other warps can run. That is, the warp schedulers do some of this overlapping for us, for free.&lt;/p&gt;

&lt;p&gt;However, we can improve on this by doing some of the scheduling manually. As an example, if we have 2 warpgroups (labeled 1 and 2 – each warpgroup is a group of 4 warps), we can use synchronization barriers (bar.sync) so that warpgroup 1 first does its GEMMs (e.g., GEMM1 of one iteration and GEMM0 of the next iteration), and then warpgroup 2 does its GEMMs while warpgroup 1 does its softmax, and so on. This “pingpong” schedule is illustrated in the figure below, where the same color denotes the same iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg5.png&quot; alt=&quot;block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This would allow us to perform the softmax in the shadow of the GEMMs of the other warpgroup. Of course, this figure is just a caricature; in practice the scheduling is not really this clean. Nevertheless, pingpong scheduling can improve FP16 attention forward pass from around 570 TFLOPS to 620 TFLOPS (head dim 128, seqlen 8K).&lt;/p&gt;

&lt;h3 id=&quot;intra-warpgroup-overlapping-of-gemm-and-softmax&quot;&gt;Intra-warpgroup overlapping of GEMM and Softmax&lt;/h3&gt;

&lt;p&gt;Even within one warpgroup, we can have some part of softmax running while the GEMMs of that warpgroup is running. This is illustrated in this figure, where the same color denotes the same iteration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6.png&quot; alt=&quot;block chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This pipelining increases throughput from around 620 TFLOPS to around 640-660 TFLOPS for FP16 attention forward, at the cost of higher register pressure. We need more registers to hold both accumulators of the GEMMs, and the input/output of softmax. Overall, we find this technique to offer a favorable tradeoff.&lt;/p&gt;

&lt;h2 id=&quot;low-precision-reduce-quantization-error-with-incoherent-processing&quot;&gt;Low-precision: reduce quantization error with incoherent processing&lt;/h2&gt;

&lt;p&gt;LLM activation can have &lt;a href=&quot;https://arxiv.org/abs/2208.07339&quot;&gt;outliers&lt;/a&gt; with much larger magnitude than the rest of the features. These outliers make it difficult to quantize, producing much larger quantization errors. We leverage incoherent processing, a technique used in the quantization literature (e.g. from &lt;a href=&quot;https://arxiv.org/abs/2307.13304&quot;&gt;QuIP&lt;/a&gt;) that multiplies the query and key with a random orthogonal matrix to “spread out” the outliers and reduce quantization error. In particular, we use the Hadamard transform (with random signs), which can be done per attention head in O(d log d) instead of O(d^2) time, where d is the head dimension. Since the Hadamard transform is memory-bandwidth bound, it can be fused with previous operations such as rotary embedding (also memory-bandwidth bound) “for free”.&lt;/p&gt;

&lt;p&gt;In our experiment where Q, K, V are generated from a standard normal distribution but 0.1% of the entries have large magnitudes (to simulate outliers), we found that incoherent processing can reduce the quantization error by 2.6x. We show numerical error comparison in the table below. Please see the paper for details.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg6a.png&quot; alt=&quot;text diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;attention-benchmark&quot;&gt;Attention benchmark&lt;/h2&gt;

&lt;p&gt;We show some results with FlashAttention-3, and compare it to FlashAttention-2, as well as the implementation in Triton and cuDNN (both of which already use new hardware features of Hopper GPUs).&lt;/p&gt;

&lt;p&gt;For FP16, we see about 1.6x-1.8x speedup over FlashAttention-2&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg7.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg8.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For FP8, we can reach close to 1.2 PFLOPS!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/flashattention-3/fg9.png&quot; alt=&quot;speed charts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;This blogpost highlights some of the optimizations for FlashAttention available on Hopper GPUs. Other optimizations (e.g., variable length sequences, persistent kernel, and in-kernel transpose for FP8) are covered in the paper.&lt;/p&gt;

&lt;p&gt;We have seen that designing algorithms that take advantage of the hardware they run on can bring significant efficiency gains and unlock new model capabilities such as long context. We look forward to future work on optimization for LLM inference, as well as generalizing our techniques to other hardware architectures.&lt;/p&gt;

&lt;p&gt;We also look forward to FlashAttention-3 being integrated in a future release of PyTorch.&lt;/p&gt;

&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;Without the wgmma instruction, the older mma.sync instruction can only reach about ⅔ the peak throughput of Hopper Tensor Cores: https://arxiv.org/abs/2402.13499v1 &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;The CUDA programming guide specifies that the throughput for special functions is 16 operations per streaming multiprocessor (SM) per clock cycle. We multiply 16 by 132 SMs and 1830 Mhz (clock speed used to calculate 989 TFLOPS of FP16 matmul) to get 3.9 TFLOPS &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Jay Shah and Ganesh Bikshandi, Colfax Research, Ying Zhang, Meta, Vijay Thakkar and Pradeep Ramani, NVIDIA, Tri Dao, TogetherAI and Princeton University</name>
        
        
      </author>

      

      

      
        <summary type="html">Attention, as a core layer of the ubiquitous Transformer architecture, is a bottleneck for large language models and long-context applications. FlashAttention (and FlashAttention-2) pioneered an approach to speed up attention on GPUs by minimizing memory reads/writes, and is now used by most libraries to accelerate Transformer training and inference. This has contributed to a massive increase in LLM context length in the last two years, from 2-4K (GPT-3, OPT) to 128K (GPT-4), or even 1M (Llama 3). However, despite its success, FlashAttention has yet to take advantage of new capabilities in modern hardware, with FlashAttention-2 achieving only 35% utilization of theoretical max FLOPs on the H100 GPU. In this blogpost, we describe three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) incoherent processing that leverages hardware support for FP8 low-precision.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Learn how to develop Android applications with ExecuTorch and Llama models</title>
      <link href="https://pytorch.org/blog/develop-android-applications/" rel="alternate" type="text/html" title="Learn how to develop Android applications with ExecuTorch and Llama models" />
      <published>2024-07-10T00:00:00-07:00</published>
      <updated>2024-07-10T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/develop-android-applications</id>
      <content type="html" xml:base="https://pytorch.org/blog/develop-android-applications/">&lt;p&gt;&lt;em&gt;This blog is courtesy of the PyTorch team at Arm. More details can be found &lt;a href=&quot;https://learn.arm.com/learning-paths/smartphones-and-mobile/build-llama3-chat-android-app-using-executorch-and-xnnpack/?utm_source=twitter&amp;amp;utm_medium=social-organic&amp;amp;utm_content=landingpage&amp;amp;utm_campaign=mk24_developer_na&quot;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Arm’s compute platform is delivering GenAI applications on phones, laptops, and servers. Cost, privacy, performance, security, and energy efficiency are just some of the reasons developers are investigating on-device AI.&lt;/p&gt;

&lt;p&gt;A new Learning Path explaining how to leverage the capabilities of large language models (LLMs) on Android using ExecuTorch and XNNPACK is now available.&lt;/p&gt;

&lt;p&gt;Here’s a summary of what you’ll learn:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Development Environment setup&lt;/p&gt;

    &lt;p&gt;The Learning Path begins by guiding you through setting up your development environment, ensuring you have all the necessary tools installed, including Android Studio, the Android NDK, Java JDK, and Python.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ExecuTorch and XNNPACK&lt;/p&gt;

    &lt;p&gt;You’ll learn about the core technologies: ExecuTorch, a framework for deploying PyTorch models to edge devices, and XNNPACK, a high-performance library for executing neural networks on Arm-based platforms.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Llama models&lt;/p&gt;

    &lt;p&gt;The Learning Path explores Llama, a family of powerful LLMs, focusing specifically on the 8B Llama 3 model. You’ll learn about quantization techniques, which are essential for optimizing model size and performance on mobile devices.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prepare Llama models for ExecuTorch&lt;/p&gt;

    &lt;p&gt;You’ll be guided through the process of downloading, exporting, and evaluating Llama models, ensuring they are ready for deployment using ExecuTorch.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Check model performance on Android&lt;/p&gt;

    &lt;p&gt;The Learning Path walks you through cross-compiling the Llama runner binary for Android, allowing you to test your model’s performance on your phone.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Build and run an Android Chat App&lt;/p&gt;

    &lt;p&gt;Finally, you’ll learn how to build a native Android chat app using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LlamaDemo&lt;/code&gt; application from the ExecuTorch repository. This hands-on experience allows you to put your knowledge into practice and create a real-world application.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Explore this Learning Path if you want to learn how to leverage the power of LLMs on your Android phone, and gain expertise in tools for on-device machine learning.&lt;/p&gt;

&lt;p&gt;Dig into the excitement of building Android chat apps and understand more about how they work on the &lt;a href=&quot;https://learn.arm.com/learning-paths/smartphones-and-mobile/build-llama3-chat-android-app-using-executorch-and-xnnpack/?utm_source=twitter&amp;amp;utm_medium=social-organic&amp;amp;utm_content=landingpage&amp;amp;utm_campaign=mk24_developer_na&quot;&gt;Arm Developer Hub&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Arm</name>
        
        
      </author>

      

      

      
        <summary type="html">This blog is courtesy of the PyTorch team at Arm. More details can be found here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerated PyTorch inference with torch.compile on AWS Graviton processors</title>
      <link href="https://pytorch.org/blog/accelerated-pytorch-inference/" rel="alternate" type="text/html" title="Accelerated PyTorch inference with torch.compile on AWS Graviton processors" />
      <published>2024-07-09T00:00:00-07:00</published>
      <updated>2024-07-09T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerated-pytorch-inference</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerated-pytorch-inference/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Originally PyTorch, used an eager mode where each PyTorch operation that forms the model is run independently as soon as it’s reached. PyTorch 2.0 introduced &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;torch.compile&lt;/a&gt; to speed up PyTorch code over the default eager mode. In contrast to eager mode, the torch.compile pre-compiles the entire model into a single graph in a manner that’s optimal for running on a given hardware platform. AWS optimized the PyTorch torch.compile feature for &lt;a href=&quot;https://aws.amazon.com/about-aws/whats-new/2022/05/amazon-ec2-c7g-instances-powered-aws-graviton3-processors/&quot;&gt;AWS Graviton3 processors&lt;/a&gt;. This optimization results in up to 2x better performance for &lt;a href=&quot;https://huggingface.co/models&quot;&gt;Hugging Face&lt;/a&gt; model inference (based on geomean of performance improvement for 33 models) and up to 1.35x better performance for &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; model inference (geomean of performance improvement for 45 models) compared to the default eager mode inference across several natural language processing (NLP), computer vision (CV), and recommendation models on AWS Graviton3-based Amazon EC2 instances. Starting with PyTorch 2.3.1, the optimizations are available in torch Python &lt;a href=&quot;https://pypi.org/project/torch/2.3.1/&quot;&gt;wheels&lt;/a&gt; and AWS Graviton PyTorch &lt;a href=&quot;https://github.com/aws/deep-learning-containers/blob/master/available_images.md#ec2-framework-graviton-containers-ec2-ecs-and-eks-support-only&quot;&gt;deep learning container (DLC)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this blog post, we show how we optimized torch.compile performance on AWS Graviton3-based EC2 instances, how to use the optimizations to improve inference performance, and the resulting speedups.&lt;/p&gt;

&lt;h2 id=&quot;why-torchcompile-and-whats-the-goal&quot;&gt;Why torch.compile and what’s the goal?&lt;/h2&gt;

&lt;p&gt;In eager mode, operators in a model are run immediately as they are encountered. It’s easier to use, more suitable for machine learning (ML) researchers, and hence is the default mode. However, eager mode incurs runtime overhead because of redundant kernel launch and memory read overhead. Whereas in torch compile mode, operators are first synthesized into a graph, wherein one operator is merged with another to reduce and localize memory reads and total kernel launch overhead.&lt;/p&gt;

&lt;p&gt;The goal for the AWS Graviton team was to optimize torch.compile backend for Graviton3 processors. PyTorch eager mode was already optimized for Graviton3 processors with &lt;a href=&quot;https://github.com/ARM-software/ComputeLibrary&quot;&gt;Arm Compute Library (ACL)&lt;/a&gt; kernels using oneDNN (also known as MKLDNN). So, the question was, how to reuse those kernels in torch.compile mode to get the best of graph compilation and the optimized kernel performance together?&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The AWS Graviton team extended the torch inductor and oneDNN primitives that reused the ACL kernels and optimized compile mode performance on Graviton3 processors. Starting with PyTorch 2.3.1, the optimizations are available in the torch Python wheels and AWS Graviton DLC. Please see the &lt;strong&gt;Running an inference&lt;/strong&gt; section that follows for the instructions on installation, runtime configuration, and how to run the tests.&lt;/p&gt;

&lt;p&gt;To demonstrate the performance improvements, we used NLP, CV, and recommendation models from &lt;a href=&quot;https://github.com/pytorch/benchmark&quot;&gt;TorchBench&lt;/a&gt; and the most downloaded NLP models from &lt;a href=&quot;https://huggingface.co/models&quot;&gt;Hugging Face&lt;/a&gt; across Question Answering, Text Classification, Token Classification, Translation, Zero-Shot Classification, Translation, Summarization, Feature Extraction, Text Generation, Text2Text Generation, Fill-Mask, and Sentence Similarity tasks to cover a wide variety of customer use cases.&lt;/p&gt;

&lt;p&gt;We started with measuring TorchBench model inference latency, in milliseconds (msec), for the eager mode, which is marked 1.0 with a red dotted line in the following graph. Then we compared the improvements from torch.compile for the same model inference, the normalized results are plotted in the graph. You can see that for the 45 models we benchmarked, there is a 1.35x latency improvement (geomean for the 45 models).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg1.png&quot; alt=&quot;PyTorch model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using TorchBench framework&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 1&lt;/strong&gt;: PyTorch model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using TorchBench framework. The reference eager mode performance is marked as 1.0. (higher is better)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Similar to the preceding TorchBench inference performance graph, we started with measuring the Hugging Face NLP model inference latency, in msec, for the eager mode, which is marked 1.0 with a red dotted line in the following graph. Then we compared the improvements from torch.compile for the same model inference, the normalized results are plotted in the graph. You can see that for the 33 models we benchmarked, there is around 2x performance improvement (geomean for the 33 models).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg2.png&quot; alt=&quot;Hugging Face NLP model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using Hugging Face example scripts&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 2&lt;/strong&gt;: Hugging Face NLP model inference performance improvement with torch.compile on AWS Graviton3-based c7g instance using Hugging Face example scripts. The reference eager mode performance is marked as 1.0. (higher is better)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-an-inference&quot;&gt;Running an inference&lt;/h2&gt;

&lt;p&gt;Starting with PyTorch 2.3.1, the optimizations are available in the torch Python wheel and in AWS Graviton PyTorch DLC. This section shows how to run inference in eager and torch.compile modes using torch Python wheels and benchmarking scripts from Hugging Face and TorchBench repos.&lt;/p&gt;

&lt;p&gt;To successfully run the scripts and reproduce the speedup numbers mentioned in this post, you need an instance from the Graviton3 family (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c7g/r7g/m7g/hpc7g&lt;/code&gt;) of hardware. For this post, we used the &lt;a href=&quot;https://aws.amazon.com/ec2/instance-types/c7g/&quot;&gt;c7g.4xl (16 vcpu) instance&lt;/a&gt;. The instance, the AMI details, and the required torch library versions are mentioned in the following snippet.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Instance: c7g.4xl instance
Region: us-west-2
AMI: ami-05cc25bfa725a144a﻿ (Ubuntu 22.04/Jammy with 6.5.0-1017-aws kernel)

# Install Python
sudo apt-get update
sudo apt-get install -y python3 python3-pip

# Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install PyTorch and extensions
python3 -m pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The generic runtime tunings implemented for eager mode inference are equally applicable for the torch.compile mode, so, we set the following environment variables to further improve the torch.compile performance on AWS Graviton3 processors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Enable the fast math GEMM kernels, to accelerate fp32 inference with bfloat16 gemm
export DNNL_DEFAULT_FPMATH_MODE=BF16

# Enable Linux Transparent Huge Page (THP) allocations,
# to reduce the tensor memory allocation latency
export THP_MEM_ALLOC_ENABLE=1

# Set LRU Cache capacity to cache the primitives and avoid redundant
# memory allocations
export LRU_CACHE_CAPACITY=1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;torchbench-benchmarking-scripts&quot;&gt;TORCHBENCH BENCHMARKING SCRIPTS&lt;/h4&gt;

&lt;p&gt;TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance. We benchmarked 45 models using the scripts from the TorchBench repo. Following code shows how to run the scripts for the eager mode and the compile mode with inductor backend.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Set OMP_NUM_THREADS to number of vcpus, 16 for c7g.4xl instance
export OMP_NUM_THREADS=16

# Install the dependencies
sudo apt-get install -y libgl1-mesa-glx
sudo apt-get install -y libpangocairo-1.0-0
python3 -m pip install psutil numpy transformers pynvml numba onnx onnxruntime scikit-learn timm effdet gym doctr opencv-python h5py==3.10.0 python-doctr 

# Clone pytorch benchmark repo
git clone https://github.com/pytorch/benchmark.git
cd benchmark
# PyTorch benchmark repo doesn't have any release tags. So,
# listing the commit we used for collecting the performance numbers
git checkout 9a5e4137299741e1b6fb7aa7f5a6a853e5dd2295

# Setup the models
python3 install.py 

# Colect eager mode performance using the following command. The results will be
# stored at .userbenchmark/cpu/metric-&amp;lt;timestamp&amp;gt;.json.
python3 run_benchmark.py cpu --model BERT_pytorch,hf_Bert,hf_Bert_large,hf_GPT2,hf_Albert,hf_Bart,hf_BigBird,hf_DistilBert,hf_GPT2_large,dlrm,hf_T5,mnasnet1_0,mobilenet_v2,mobilenet_v3_large,squeezenet1_1,timm_efficientnet,shufflenet_v2_x1_0,timm_regnet,resnet50,soft_actor_critic,phlippe_densenet,resnet152,resnet18,resnext50_32x4d,densenet121,phlippe_resnet,doctr_det_predictor,timm_vovnet,alexnet,doctr_reco_predictor,vgg16,dcgan,yolov3,pytorch_stargan,hf_Longformer,timm_nfnet,timm_vision_transformer,timm_vision_transformer_large,nvidia_deeprecommender,demucs,tts_angular,hf_Reformer,pytorch_CycleGAN_and_pix2pix,functorch_dp_cifar10,pytorch_unet --test eval --metrics=&quot;latencies,cpu_peak_mem&quot;

# Collect torch.compile mode performance with inductor backend
# and weights pre-packing enabled. The results will be stored at
# .userbenchmark/cpu/metric-&amp;lt;timestamp&amp;gt;.json
python3 run_benchmark.py cpu --model BERT_pytorch,hf_Bert,hf_Bert_large,hf_GPT2,hf_Albert,hf_Bart,hf_BigBird,hf_DistilBert,hf_GPT2_large,dlrm,hf_T5,mnasnet1_0,mobilenet_v2,mobilenet_v3_large,squeezenet1_1,timm_efficientnet,shufflenet_v2_x1_0,timm_regnet,resnet50,soft_actor_critic,phlippe_densenet,resnet152,resnet18,resnext50_32x4d,densenet121,phlippe_resnet,doctr_det_predictor,timm_vovnet,alexnet,doctr_reco_predictor,vgg16,dcgan,yolov3,pytorch_stargan,hf_Longformer,timm_nfnet,timm_vision_transformer,timm_vision_transformer_large,nvidia_deeprecommender,demucs,tts_angular,hf_Reformer,pytorch_CycleGAN_and_pix2pix,functorch_dp_cifar10,pytorch_unet --test eval --torchdynamo inductor --freeze_prepack_weights --metrics=&quot;latencies,cpu_peak_mem&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On successful completion of the inference runs, the script stores the results in JSON format. The following is the sample output:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{
 &quot;name&quot;: &quot;cpu&quot;
 &quot;environ&quot;: {
     &quot;pytorch_git_version&quot;: &quot;d44533f9d073df13895333e70b66f81c513c1889&quot;
  },
  
  &quot;metrics&quot;: {
       &quot;BERT_pytorch-eval_latency&quot;: 56.3769865,
       &quot;BERT_pytorch-eval_cmem&quot;: 0.4169921875
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;hugging-face-benchmarking-scripts&quot;&gt;HUGGING FACE BENCHMARKING SCRIPTS&lt;/h4&gt;

&lt;p&gt;Google T5 Small Text Translation model is one of the around 30 Hugging Face models we benchmarked. We’re using it as a sample model to demonstrate how to run inference in eager and compile modes. The additional configurations and APIs required to run it in compile mode are highlighted in &lt;strong&gt;BOLD&lt;/strong&gt;. Save the following script as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;google_t5_small_text_translation.py&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import argparse
from transformers import T5Tokenizer, T5Model
import torch
from torch.profiler import profile, record_function, ProfilerActivity
&lt;b&gt;import torch._inductor.config as config
config.cpp.weight_prepack=True
config.freezing=True&lt;/b&gt;

def test_inference(mode, num_iter):
    tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
    model = T5Model.from_pretrained(&quot;t5-small&quot;)

    input_ids = tokenizer(
        &quot;Studies have been shown that owning a dog is good for you&quot;, return_tensors=&quot;pt&quot;
    ).input_ids  # Batch size 1
    decoder_input_ids = tokenizer(&quot;Studies show that&quot;, return_tensors=&quot;pt&quot;).input_ids  # Batch size 1

    &lt;b&gt;if (mode == 'compile'):
        model = torch.compile(model)&lt;/b&gt;

    with torch.no_grad():
        for _ in range(50):
            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

        with profile(activities=[ProfilerActivity.CPU]) as prof:
            with record_function(&quot;model_inference&quot;):
                for _ in range(num_iter):
                    outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

    print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;))

def main() -&amp;gt; None:
    global m, args
    parser = argparse.ArgumentParser(__doc__)
    parser.add_argument(
        &quot;-m&quot;,
        &quot;--mode&quot;,
        choices=[&quot;eager&quot;, &quot;compile&quot;],
        default=&quot;eager&quot;,
        help=&quot;Which test to run.&quot;,
    )
    parser.add_argument(
        &quot;-n&quot;,
        &quot;--number&quot;,
        type=int,
        default=100,
        help=&quot;how many iterations to run.&quot;,
    )
    args = parser.parse_args()
    test_inference(args.mode, args.number)

if __name__ == &quot;__main__&quot;:
    main()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Run the script with the following steps:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Set OMP_NUM_THREADS to number of vcpus to 4 because
# the scripts are running inference in sequence, and
# they don't need large number of vcpus
export OMP_NUM_THREADS=4

# Install the dependencies
python3 -m pip install transformers

# Run the inference script in Eager mode
# using number of iterations as 1 just to show the torch profiler output
# but for the benchmarking, we used 1000 iterations.
python3 google_t5_small_text_translation.py -n 1 -m eager

# Run the inference script in torch compile mode
python3 google_t5_small_text_translation.py -n 1 -m compile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On successful completion of the inference runs, the script prints the torch profiler output with the latency breakdown for the torch operators. The following is the sample output from torch profiler:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Torch profiler output for the eager mode run on c7g.xl (4vcpu)
------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                aten::mm        40.71%      12.502ms        40.71%      12.502ms     130.229us            96  
         model_inference        26.44%       8.118ms       100.00%      30.708ms      30.708ms             1  
               aten::bmm         6.85%       2.102ms         9.47%       2.908ms      80.778us            36  
            aten::matmul         3.73%       1.146ms        57.26%      17.583ms     133.205us           132  
            aten::select         1.88%     576.000us         1.90%     583.000us       0.998us           584  
         aten::transpose         1.51%     464.000us         1.83%     563.000us       3.027us           186  
------------------------ ------------ ------------ ------------ ------------ ------------ -------------------
Self CPU time total: 30.708ms

# Torch profiler output for the compile mode run for the same model on the same instance
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
        mkldnn::_linear_pointwise        37.98%       5.461ms        45.91%       6.602ms      68.771us            96  
            Torch-Compiled Region        29.56%       4.251ms        98.53%      14.168ms      14.168ms             1  
                        aten::bmm        14.90%       2.143ms        21.73%       3.124ms      86.778us            36  
                     aten::select         4.51%     648.000us         4.62%     665.000us       1.155us           576  
                       aten::view         3.29%     473.000us         3.29%     473.000us       1.642us           288  
                      aten::empty         2.53%     364.000us         2.53%     364.000us       3.165us           115  
--------------------------------- ------------ ------------ ------------ ------------ ------------ --------------------
Self CPU time total: 14.379ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;technical-deep-dive-what-are-the-challenges-and-optimization-details&quot;&gt;Technical deep dive: What are the challenges and optimization details&lt;/h2&gt;

&lt;p&gt;Underpinning torch.compile are new technologies – TorchDynamo, AOTDispatcher, and TorchInductor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TorchDynamo&lt;/strong&gt; captures PyTorch programs safely using Python Frame Evaluation Hooks&lt;br /&gt;
&lt;strong&gt;AOTDispatcher&lt;/strong&gt; overloads PyTorch’s autograd engine as a tracing autodiff for generating ahead-of-time backward traces.&lt;br /&gt;
&lt;strong&gt;TorchInductor&lt;/strong&gt; is a deep learning compiler that generates fast code for multiple accelerators and backends.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerated-pytorch-inference/fg3.png&quot; alt=&quot;The PyTorch compilation process source&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 3&lt;/strong&gt;: The PyTorch compilation process&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;When torch.compile is invoked, torch dynamo rewrites Python bytecode to extract sequences of PyTorch operations into an &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;FX&lt;/a&gt; &lt;a href=&quot;https://pytorch.org/docs/stable/fx.html&quot;&gt;Graph&lt;/a&gt;, which is then compiled with inductor backend. For a typical inference scenario where the graph is frozen and gradient calculations are disabled, the inductor invokes platform specific optimizations like graph rewrite into more performant operators, operator fusion, and weights pre-packing.&lt;/p&gt;

&lt;p&gt;However, on Graviton3, the inductor wasn’t able to perform any of those optimizations because there was no aarch64 backend defined. To fix this, we extended the inductor’s FX passes to pick oneDNN operators for linear layer compilation on Graviton3 processors with ACL backend. The code snippet for this follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;packed_weight_op = (
    mkldnn._reorder_linear_weight
    if (is_bf16_weight or mkldnn._is_mkldnn_acl_supported())
                    
packed_linear_inputs: Tuple[Any, ...] = (input, packed_weight_node)
if is_bf16_weight or mkldnn._is_mkldnn_acl_supported():
    packed_linear_inputs += (bias, &quot;none&quot;, [], &quot;&quot;)
    packed_linear_op = mkldnn._linear_pointwise.default
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After this was done, the FX pass was successful in compiling the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;matmul &lt;/code&gt;operators to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linear_pointwise &lt;/code&gt;. The following snippet highlights the matmul operator in the original model:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; %attention_scores   : [num_users=1] = call_function[target=torch.matmul](args = (%query_layer, %transpose), kwargs = {})
 %attention_scores_1 : [num_users=1] = call_function[target=operator.truediv](args = (%attention_scores, 8.0), kwargs = {})
 %attention_scores_2 : [num_users=1] = call_function[target=operator.add](args = (%attention_scores_1, %extended_attention_mask_3), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following snippet highlights the linear_pointwise operator in the compiled graph:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%_linear_pointwise_default_140 : [num_users=2] = call_function[target=torch.ops.mkldnn._linear_pointwise.default](args = (%add_7, %_frozen_param278, %_frozen_param16, none, [], ), kwargs = {})
%mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.5), kwargs = {})
%mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.7071067811865476), kwargs = {})
%erf   : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_6,), kwargs = {})
%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This completes the torch inductor changes required to compile the graph into optimized operators on AWS Graviton3 processors. Next comes the actual inference where the compiled graph is dispatched to be run. OneDNN with ACL was the backend we chose during the inductor compilation, so, the new operators were dispatched to oneDNN as expected, for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn._linear_pointwise&lt;/code&gt;. However, due to gaps in oneDNN ACL primitives, the operators were run with C++ reference kernels instead of the optimized ACL kernels. Hence, the compile performance was still significantly behind the eager mode performance.&lt;/p&gt;

&lt;p&gt;There were mainly three areas where oneDNN ACL primitives lack support for torch.compile mode. The following section talks about them in detail.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. ACL primitives didn’t have support for weights in blocked layout&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ACL primitives originally designed for eager mode supported weights only in the standard channels last (&lt;a href=&quot;https://oneapi-src.github.io/oneDNN/dev_guide_understanding_memory_formats.html#nhwc&quot;&gt;NHWC&lt;/a&gt;) format, without any pre-packing. Whereas weights pre-packing into blocked layout is one of the main optimizations in the inductor compilation passes where the weights are reordered into blocks specific to the runtime platform. This avoids the redundant and on-the-fly reorders when running the General Matrix Multiplication (GEMM), which otherwise would be the bottleneck for inference performance. But the ACL primitives didn’t have support for blocked layout and hence the operators were run with oneDNN C++ reference kernels instead.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Mixed precision primitives weren’t supported in oneDNN&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AWS Graviton3 processors support &lt;a href=&quot;https://developer.arm.com/documentation/ddi0596/2020-12/SVE-Instructions/BFMMLA--BFloat16-floating-point-matrix-multiply-accumulate-&quot;&gt;bfloat16 MMLA instructions&lt;/a&gt; which can be used to accelerate fp32 inference with bfloat16 GEMM as a mixed precision compute. ACL supports bfloat16 mixed precision GEMM kernels, and are integrated into oneDNN as a fast math compute option for the existing fp32 operators. However, the fast math approach didn’t work for compile mode because of weights pre-packing optimization. The compile mode requires explicit mixed precision primitive implementation in oneDNN in order to use bfloat16 acceleration.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. ACL primitives didn’t support fused kernels for some of the activation functions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In eager mode, operators are dispatched individually because the model is run independently as soon as it’s reached. Whereas in compile mode, operator fusion is another important optimization where the operators are fused for runtime efficiency. For example, Gaussian Error Linear Unit (&lt;a href=&quot;https://arxiv.org/pdf/1606.08415.pdf#%3A~%3Atext%3DWe%20propose%20the%20Gaussian%20Error%2Cstandard%20Gaussian%20cumulative%20distribution%20function&quot;&gt;GELU&lt;/a&gt;) is one of the most widely used activation functions in transformers-based neural network architectures. So, it’s typical to have a linear layer (with matrix multiplications) followed by GELU activation. As part of compiling the model into efficient operators, the torch inductor fuses matmul and GELU into a single linearpointwise+gelu operator. However, oneDNN ACL primitives didn’t have the support for fused kernels with GELU.&lt;/p&gt;

&lt;p&gt;We addressed these gaps by extending oneDNN primitives to handle the additional layouts and new primitive definitions. The following sections talk about the optimizations in detail.&lt;/p&gt;

&lt;h3 id=&quot;optimization-1-extended-acl-primitives-to-accept-weight-tensors-in-blocked-layout&quot;&gt;Optimization 1: Extended ACL primitives to accept weight tensors in blocked layout&lt;/h3&gt;

&lt;p&gt;We extended the ACL primitives to accept blocked layout in addition to the the standard NHWC format. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const bool is_weights_md_format_ok
                    = utils::one_of(weights_format_kind_received,
                      format_kind::any, format_kind::blocked);


const memory_desc_t weights_md_received = weights_md_;
acl_utils::reorder_to_weight_format(aip.wei_tensor_info,
             weights_md_, expected_weight_format, inner_dim, o_dim,
             remaining_dims, {});

ACL_CHECK_SUPPORT(
     (weights_format_kind_received == format_kind::blocked)
      &amp;amp;&amp;amp; !(dnnl_memory_desc_equal(
      &amp;amp;weights_md_received, &amp;amp;weights_md_)),
      &quot;specified blocked format not supported by ACL, use &quot;
      &quot;format_kind_t::any to find a supported blocked format for &quot;
      &quot;your platform&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;optimization-2-defined-new-acl-primitives-to-handle-mixed-precision-operators-weights-in-bfloat16-and-activations-in-fp32&quot;&gt;Optimization 2: Defined new ACL primitives to handle mixed precision operators (weights in bfloat16 and activations in fp32)&lt;/h3&gt;

&lt;p&gt;We defined mixed precision primitive definitions and updated the existing oneDNN ACL fp32 primitives to handle bfloat16 tensors.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; /* With graph compilation, we are able to reorder and pre-pack the weights during the model load
  * and compilation phase itself so that redundant and on-the-fly reorders can be avoided.
  * This primitive definition is to support gemm fastmath mode for the compile scenario where src is
  * in fp32 and weights are in bf16
  */
 {{forward, f32, bf16, f32}, {
    CPU_INSTANCE_AARCH64_ACL(acl_inner_product_fwd_t)
    nullptr,
 }},
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;optimization-3-disabled-operator-fusion-pass-in-torch-inductor&quot;&gt;Optimization 3: Disabled operator fusion pass in torch inductor&lt;/h3&gt;

&lt;p&gt;We bypassed the operator fusion pass in torch inductor so that the compiled graph doesn’t contain GELU fused operators. This is a temporary solution to enable ACL kernels in torch.compile. There is a work in progress to enable operator fusion pass for the future PyTorch releases. With this workaround, we were able to successfully dispatch the linear layer to ACL. As shown in the following torch.profiler output, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten::addmm &lt;/code&gt;(one of the variants of the matmul operator) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aten::gelu &lt;/code&gt;in the original model (as highlighted in &lt;em&gt;Image 4&lt;/em&gt;) was compiled to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mkldnn::_linear_pointwise &lt;/code&gt;without &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gelu &lt;/code&gt;operator fusion (as highlighted in &lt;em&gt;Image 5&lt;/em&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                aten::addmm        73.32%      46.543ms        74.49%      47.287ms     647.767us            73  
            model_inference         9.92%       6.296ms       100.00%      63.479ms      63.479ms             1  
                  aten::bmm         4.37%       2.776ms         5.46%       3.467ms     144.458us            24  
                aten::copy_         1.74%       1.102ms         1.74%       1.102ms       8.103us           136  
                 aten::gelu         1.50%     950.000us         1.50%     950.000us      79.167us            12  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 4&lt;/strong&gt;: torch.profiler output for Hugging Face bert base model inference in Eager mode, showing addmm and gelu operators&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;mt-3 mb-3&quot;&gt;&amp;nbsp;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                 Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
-----------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                            mkldnn::_linear_pointwise        53.61%      15.529ms        57.53%      16.665ms     228.288us            73  
                                Torch-Compiled Region        36.95%      10.705ms        99.31%      28.769ms      28.769ms             1  
    aten::_scaled_dot_product_flash_attention_for_cpu         3.67%       1.064ms         4.43%       1.284ms     107.000us            12  
                                           aten::view         1.97%     572.000us         1.97%     572.000us       2.509us           228  
                                          aten::empty         1.38%     399.000us         1.38%     399.000us       3.270us           122 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 5&lt;/strong&gt;: torch.profiler output for Hugging Face Bert base model inference in torch.compile mode, showing linear_pointwise operator without gelu fusion&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lastly, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gelu &lt;/code&gt;operator was compiled into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;erf &lt;/code&gt;(error function) and was dispatched to an inductor auto vectorization backend. The following snippets show the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;erf &lt;/code&gt;operator in the compiled graph and running it using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;libm.so&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%_linear_pointwise_default_140 : [num_users=2] = call_function[target=torch.ops.mkldnn._linear_pointwise.default](args = (%add_7, %_frozen_param278, %_frozen_param16, none, [], ), kwargs = {})
%mul_5 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.5), kwargs = {})
%mul_6 : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%_linear_pointwise_default_140, 0.7071067811865476), kwargs = {})
%erf   : [num_users=1] = call_function[target=torch.ops.aten.erf.default](args = (%mul_6,), kwargs = {})
%add_8 : [num_users=1] = call_function[target=torch.ops.aten.add.Tensor](args = (%erf, 1), kwargs = {})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 6&lt;/strong&gt;: snippet after post grad pass showing erf function in the compiled graph&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;mt-3 mb-3&quot;&gt;&amp;nbsp;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;     0.82%     0.40%  python3  libm.so.6            [.] erff32
     0.05%     0.00%  python3  libtorch_python.so   [.] torch::autograd::THPVariable_erf
     0.05%     0.00%  python3  libtorch_cpu.so      [.] at::_ops::erf::call
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 7&lt;/strong&gt;: Linux perf report showing erf dispatch to libm.so&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With this work, we were able to optimize &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile &lt;/code&gt;performance on Graviton3 processors by using inductor graph compilation along with the oneDNN+ACL backend.&lt;/p&gt;

&lt;h3 id=&quot;torchbench-enhancements&quot;&gt;TorchBench enhancements&lt;/h3&gt;

&lt;p&gt;To demonstrate the torch.compile performance improvements on AWS Graviton3 processors, we extended TorchBench framework to add a new argument to enable graph freeze and weights pre-packing and disable torch auto grad for eval test mode. The code snippet for this is as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;parser.add_argument(
 &quot;—freeze_prepack_weights&quot;,
 action='store_true',
 help=&quot;set to freeze the graph and prepack weights&quot;,
 )

if args.freeze_prepack_weights:
 torch._inductor.config.freezing=True
 torch._inductor.config.cpp.weight_prepack=True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Image 8&lt;/strong&gt;: Added freeze_prepack_weights option for torchdynamo backend in TorchBench to demonstrate torch.compile performance improvements on AWS Graviton3 processors&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We have upstreamed all the optimizations, and starting with PyTorch 2.3.1, these are supported in torch Python wheels and AWS Graviton PyTorch DLC.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next&lt;/h2&gt;

&lt;p&gt;Next, we’re extending the torch inductor CPU backend support to compile Llama model, and adding support for fused GEMM kernels to enable torch inductor operator fusion optimization on AWS Graviton3 processors.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this tutorial, we covered how we optimized torch.compile performance on AWS Graviton3-based EC2 instances, how to use the optimizations to improve PyTorch model inference performance, and demonstrated the resulting speedups. We hope that you will give it a try! If you need any support with ML software on Graviton, please open an issue on the AWS Graviton Technical Guide &lt;a href=&quot;https://github.com/aws/aws-graviton-getting-started&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We would like to thank the PyTorch community for the baseline torch.compile framework and their continued efforts to optimize it further.&lt;/p&gt;

&lt;p&gt;References:  &lt;a href=&quot;https://pytorch.org/assets/pytorch2-2.pdf&quot;&gt;https://pytorch.org/assets/pytorch2-2.pdf&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;author&quot;&gt;Author&lt;/h2&gt;

&lt;p&gt;Sunita Nadampalli is a Software Development Manager and AI/ML expert at AWS. She leads AWS Graviton software performance optimizations for AI/ML and HPC workloads. She is passionate about open source software development and delivering high-performance and sustainable software solutions for SoCs based on the Arm ISA.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sunita Nadampalli</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing Hacker Cup AI Track at NeurIPS 2024</title>
      <link href="https://pytorch.org/blog/hacker-cup/" rel="alternate" type="text/html" title="Announcing Hacker Cup AI Track at NeurIPS 2024" />
      <published>2024-07-03T00:00:00-07:00</published>
      <updated>2024-07-03T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hacker-cup</id>
      <content type="html" xml:base="https://pytorch.org/blog/hacker-cup/">&lt;p&gt;The PyTorch team in partnership with Meta Hacker Cup, and Microsoft Research, are excited to announce the Hacker Cup AI Track at NeurIPS 2024. This will be the first AI track for the popular Meta Hacker Cup programming competition designed to assess the capabilities of Generative AI in performing autonomous code generation tasks. We aim to test the limits of AI in complex coding challenges and measure the performance gap between AI systems and human programmers. We will provide access to all Hacker Cup problems since 2011 alongside their respective solutions in a multimodal (image and text) format, and utilize the existing Hacker Cup infrastructure for competitor evaluation. Featuring both &lt;em&gt;open evaluation, open model&lt;/em&gt; and &lt;em&gt;open evaluation, closed model&lt;/em&gt; tracks, this competition invites diverse participation from research institutions of varied interests and resource constraints, including academic labs, AI startups, large technology companies, and AI enthusiasts. Our goal is to develop and democratize meaningful advancements in code automation with the very first open evaluation process for competitive AI programmers. Registration will begin in &lt;strong&gt;Early August&lt;/strong&gt;, with our first qualification round on &lt;strong&gt;September 20th.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more information please visit our website at &lt;a href=&quot;https://www.facebook.com/codingcompetitions/hacker-cup/&quot;&gt;https://www.facebook.com/codingcompetitions/hacker-cup/&lt;/a&gt; &lt;strong&gt;and join our Discord&lt;/strong&gt; at &lt;a href=&quot;https://discord.com/invite/wWeN9hTH32&quot;&gt;discord.gg/wWeN9hTH32&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch team in partnership with Meta Hacker Cup, and Microsoft Research, are excited to announce the Hacker Cup AI Track at NeurIPS 2024. This will be the first AI track for the popular Meta Hacker Cup programming competition designed to assess the capabilities of Generative AI in performing autonomous code generation tasks. We aim to test the limits of AI in complex coding challenges and measure the performance gap between AI systems and human programmers. We will provide access to all Hacker Cup problems since 2011 alongside their respective solutions in a multimodal (image and text) format, and utilize the existing Hacker Cup infrastructure for competitor evaluation. Featuring both open evaluation, open model and open evaluation, closed model tracks, this competition invites diverse participation from research institutions of varied interests and resource constraints, including academic labs, AI startups, large technology companies, and AI enthusiasts. Our goal is to develop and democratize meaningful advancements in code automation with the very first open evaluation process for competitive AI programmers. Registration will begin in Early August, with our first qualification round on September 20th.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Powering the AI Revolution: The PyTorch Documentary</title>
      <link href="https://pytorch.org/blog/pytorch-documentary/" rel="alternate" type="text/html" title="Powering the AI Revolution: The PyTorch Documentary" />
      <published>2024-06-25T00:00:00-07:00</published>
      <updated>2024-06-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-documentary</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-documentary/">&lt;p&gt;Now live: The official &lt;a href=&quot;https://documentary.pytorch.org/&quot;&gt;PyTorch Documentary&lt;/a&gt;! This film unveils the authentic narrative of PyTorch’s inception, attributing its existence to a dedicated group of unsung heroes driving technological innovation.&lt;/p&gt;

&lt;p&gt;The documentary shares the strength of the PyTorch community, resonating with our communities across the globe. We hope this story of PyTorch inspires greater contributions, attracts more contributors to the project, and fosters widespread recognition of PyTorch’s significance in the open source community.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/EjgTv6aSeqk?si=GKGvwuY7VA9iPGKm&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; referrerpolicy=&quot;strict-origin-when-cross-origin&quot; allowfullscreen=&quot;&quot; class=&quot;mb-5 mt-3&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;We couldn’t have produced this without the support of our PyTorch Foundation members and sponsors:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/doc-logos.jpg&quot; alt=&quot;company logos&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;amd&quot;&gt;AMD&lt;/h3&gt;

&lt;p&gt;“PyTorch’s growth and adoption in the AI community is a testament to open collaboration. The collective efforts of all the contributors have helped propel PyTorch as one of the most widely adopted AI frameworks in the industry. AMD is proud to be a part of this movement - making sure that the future of AI is open - and we are excited to continue contributing to this vibrant ecosystem.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Niles Burbank, AMD&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;aws&quot;&gt;AWS&lt;/h3&gt;

&lt;p&gt;“The release of the PyTorch Documentary showcases the innovation and real-world impact of one of the most widely adopted open source machine learning frameworks. By supporting and contributing to the PyTorch community, AWS helps enable cutting-edge machine learning research that drives advancements in AI capabilities. We are excited about the documentary as it highlights the power of collaboration in propelling PyTorch to the forefront of machine learning and empowering developers and data scientists to create groundbreaking models. At AWS, we celebrate frameworks like PyTorch that foster environments where open source machine learning technologies can grow and benefit the community at-large, as well as our customers.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Brian Granger, AWS&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;google-cloud&quot;&gt;Google Cloud&lt;/h3&gt;

&lt;p&gt;“Google recognizes the impact of PyTorch on the AI community, providing researchers and developers with powerful, flexible tools for innovation. This documentary not only celebrates the remarkable achievements of the PyTorch community but also highlights the collaborative spirit driving advancements in AI. We look forward to continuing our support for PyTorch and fostering an open ecosystem that accelerates machine learning research and application.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Dwarak Rajagopal, Google&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;meta&quot;&gt;Meta&lt;/h3&gt;

&lt;p&gt;“We have been so impressed with the growth and collaboration that PyTorch has created over the years. From very humble beginnings at Meta to a cornerstone in AI research and development, the documentary showcases the dedication of our contributors since the start. It’s an honor to be a part of something so impactful, and now it’s been documented for our community to take part in.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Soumith Chintala, Meta&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;microsoft-azure&quot;&gt;Microsoft Azure&lt;/h3&gt;

&lt;p&gt;“We’re truly excited about the premiere of the PyTorch Documentary. At Microsoft, PyTorch has been our default deep learning framework for building AI solutions including Microsoft Copilot. Additionally, we have made significant investments to create an optimized environment for our customers to develop, train, fine-tune and deploy their PyTorch workloads on Azure and Windows, furthering our commitment to democratize AI.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Eric Boyd, Microsoft&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;pytorch-foundation&quot;&gt;PyTorch Foundation&lt;/h3&gt;

&lt;p&gt;“The release of the PyTorch documentary marks a significant milestone for our community, showcasing the incredible journey and rapid evolution of PyTorch. We are excited to share these stories and achievements with the world, and we look forward to continuing to foster innovation and growth of the PyTorch community and PyTorch’s evolving ecosystem.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;– Matt White, PyTorch Foundation&lt;/strong&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>The PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">Now live: The official PyTorch Documentary! This film unveils the authentic narrative of PyTorch’s inception, attributing its existence to a dedicated group of unsung heroes driving technological innovation.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Training MoEs at Scale with PyTorch</title>
      <link href="https://pytorch.org/blog/training-moes/" rel="alternate" type="text/html" title="Training MoEs at Scale with PyTorch" />
      <published>2024-06-23T00:00:00-07:00</published>
      <updated>2024-06-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/training-moes</id>
      <content type="html" xml:base="https://pytorch.org/blog/training-moes/">&lt;p&gt;Over the past year, Mixture of Experts (MoE) models have surged in popularity, fueled by powerful open-source models like &lt;a href=&quot;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&quot;&gt;DBRX&lt;/a&gt;, &lt;a href=&quot;https://mistral.ai/news/mixtral-of-experts/&quot;&gt;Mixtral&lt;/a&gt;, &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V2&quot;&gt;DeepSeek&lt;/a&gt;, and many more. At Databricks, we’ve worked closely with the PyTorch team to scale training of MoE models. In this blog post, we’ll talk about how we scale to over three thousand GPUs using &lt;a href=&quot;https://pytorch.org/tutorials/beginner/dist_overview.html&quot;&gt;PyTorch Distributed&lt;/a&gt; and &lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt;, an efficient open-source MoE implementation in PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-moe&quot;&gt;What is a MoE?&lt;/h2&gt;

&lt;p&gt;A MoE model is a model architecture that uses multiple expert networks to make predictions. A gating network is used to route and combine the outputs of experts, ensuring each expert is trained on a different, specialized distribution of tokens. The architecture of a transformer-based large language model typically consists of an embedding layer that leads into multiple transformer blocks (Figure 1, Subfigure A). Each transformer block contains an attention block and a dense feed forward network (Figure 1, Subfigure B). These transformer blocks are stacked such that the output of one transformer block leads to the input of the next block. The final output goes through a fully connected layer and softmax to obtain probabilities for the next token to output.&lt;/p&gt;

&lt;p&gt;When using a MoE in LLMs, the dense feed forward layer is replaced by a MoE layer which consists of a gating network and a number of experts (Figure 1, Subfigure D). The gating network, typically a linear feed forward network, takes in each token and produces a set of weights that determine which tokens are routed to which experts. The experts themselves are typically implemented as a feed forward network as well. During training, the gating network adapts to assign inputs to the experts, enabling the model to specialize and improve its performance. The router outputs are then used to weigh expert outputs to give the final output of the MoE layer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg1.png&quot; alt=&quot;Figure 1: Using Mixture of Experts in a transformer block&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: Using Mixture of Experts in a transformer block&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Compared to dense models, MoEs provide more efficient training for a given compute budget. This is because the gating network only sends tokens to a subset of experts, reducing the computational load. As a result, the capacity of a model (its total number of parameters) can be increased without proportionally increasing the computational requirements. During inference, only some of the experts are used, so a MoE is able to perform faster inference than a dense model. However, the entire model needs to be loaded in memory, not just the experts being used.&lt;/p&gt;

&lt;p&gt;The sparsity in MoEs that allows for greater computational efficiency comes from the fact that a particular token will only be routed to a subset of experts. The number of experts and how experts are chosen depends on the implementation of the gating network, but a common method is top k. The gating network first predicts a probability value for each expert, then routes the token to the top k experts to obtain the output. However, if all tokens always go to the same subset of experts, training becomes inefficient and the other experts end up undertrained. To alleviate this problem, a load balancing loss is introduced that encourages even routing to all experts.&lt;/p&gt;

&lt;p&gt;The number of experts and choosing the top k experts is an important factor in designing MoEs. A higher number of experts allows scaling up to larger models without increasing computational cost. This means that the model has a higher capacity for learning, however, past a certain point the performance gains tend to diminish. The number of experts chosen needs to be balanced with the inference costs of serving the model since the entire model needs to be loaded in memory. Similarly, when choosing top k, a lower top k during training results in smaller matrix multiplications, leaving free computation on the table if communication costs are large enough. During inference, however, a higher top k generally leads to slower inference speed.&lt;/p&gt;

&lt;h2 id=&quot;megablocks&quot;&gt;MegaBlocks&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/databricks/megablocks&quot;&gt;MegaBlocks&lt;/a&gt; is an efficient MoE implementation that uses sparse matrix multiplication to compute expert outputs in parallel despite uneven token assignment. MegaBlocks implements a dropless MoE that avoids dropping tokens while using GPU kernels that maintain efficient training. Prior to MegaBlocks, dynamic routing formulations forced a tradeoff between model quality and hardware efficiency. Previously, users had to either drop tokens from computation or waste computation and memory on padding. Experts can receive a variable number of tokens and the expert computation can be performed efficiently using block sparse matrix multiplication. We’ve &lt;a href=&quot;https://www.databricks.com/blog/bringing-megablocks-databricks&quot;&gt;integrated MegaBlocks into LLM Foundry&lt;/a&gt; to enable scaling MoE training to thousands of GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg2.png&quot; alt=&quot;Figure 2: Matrix multiplication for expert computations&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: Matrix multiplication for expert computations&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;expert-parallelism&quot;&gt;Expert Parallelism&lt;/h3&gt;

&lt;p&gt;As models scale to larger sizes and fail to fit on a single GPU, we require more advanced forms of parallelism. Expert parallelism is a form of model parallelism where we place different experts on different GPUs for better performance. Instead of expert weights being communicated across all GPUs, tokens are sent to the device that contains the expert. By moving data instead of weights, we can aggregate data across multiple machines for a single expert. The router determines which tokens from the input sequence should be sent to which experts. This is typically done by computing a gating score for each token-expert pair, and then routing each token to the top-scoring experts. Once the token-to-expert assignments are determined, an all-to-all communication step is performed to dispatch the tokens to the devices hosting the relevant experts. This involves each device sending the tokens assigned to experts on other devices, while receiving tokens assigned to its local experts.&lt;/p&gt;

&lt;p&gt;The key advantage of expert parallelism is processing a few, larger matrix multiplications instead of several small matrix multiplications. As each GPU only has a subset of experts, it only has to do computation for those experts. Correspondly, as we aggregate tokens across multiple GPUs, the size of each matrix is proportionally larger. As GPUs are optimized for large-scale parallel computations, larger operations can better exploit their capabilities, leading to higher utilization and efficiency. A more in depth explanation of the benefits of larger matrix multiplications can be found &lt;a href=&quot;https://www.thonking.ai/p/what-shapes-do-matrix-multiplications&quot;&gt;here&lt;/a&gt;. Once the computation is complete, another all-to-all communication step is performed to send the expert outputs back to their original devices.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg3.png&quot; alt=&quot;Figure 3: Token routing in expert parallelism&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Token routing in expert parallelism&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We leverage PyTorch’s &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt;, a low-level abstraction for describing how tensors are sharded and replicated, to effectively implement expert parallelism. We first manually place experts on different GPUs, typically sharding across a node to ensure we can leverage NVLink for fast GPU communication when we route tokens. We can then build a &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_device_mesh.html&quot;&gt;device mesh&lt;/a&gt; on top of this layout, which lets us succinctly describe the parallelism across the entire cluster. We can use this device mesh to easily checkpoint or rearrange experts when we need alternate forms of parallelism.&lt;/p&gt;

&lt;h3 id=&quot;scaling-zero-3-with-pytorch-fsdp&quot;&gt;Scaling ZeRO-3 with PyTorch FSDP&lt;/h3&gt;

&lt;p&gt;In conjunction with expert parallelism, we use data parallelism for all other layers, where each GPU stores a copy of the model and optimizer and processes a different chunk of data. After each GPU has completed a forward and backward pass, gradients are accumulated across GPUs for a global model update.&lt;/p&gt;

&lt;p&gt;ZeRO-3 is a form of data parallelism where weights and optimizers are sharded across each GPU instead of being replicated. Each GPU now only stores a subset of the full model, dramatically reducing memory pressure. When a part of the model is needed for computation, it is gathered across all the GPUs, and after the computation is complete, the gathered weights are discarded. We use PyTorch’s implementation of ZeRO-3, called &lt;a href=&quot;https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/&quot;&gt;Fully Sharded Data Parallel (FSDP)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As we scale to thousands of GPUs, the cost of communication across devices increases, slowing down training. Communication increases due to the need to synchronize and share model parameters, gradients, and optimizer states across all GPUs which involves all-gather and reduce-scatter operations. To mitigate this issue while keeping the benefits of FSDP, we utilize Hybrid Sharded Data Parallel (HSDP) to shard the model and optimizer across a set number of GPUs and replicate this multiple times to fully utilize the cluster. With HSDP, an additional all reduce operation is needed in the backward pass to sync gradients across replicas. This approach allows us to balance memory efficiency and communication cost during large scale distributed training. To use HSDP we can extend our previous device mesh from expert parallelism and let PyTorch do the heavy lifting of actually sharding and gathering when needed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg4.png&quot; alt=&quot;Figure 4: FSDP and HSDP&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: FSDP and HSDP&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;With PyTorch, we can effectively combine these two types of parallelism, leveraging FSDP’s higher level API while using the lower-level &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/distributed/_tensor/README.md&quot;&gt;DTensor&lt;/a&gt; abstraction when we want to implement something custom like expert parallelism. We now have a 3D device mesh with expert parallel shard dimension, ZeRO-3 shard dimension, and a replicate dimension for pure data parallelism. Together, these techniques deliver near linear scaling across very large clusters, allowing us to achieve MFU numbers over 40%.&lt;/p&gt;

&lt;h3 id=&quot;elastic-checkpointing-with-torch-distributed&quot;&gt;Elastic Checkpointing with Torch Distributed&lt;/h3&gt;

&lt;p&gt;Fault tolerance is crucial for ensuring that LLMs can be trained reliably over extended periods, especially in distributed environments where node failures are common. To avoid losing progress when jobs inevitably encounter failures, we checkpoint the state of the model, which includes parameters, optimizer states, and other necessary metadata. When a failure occurs, the system can resume from the last saved state rather than starting over. To ensure robustness to failures, we need to checkpoint often and save and load checkpoints in the most performant way possible to minimize downtime. Additionally, if too many GPUs fail, our cluster size may change. Accordingly, we need the ability to elastically resume on a different number of GPUs.&lt;/p&gt;

&lt;p&gt;PyTorch supports elastic checkpointing through its distributed training framework, which includes utilities for both saving and loading checkpoints across different cluster configurations. PyTorch Distributed Checkpoint ensures the model’s state can be saved and restored accurately across all nodes in the training cluster in parallel, regardless of any changes in the cluster’s composition due to node failures or additions.&lt;/p&gt;

&lt;p&gt;Additionally, when training very large models, the size of checkpoints may be very large, leading to very slow checkpoint upload and download times. PyTorch Distributed Checkpoint supports sharded checkpoints, which enables each GPU to save and load only its portion of the model. When combining sharded checkpointing with elastic training, each GPU reads the metadata file to determine which shards to download on resumption. The metadata file contains information on what parts of each tensor are stored in each shard. The GPU can then download the shards for its part of the model and load that part of the checkpoint.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-moes/fg5.png&quot; alt=&quot;Figure 5: Checkpointing saving and resumption resharded on additional GPUs&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: Checkpointing saving and resumption resharded on additional GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;By parallelizing checkpointing across GPUs, we can spread out network load, improving robustness and speed. When training a model with 3000+ GPUs, network bandwidth quickly becomes a bottleneck. We take advantage of the replication in HSDP to first download checkpoints on one replica and then send the necessary shards to other replicas. With our integration in &lt;a href=&quot;https://github.com/mosaicml/composer&quot;&gt;Composer&lt;/a&gt;, we can reliably upload checkpoints to cloud storage as frequently as every 30 minutes and automatically resume from the latest checkpoint in the event of a node failure in less than 5 minutes.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We’re very excited to see how PyTorch is enabling training state-of-the-art LLMs with great performance. In our post, we’ve shown how we implemented efficient MoE training through Pytorch Distributed and MegaBlocks on Foundry. Furthermore, Pytorch elastic checkpointing allowed us to quickly resume training on a different number of GPUs when node failures occurred. Using Pytorch HSDP has allowed us to scale training efficiently as well as improve checkpointing resumption times. We look forward to continuing building on a strong and vibrant open-source community to help bring great AI models to everyone. Come join us in building great models at &lt;a href=&quot;https://github.com/mosaicml/llm-foundry&quot;&gt;LLM Foundry&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/pytorch&quot;&gt;PyTorch&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Brian Chu, Mihir Patel, Less Wright, Vitaliy Chiley, Evan Racah, Wanchao Liang, Iris Zhang, Andrew Gu</name>
        
        
      </author>

      

      

      
        <summary type="html">Over the past year, Mixture of Experts (MoE) models have surged in popularity, fueled by powerful open-source models like DBRX, Mixtral, DeepSeek, and many more. At Databricks, we’ve worked closely with the PyTorch team to scale training of MoE models. In this blog post, we’ll talk about how we scale to over three thousand GPUs using PyTorch Distributed and MegaBlocks, an efficient open-source MoE implementation in PyTorch.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity</title>
      <link href="https://pytorch.org/blog/accelerating-neural-network-training/" rel="alternate" type="text/html" title="Accelerating Neural Network Training with Semi-Structured (2:4) Sparsity" />
      <published>2024-06-20T00:00:00-07:00</published>
      <updated>2024-06-20T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-neural-network-training</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-neural-network-training/">&lt;p&gt;Over the past year, we’ve added support for semi-structured (2:4) sparsity into PyTorch. With just a few lines of code, we were able to show a 10% end-to-end inference speedup on &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity#segment-anything&quot;&gt;segment-anything&lt;/a&gt; by replacing dense matrix multiplications with sparse matrix multiplications.&lt;/p&gt;

&lt;p&gt;However, matrix multiplications are not unique to neural network inference - they happen during training as well. By expanding on the core primitives we used earlier to accelerate inference, we were also able to accelerate model training. We wrote a replacement nn.Linear layer, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SemiSparseLinear&lt;/code&gt;, that is able to achieve a 1.3x &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity/training#benchmarking&quot;&gt;speedup&lt;/a&gt; across the forwards + backwards pass of the linear layers in the MLP block of ViT-L on a NVIDIA A100.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;End-to-end, we see a wall time reduction of 6% for a &lt;a href=&quot;https://github.com/facebookresearch/dinov2&quot;&gt;DINOv2 ViT-L&lt;/a&gt; training, with virtually no accuracy degradation out of the box (82.8 vs 82.7 on ImageNet top-1 accuracy).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg1.png&quot; alt=&quot;2 strategies for training a ViT model&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We compare 2 strategies for training a ViT model for 125k iterations on 4x NVIDIA A100s: either fully dense (blue), or sparse for 70% of the training, then dense (orange). Both achieve similar results on the benchmarks, but the sparse variant trains 6% faster. For both experiments, we evaluate the intermediate checkpoints with and without sparsity.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As far as we are aware, &lt;strong&gt;this is the first OSS implementation of accelerated sparse training&lt;/strong&gt; and we’re excited to provide a user API in &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity/training#benchmarking&quot;&gt;torchao&lt;/a&gt;. You can try accelerating your own training runs with just a few lines of code:&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Requires torchao and pytorch nightlies and CUDA compute capability 8.0+
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.sparsity.training&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;SemiSparseLinear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;swap_linear_with_semi_sparse_linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4096&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;half&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Specify the fully-qualified-name of the nn.Linear modules you want to swap
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparse_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;seq.0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SemiSparseLinear&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Swap nn.Linear with SemiSparseLinear, you can run your normal training loop after this step
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;swap_linear_with_semi_sparse_linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;how-does-this-work&quot;&gt;How does this work?&lt;/h2&gt;

&lt;p&gt;The general idea behind sparsity is simple: skip calculations involving zero-valued tensor elements to speed up matrix multiplication. However, simply setting weights to zero isn’t enough, as the dense tensor still contains these pruned elements and dense matrix multiplication kernels will continue to process them, incurring the same latency and memory overhead. To achieve actual performance gains, we need to replace dense kernels with sparse kernels that intelligently bypass calculations involving pruned elements.&lt;/p&gt;

&lt;p&gt;These kernels work on sparse matrices, which remove the pruned elements and store the specified elements in a compressed format. There are many different sparse formats, but we’re particularly interested in &lt;strong&gt;semi-structured sparsity,&lt;/strong&gt; also known as &lt;strong&gt;2:4 structured sparsity&lt;/strong&gt; or &lt;strong&gt;fine-grained structured sparsity&lt;/strong&gt; or more generally &lt;strong&gt;N:M structured sparsity&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg2.png&quot; alt=&quot;2:4 sparse compressed representation&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2:4 sparse compressed representation. Original &lt;a href=&quot;https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/&quot;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A 2:4-sparse matrix is a matrix where at most 2 elements are non-zero for every 4 elements, as illustrated in the image above. Semi-structured sparsity is attractive because it exists in a goldilocks spot of performance and accuracy:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;NVIDIA GPUs since Ampere offer hardware acceleration and library support (&lt;a href=&quot;https://docs.nvidia.com/cuda/cusparselt/index.html&quot;&gt;cuSPARSELt&lt;/a&gt;) for this format, with matrix multiplication being up to 1.6x faster&lt;/li&gt;
  &lt;li&gt;Pruning models to fit this sparsity pattern does not degrade accuracy as much as other patterns. NVIDIA’s &lt;a href=&quot;https://arxiv.org/pdf/2104.08378&quot;&gt;whitepaper&lt;/a&gt; shows pruning then retraining is able to recover accuracy for most vision models.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg3.png&quot; alt=&quot;Illustration of 2:4 (sparse) matrix multiplication on NVIDIA GPUs&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustration of 2:4 (sparse) matrix multiplication on NVIDIA GPUs. Original &lt;a href=&quot;https://arxiv.org/pdf/2104.08378&quot;&gt;source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Accelerating inference with semi-structured sparsity is straightforward. Since our weights are fixed during inference, we can prune and compress the weight ahead of time (offline) and store the compressed sparse representation instead of our dense tensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg4.png&quot; alt=&quot;flow chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, instead of dispatching to dense matrix multiplication we dispatch to sparse matrix multiplication, passing in the compressed sparse weight instead of the normal dense one. For more information about accelerating models for inference using 2:4 sparsity, please refer to our &lt;a href=&quot;https://pytorch.org/tutorials/advanced/semi_structured_sparse.html?highlight=beta&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;extending-sparse-inference-acceleration-to-training&quot;&gt;Extending sparse inference acceleration to training&lt;/h2&gt;

&lt;p&gt;In order to use sparsity to reduce the training time of our models, we need to consider when the mask is calculated, as once we store the compressed representation the mask is fixed.&lt;/p&gt;

&lt;p&gt;Training with a fixed mask applied to an existing trained dense model (also known as &lt;strong&gt;pruning&lt;/strong&gt;) does not degrade accuracy, but this requires two training runs - one to obtain the dense model and another to make it sparse, offering no speedups.&lt;/p&gt;

&lt;p&gt;Instead we’d like to train a sparse model from scratch (&lt;strong&gt;dynamic sparse training&lt;/strong&gt;), but training from scratch with a fixed mask will lead to a significant drop in evaluations, as the sparsity mask would be selected at initialization, when the model weights are essentially random.&lt;/p&gt;

&lt;p&gt;To maintain the accuracy of the model when training from scratch, we prune and compress the weights at runtime, so that we can calculate the optimal mask at each step of the training process.&lt;/p&gt;

&lt;p&gt;Conceptually you can think of our approach as an approximate matrix multiplication technique, where we &lt;code&gt;`prune_and_compress`&lt;strong&gt; &lt;/strong&gt;&lt;/code&gt;and dispatch to &lt;code&gt;`sparse_GEMM`&lt;/code&gt; in less time than a &lt;code&gt;`dense_GEMM`&lt;/code&gt; call would take. This is difficult because the native pruning and compression functions are too slow to show speedups.&lt;/p&gt;

&lt;p&gt;Given the shapes of our ViT-L training matrix multiplications (13008x4096x1024), we measured the runtime of a dense and sparse GEMM respectively at 538us and 387us. In other words, the pruning and compression step of the weight matrix must run in less than 538-387=151us to have any efficiency gain. Unfortunately, the compression kernel provided in cuSPARSELt already takes 380us (without even considering the pruning step!).&lt;/p&gt;

&lt;p&gt;Given the max NVIDIA A100 memory IO (2TB/s), and considering that a prune and compress kernel would be memory bound, we could theoretically prune and compress our weight (4096x1024x2 bytes=8MB) in 4us (8MB / 2TB/s)! And in fact, we were able to write a kernel that prunes and compresses a matrix into 2:4-sparse format, and runs in 36 us (10x faster than the compression kernel in cuSPARSELt), making the entire GEMM (including the sparsification) faster. Our kernel is &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/122350&quot;&gt;available&lt;/a&gt; for use in PyTorch.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg5.png&quot; alt=&quot;Our custom sparsification kernel&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Our custom sparsification kernel, which includes pruning + compression, is ~30% faster across a linear layer forward+backward. Benchmarks run on a NVIDIA A100-80GB GPU.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;writing-a-performant-runtime-sparsification-kernel&quot;&gt;Writing a performant runtime sparsification kernel&lt;/h3&gt;

&lt;p&gt;There were multiple challenges we faced in order to implement a performant runtime sparsification kernel, which we will explore below.&lt;/p&gt;

&lt;h4 id=&quot;1-handling-the-backwards-pass&quot;&gt;1) Handling the backwards pass&lt;/h4&gt;

&lt;p&gt;For the backwards pass, we need to calculate dL/dX and dL/dW for the gradient update and the subsequent layer, which means we need to calculate xW&lt;sup&gt;T&lt;/sup&gt; and x&lt;sup&gt;T&lt;/sup&gt;W respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg6.png&quot; alt=&quot;Overview of runtime sparsification for training acceleration (FW + BW pass)&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Overview of runtime sparsification for training acceleration (FW + BW pass)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;However this is problematic, because the compressed representation cannot be transposed, since there’s no guarantee that the tensor is 2:4 sparse in both directions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg7.png&quot; alt=&quot;Both matrices are valid 2:4 matrices. However, the right one is no longer a valid 2:4 matrix once transposed because one column contains more than 2 elements&quot; style=&quot;width:100%;display:block;max-width:500px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Both matrices are valid 2:4 matrices. However, the right one is no longer a valid 2:4 matrix once transposed because one column contains more than 2 elements&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Therefore, we prune a 4x4 tile, instead of a 1x4 strip. We greedily preserve the largest values, ensuring that we take at most 2 values for each row / column. While this approach is not guaranteed to be optimal, as we sometimes only preserve 7 values instead of 8, it efficiently calculates a tensor that is 2:4 sparse both row-wise and column-wise.&lt;/p&gt;

&lt;p&gt;We then compress both the packed tensor and the packed transpose tensor, storing the transpose tensor for the backwards pass. By calculating both the packed and packed transpose tensor at the same time, we avoid a secondary kernel call in the backwards pass.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg8.png&quot; alt=&quot;Our kernel prunes the weight matrix in registers&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Our kernel prunes the weight matrix in registers, and writes the compressed values in global memory. It also prunes at the same time W.t, which is needed for the backward pass, minimizing the memory IO&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;There’s some additional transpose trickery needed to handle the backwards pass - the underlying hardware only supports operations where the first matrix is sparse. For weight sparsification during inference, when we need to calculate xW&lt;sup&gt;T&lt;/sup&gt; we rely on transpose properties to swap the order of the operands.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg1.jpg&quot; alt=&quot;Math formula&quot; style=&quot;width:100%;display:block;max-width:300px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During inference, we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; to fuse the outer transpose into subsequent pointwise ops in order to avoid paying a performance penalty.&lt;/p&gt;

&lt;p&gt;However in the case of the backwards pass of training, we have no subsequent pointwise op to fuse with. Instead, we fuse the transposition into our matrix multiplication by taking advantage of cuSPARSELt’s ability to specify the row / column layout of the result matrix.&lt;/p&gt;

&lt;h4 id=&quot;2-kernel-tiling-for-efficient-memory-io&quot;&gt;2) Kernel tiling for efficient memory-IO&lt;/h4&gt;

&lt;p&gt;In order for our kernel to be as efficient as possible, we want to coalesce our reads / writes, as we found that memory IO to be the main bottleneck. This means that within a CUDA thread, we want to read/write chunks of 128 bytes at a time, so that multiple parallel reads/writes can be coalesced into a single request by the GPU memory controller.&lt;/p&gt;

&lt;p&gt;Therefore, instead of a thread handling a single 4x4 tile, which is only 4x4x2 = 32 bytes, we decided that each thread will handle 4 4x4 tiles (aka an 8x8 tile), which allows us to operate 8x8x2 =128 byte chunks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg9.png&quot; alt=&quot;Kernel tiling for efficient memory-IO&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3-sorting-elements-in-a-4x4-tile-without-warp-divergence&quot;&gt;3) Sorting elements in a 4x4 tile without warp-divergence&lt;/h4&gt;

&lt;p&gt;For each individual 4x4 tile within our thread we calculate a bitmask that specifies which elements to prune and which elements to keep. To do this we sort all 16 elements and greedily preserve elements, so long as they do not break our 2:4 row / col constraint. This preserves only the weights with the largest values.&lt;/p&gt;

&lt;p&gt;Crucially we observe that we are only ever sorting a fixed number of elements, so by using a branchless &lt;a href=&quot;https://en.wikipedia.org/wiki/Sorting_network&quot;&gt;sorting network&lt;/a&gt;, we can avoid warp divergence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg10.png&quot; alt=&quot;Sorting network diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For clarity, the transposed packed tensor and metadata are omitted. Sorting network diagram taken from &lt;a href=&quot;https://en.wikipedia.org/wiki/Sorting_network&quot;&gt;Wikipedia&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Warp divergence occurs when we have conditional execution inside across a thread block. In CUDA, work items in the same work group (thread block) are dispatched at the hardware level in batches (warps). If we have conditional execution, such that some work-items in the same batch run different instructions, then they are masked when the warp is dispatched, or dispatched sequentially.&lt;/p&gt;

&lt;p&gt;For example, if we have some code like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if (condition) do(A) else do(B)&lt;/code&gt;, where condition is satisfied by all the odd-numbered work items, then the total runtime of this conditional statement is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do(A) + do(B)&lt;/code&gt;, since we would dispatch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do(A)&lt;/code&gt; for all odd-numbered work-items, masking out even-numbered work-items, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;do(B)&lt;/code&gt; for all even numbered work-items, masking out odd-numbered work-items. This &lt;a href=&quot;https://www.reddit.com/r/CUDA/comments/gkpjxe/what_is_warp_divergence/#:~:text=Warp%20divergence%20is%20a%20%22Compute,later%20processed%20using%20different%20instructions.&quot;&gt;answer&lt;/a&gt; provides more information about warp divergence.&lt;/p&gt;

&lt;h4 id=&quot;4-writing-the-compressed-matrices-and-metadata&quot;&gt;4) Writing the compressed matrices and metadata&lt;/h4&gt;

&lt;p&gt;Once the bitmask has been computed, the weight data has to be written back in a compressed format in global memory. This is not trivial, because the data needs to stay in registers, and it’s not possible to index registers (eg &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C[i++] = a&lt;/code&gt; prevents us from storing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt; in registers). Furthermore, we found that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nvcc&lt;/code&gt; was using many more registers than we expected, which caused register spilling and impacted global performance. We write this compressed matrix to global memory in Column-Major format to make the writes more efficient.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg11.png&quot; alt=&quot;compressed matrix to global memory in Column-Major format&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also need to write the cuSPARSELt metadata as well. This metadata layout is quite similar to the one from the open-source CUTLASS library and is optimized for being loaded efficiently through shared-memory in the GEMM kernel with the PTX &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ldmatrix&lt;/code&gt; instruction.&lt;/p&gt;

&lt;p&gt;However, this layout is not optimized to be written efficiently: the first 128 bits of the metadata tensor contains metadata about the first 32 columns of the rows 0, 8, 16 and 24. Recall that each thread handles an 8x8 tile, which means that this information is scattered across 16 threads.&lt;/p&gt;

&lt;p&gt;We rely on a series of warp-shuffle operations, once for the original and transposed representation respectively to write the metadata. Fortunately, this data represents less than 10% of the total IO, so we can afford to not fully coalesce the writes.&lt;/p&gt;

&lt;h2 id=&quot;dinov2-sparse-training-experimental-setup-and-results&quot;&gt;DINOv2 Sparse Training: Experimental Setup and Results&lt;/h2&gt;

&lt;p&gt;For our experiments, the ViT-L model is trained on ImageNet for 125k steps using the DINOv2 method. All our experiments were run on 4x AMD EPYC 7742 64-core CPUs and 4x NVIDIA A100-80GB GPUs. During sparse training, the model is trained with 2:4 sparsity enabled for the first part of the training, where only half of the weights are enabled. This sparsity mask on the weights is dynamically recomputed at every step, as weights are continuously updated during the optimization. For the remaining steps, the model is trained densely, producing a final model without 2:4 sparsity (except the 100% sparse training setup), which is then evaluated.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Training setup
   &lt;/td&gt;
   &lt;td&gt;ImageNet 1k log-regression
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;0% sparse (125k dense steps, baseline)
   &lt;/td&gt;
   &lt;td&gt;82.8
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;40% sparse (50k sparse -&amp;gt; 75k dense steps)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;82.9&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;60% sparse (75k sparse -&amp;gt; 50k dense steps)
   &lt;/td&gt;
   &lt;td&gt;82.8
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;70% sparse (87.5k sparse -&amp;gt; 37.5k dense steps)
   &lt;/td&gt;
   &lt;td&gt;82.7
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;80% sparse (100k sparse -&amp;gt; 25k dense steps)
   &lt;/td&gt;
   &lt;td&gt;82.7
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;90% sparse (112.5k sparse -&amp;gt; 12.5k dense steps)
   &lt;/td&gt;
   &lt;td&gt;82.0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;100% sparse (125k sparse steps) 
   &lt;/td&gt;
   &lt;td&gt;82.3 (2:4-sparse model)
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-neural-network-training/fg12.png&quot; alt=&quot;sparsity training diagrams&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the sparse training steps, in the backward pass we obtain a dense gradient for the sparse weights. For the gradient descent to be sound, we should also sparsify this gradient before using it in the optimizer to update the weights. Instead of doing that, we use the full dense gradient to update the weights - we found this to work better in practice: this is the STE (&lt;a href=&quot;https://arxiv.org/pdf/1903.05662&quot;&gt;Straight Through Estimator&lt;/a&gt;) strategy. In other words, we update all the parameters at every step, even the ones we don’t use.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;In this blog post, we’ve shown how to accelerate neural network training with semi-structured sparsity and explained some of the challenges we faced. We were able to achieve a 6% end to end speedup on DINOv2 training with a small 0.1 pp accuracy drop.&lt;/p&gt;

&lt;p&gt;There are several areas of expansion for this work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Expansion to new sparsity patterns:&lt;/strong&gt; Researchers have created new sparsity patterns like &lt;a href=&quot;https://arxiv.org/pdf/2310.02065&quot;&gt;V:N:M&lt;/a&gt; sparsity that use the underlying semi-structured sparse kernels to allow for more flexibility. This is especially interesting for applying sparsity to LLMs, as 2:4 sparsity degrades accuracy too much, but we have seen some positive &lt;a href=&quot;https://arxiv.org/pdf/2310.06927&quot;&gt;results&lt;/a&gt; for more general N:M pattern.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Performance optimizations for sparse fine-tuning:&lt;/strong&gt; This post covers sparse training from scratch, but oftentimes we want to fine-tune a foundational model. In this case, a static mask may be sufficient to preserve accuracy which would enable us to make additional performance optimizations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;More experiments on pruning strategy:&lt;/strong&gt; We calculate the mask at each step of the network, but calculating the mask every n steps may yield better training accuracy. Overall, figuring out the best strategy to use semi-structured sparsity during training is an open area of research.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compatibility with fp8:&lt;/strong&gt; The hardware also supports fp8 semi-structured sparsity, and this approach should work similarly with fp8 in principle. In practice, we would need to write similar sparsification kernels, and could possibly fuse them with the scaling of the tensors.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Activation Sparsity:&lt;/strong&gt; Efficient sparsification kernels also enable to sparsify the activations during training. Because the sparsification overhead grows linearly with the sparsified matrix size, setups with large activation tensors compared to the weight tensors could benefit more from activation sparsity than weight sparsity. Furthermore, activations are naturally sparse because of the usage of ReLU or GELU activation functions, reducing accuracy degradation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are interested in these problems, please feel free to open an issue / PR in &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao&lt;/a&gt;, a community we’re building for architecture optimization techniques like quantization and sparsity.  Additionally, if you have general interest in sparsity please reach out in &lt;a href=&quot;discord.gg/cudamode&quot;&gt;CUDA-MODE&lt;/a&gt; (#sparsity)&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jesse Cai, Daniel Haziza, Supriya Rao</name>
        
        
      </author>

      

      

      
        <summary type="html">Over the past year, we’ve added support for semi-structured (2:4) sparsity into PyTorch. With just a few lines of code, we were able to show a 10% end-to-end inference speedup on segment-anything by replacing dense matrix multiplications with sparse matrix multiplications.</summary>
      

      
      
    </entry>
  
</feed>



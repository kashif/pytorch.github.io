<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-01-25T17:03:41-08:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Accelerating Generative AI with PyTorch IV: Seamless M4T, fast</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai-4/" rel="alternate" type="text/html" title="Accelerating Generative AI with PyTorch IV: Seamless M4T, fast" />
      <published>2024-01-23T00:00:00-08:00</published>
      <updated>2024-01-23T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai-4</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai-4/">&lt;p&gt;This post is the fourth part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. To skip to the code, check out our github (&lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/pull/328&quot;&gt;seamless_communication&lt;/a&gt;, &lt;a href=&quot;https://github.com/facebookresearch/fairseq2/pull/272&quot;&gt;fairseq2&lt;/a&gt;). We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;Segment Anything over 8x&lt;/a&gt; using only pure, native PyTorch. In part two, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;Llama-7B by almost 10x&lt;/a&gt; using only native PyTorch optimizations. In part three, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-3/&quot;&gt;text-to-image diffusion models up to 3x&lt;/a&gt; using only native Pytorch optimizations.&lt;/p&gt;

&lt;p&gt;In this blog, we’ll focus on speeding up FAIR’s Seamless M4T-v2 model resulting in &lt;strong&gt;2x speedup for text decoder module &lt;em&gt;and&lt;/em&gt; 30x for vocoder module, resulting in 2.7x speedup for end-to-end inference&lt;/strong&gt;, with no loss of accuracy by using CUDA Graph and native PyTorch optimization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;torch.compile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg1.png&quot; alt=&quot;End to End Inference Speedup&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Seamless M4T is an open-source foundational speech/text translation and transcription technology developed by FAIR. Seamless M4T is a massively multilingual and multimodal machine translation model, with the &lt;a href=&quot;https://fb.workplace.com/groups/2447831298797573/permalink/3604327353147956/&quot;&gt;latest version&lt;/a&gt; (Seamless M4T-v2) released on November 30th, 2023. The high-level model architecture of Seamless M4T-v2 is illustrated in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg2.png&quot; alt=&quot;Model Architecture of Seamless M4T-v2&quot; style=&quot;width:100%;max-width:600px; display:block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Model Architecture of Seamless M4T-v2.&lt;/p&gt;

&lt;p&gt;Accelerating inference latency is crucial for translation models to improve user experience through faster communication across languages. In particular, batch_size=1 is often used for fast translation where latency matters a lot in applications such as chatbots, speech translation, and live subtitling. Therefore, we conducted the performance analysis on inference with batch_size=1, as shown in Figure 2 to understand the Amdahl’s Law bottleneck. Our results indicate that the text decoder and vocoder are the most time-consuming modules, accounting for 61% and 23% of the inference time, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg3.png&quot; alt=&quot;Text decoder and vocoder are the most time consuming module. Breakdown of inference time by modules for English-Spanish S2ST (Speech-to-Speech-Text) task for batch_size=1 on A100 GPU.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; &lt;span style=&quot;text-decoration:underline;&quot;&gt;Text decoder and vocoder are the most time consuming module&lt;/span&gt;. Breakdown of inference time by modules for English-Spanish S2ST (Speech-to-Speech-Text) task for batch_size=1 on A100 GPU.&lt;/p&gt;

&lt;p&gt;To take a closer look at the performance bottleneck of the text decoder and vocoder, we analyzed GPU traces for the text decoder and vocoder for the 8th sample for the English-Spanish translation example of &lt;a href=&quot;https://huggingface.co/datasets/google/fleurs&quot;&gt;FLEURS&lt;/a&gt; dataset as shown in Figure 3. It revealed that the &lt;strong&gt;text decoder and vocoder are heavily CPU-bound modules.&lt;/strong&gt; We observed a significant gap incurred by CPU overhead that delayed the launch of GPU kernels, resulting in a substantial increase in the execution time for both the modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg4.jpg&quot; alt=&quot;CPU and GPU trace for Text Decoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; CPU and GPU trace for Text Decoder&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg5.jpg&quot; alt=&quot;CPU and GPU trace for Vocoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b)&lt;/strong&gt; CPU and GPU trace for Vocoder&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; &lt;span style=&quot;text-decoration:underline;&quot;&gt;Text Decoder and Vocoder are heavily CPU-bound modules&lt;/span&gt;. CPU and GPU trace for (a) Text Decoder (b) Vocoder for the 8th sample for English-Spanish translation example of &lt;a href=&quot;https://huggingface.co/datasets/google/fleurs&quot;&gt;FLEURS&lt;/a&gt; dataset. The trace is obtained by running inference with batch_size=1 on A100 gpu.&lt;/p&gt;

&lt;p&gt;Based on the real-system performance analysis results that text_decoder and vocoder are heavily CPU bound modules in Seamless M4T-v2, we enabled torch.compile + CUDA Graph to those modules. In this post, we share modifications required to enable torch.compile + CUDA Graph on each module for batch_size=1 inference scenario, discussion on CUDA Graph and next step plans.&lt;/p&gt;

&lt;h2 id=&quot;torchcompile-with-cuda-graph&quot;&gt;Torch.compile with CUDA Graph&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a PyTorch API that allows users to compile PyTorch models into a standalone executable or script which is generally used for optimizing model performance by removing unnecessary overhead.&lt;/p&gt;

&lt;p&gt;CUDA Graph is a feature provided by NVIDIA that allows for the optimization of kernel launches in CUDA applications. It creates an execution graph of CUDA kernels, which can be pre-processed and optimized by the driver before being executed on the GPU. The main advantage of using CUDA Graph is that it reduces the overhead associated with launching individual kernels, as the graph can be launched as a single unit, reducing the number of API calls and data transfers between the host and device. This can lead to significant performance improvements, especially for applications that have a large number of small kernels or repeat the same set of kernels multiple times. If this is something you are interested in learning more about, check out this paper that highlights the important role of data for accelerated computing: &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/5762730&quot;&gt;Where is the data? Why you cannot debate CPU vs. GPU performance without the answer&lt;/a&gt;&lt;/strong&gt; by our own Kim Hazelwood! This is when NVIDIA was heavily investing in general-purpose GPU (GPGPUs) and before deep learning revolutionized the computing industry!&lt;/p&gt;

&lt;p&gt;However, because CUDA Graph operates on 1) fixed memory pointer, 2) fixed shape of tensors, that are recorded at the compile time, we introduced the following improvements for CUDA Graph to be reused across multiple sizes of inputs to &lt;em&gt;prevent CUDA Graph generation for each iteration&lt;/em&gt; and let the data inside CUDA Graph be reused across different runs &lt;em&gt;to share KV Cache for multiple decoding steps&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;text-decoder&quot;&gt;Text Decoder&lt;/h2&gt;

&lt;p&gt;The Text Decoder in Seamless is a decoder from NLLB [&lt;a href=&quot;https://ai.meta.com/research/no-language-left-behind/&quot;&gt;1&lt;/a&gt;] that performs T2TT (Text to Text Translation). Also, this module is a CPU-bound model where gpu execution time is not long enough to hide CPU overhead because of &lt;strong&gt;the nature of auto-regressive generation that requires sequential processing of tokens&lt;/strong&gt;, which limits the amount of parallelism that can be achieved on the GPU. Based on this observation, we enabled torch.compile + CUDA Graph for the text decoders to reduce the dominating  CPU overhead as shown in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg6.png&quot; alt=&quot;CPU and GPU trace for Text Decoder after torch.compile + CUDA Graph are enabled&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; CPU and GPU trace for Text Decoder after torch.compile + CUDA Graph are enabled.&lt;/p&gt;

&lt;h3 id=&quot;1--updating-and-retrieving-kv-cache&quot;&gt;1.  Updating and retrieving KV cache&lt;/h3&gt;

&lt;p&gt;During inference, the text decoder has two computation phases: a prefill phase that consumes the prompt and an incremental generation phase that generates output tokens one by one. Given a high enough batch size or input length, prefill operates on a sufficiently high number of tokens in parallel — GPU performance is the bottleneck and the CPU overheads do not impact performance significantly. On the other hand, incremental token generation is always executed with sequence length 1 and it is often executed with a small batch size (even 1), e.g. for interactive use cases. Thus, incremental generation can be limited by the CPU speed and thus is a good candidate for torch.compile + CUDA Graph.&lt;/p&gt;

&lt;p&gt;However, during the incremental token generation phase, the sequence_length dimension of key and value involved in the attention computation increases by one with each step while the sequence length of query always remains 1. Specifically, key/value are generated by appending the newly computed key/value of sequence length 1 to the key/value stored in the KV cache so far. But as mentioned above, CUDA Graph records all the shapes of tensors during compilation and replay with the recorded shapes. Thus, few modifications have been made to address this issue following the great work &lt;a href=&quot;https://blog.fireworks.ai/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning-353bf6241248&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;a) We modify the KV-cache handling to take the indices in which to write new values in a CUDA Tensor (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt;) rather than a Python integer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg7.png&quot; alt=&quot;Modification to KV cache append and get&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Modification to KV cache &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;append&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;b) We also modify attention to work with the fixed shape of key and value over the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_length&lt;/code&gt;. We only compute softmax over the sequence positions up to the current decoding step (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt;) . To mask out sequence positions &amp;gt; current decoding step (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos)&lt;/code&gt;, we create a boolean mask tensor (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;) where sequence positions &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt; are set to False.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg8.png&quot; alt=&quot;Helper function to generate valid_seq_pos and mask&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Helper function to generate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It’s important to post that these modifications result in an increase in the amount of computation required, as we compute attention over more sequence positions than necessary (up to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_length&lt;/code&gt;). However, despite this drawback, our results demonstrate that torch.compile + CUDA Graph still provide significant performance benefits compared to standard PyTorch code.&lt;/p&gt;

&lt;p&gt;c) As different inference samples have different sequence length, it also generates different shapes of inputs that are to be projected to key and value for the cross attention layers. Thus, we pad the input to have a static shape and generate a padding mask to mask out padded output.&lt;/p&gt;

&lt;h3 id=&quot;2-memory-pointer-management&quot;&gt;2. Memory Pointer Management&lt;/h3&gt;

&lt;p&gt;As CUDA Graph records memory pointers along with the shape of tensors, it is important to make different inference samples to correctly reference the recorded memory pointer (e.g., KV cache) to avoid compiling CUDA Graph for each inference sample. However, some parts of the Seamless codebase made different inference samples to refer to different memory addresses, so we made modifications to improve the memory implications.&lt;/p&gt;

&lt;p&gt;e) Seamless adopts beam search as a text decoding strategy. In the beam search process, we need to perform KV cache reordering for all the attention layers for each incremental decoding step to make sure each selected beam performs with corresponding KV cache as shown in the code snippet below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg8b.png&quot; alt=&quot;KV cache reordering operation for beam search decoding strategy&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; KV cache reordering operation for beam search decoding strategy.&lt;/p&gt;

&lt;p&gt;The above code allocates new memory space and overwrites the original memory pointer for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_k&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_v&lt;/code&gt;. Thus we modified KV cache reordering to keep the memory pointer of each cache as was recorded during compilation by using &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html&quot;&gt;copy_&lt;/a&gt; operator.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg9.png&quot; alt=&quot;In-place update for KV cache using copy_ operator&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; In-place update for KV cache using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;copy_&lt;/code&gt; operator&lt;/p&gt;

&lt;p&gt;f) After enabling torch.compile + CUDA Graph to text decoder by modifying the code as mentioned above, the overhead of text decoder shifts to KV cache reordering as shown in Figure 10. KV cache reordering repeatedly calls index_select 96 times (assuming 24 decoder layers where each layer consists of two types of attention layers with cache for key and value).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg10.png&quot; alt=&quot;CPU and GPU trace for Text Decoder after enabling torch.compile + CUDA Graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; CPU and GPU trace for Text Decoder after enabling torch.compile + CUDA Graph.&lt;/p&gt;

&lt;p&gt;As part of accelerating text decoder, we additionally applied torch.compile to KV cache reordering to benefit from fusing kernels as shown in Figure 11. Note that we cannot use CUDA Graph here (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode='max-autotune'&lt;/code&gt;) here, because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;copy_&lt;/code&gt; operation modifies the inputs which violates the static input requirement of CUDA graph version in torch.compile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg11.png&quot; alt=&quot;Applying torch.compile to KV Cache reordering&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; Applying torch.compile to KV Cache reordering.&lt;/p&gt;

&lt;p&gt;As a result of enabling torch.compile to KV cache reordering, the gpu kernels that were launched separately (Figure 12(a)) are now fused so there are much fewer gpu kernels to launch (Figure 12(b)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg12.png&quot; alt=&quot;CPU and GPU trace for KV cache reordering before enabling torch.compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; CPU and GPU trace for KV cache reordering &lt;strong&gt;before&lt;/strong&gt; enabling torch.compile&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg13.png&quot; alt=&quot;CPU and GPU trace for KV cache reordering after enabling torch.compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b)&lt;/strong&gt; CPU and GPU trace for KV cache reordering &lt;strong&gt;after&lt;/strong&gt; enabling torch.compile&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 12.&lt;/strong&gt; CPU and GPU trace for KV cache reordering (a) before and (b) after enabling torch.compile&lt;/p&gt;

&lt;h2 id=&quot;vocoder&quot;&gt;Vocoder&lt;/h2&gt;

&lt;p&gt;Vocoder in Seamless is a HiFi-GAN unit-vocoder that converts generated units to waveform output where an unit is a representation of speech that combines different aspects such as phonemes and syllables, which can be used to generate sounds that are audible to humans. Vocoder is a relatively simple module that consists of Conv1d and ConvTranspose1d layers and is a CPU bound module as shown in FIgure 3. Based on this observation, we decided to enable torch.compile + CUDA Graph for vocoder to reduce the disproportionally large CPU overhead as shown in Figure 10. But there were several fixes to be made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg14.png&quot; alt=&quot;CPU and GPU trace for Vocoder after torch.compile + CUDA Graph are enabled&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 13.&lt;/strong&gt; CPU and GPU trace for Vocoder after torch.compile + CUDA Graph are enabled.&lt;/p&gt;

&lt;p&gt;a) The input tensor shape of the vocoder is different across different inference samples. But as CUDA Graph records the shape of tensors and replays them, we had to pad the input to the fixed size with zeros. Since vocoder only consists of Conv1d layers, we do not need an additional padding mask, and padding with zeros is sufficient.&lt;/p&gt;

&lt;p&gt;b) Vocoder consists of conv1d layers wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.utils.weight_norm&lt;/code&gt; (see &lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/models/vocoder/hifigan.py#L37-L112&quot;&gt;here&lt;/a&gt;). However, applying torch.compile directly to Vocoder incurs graph break as below, which leads to suboptimal performance improvement. This graph break happens inside the hook handling part in the PyTorch code of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight_norm&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: setattr(UserDefinedObjectVariable) &amp;lt;function Module.__setattr__ at 0x7fac8f483c10&amp;gt; from user code at:
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/vocoder.py&quot;, line 49, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return self.code_generator(x, dur_prediction)  # type: ignore[no-any-return]1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return forward_call(*args, **kwargs)
[2023-12-13 04:26:16,822] [1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/codehifigan.py&quot;, line 101, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return super().forward(x)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/hifigan.py&quot;, line 185, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     x = self.ups[i](x)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1550, in _call_impl
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args_result = hook(self, args)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py&quot;, line 65, in __call__
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     setattr(module, self.name, self.compute_weight(module))
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the weights of layers do not change during the inference, we do not need weight normalization. So we simply removed weight normalization for Vocoder as shown in Figure 14, by utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remove_weight_norm&lt;/code&gt; function which is already provided at the Seamless codebase (&lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/models/vocoder/hifigan.py#L198-L205&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg15.png&quot; alt=&quot;Removing weight_norm for Vocoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 14.&lt;/strong&gt; Removing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight_norm&lt;/code&gt; for Vocoder&lt;/p&gt;

&lt;h2 id=&quot;performance-evaluation--impact-of-cuda-graph&quot;&gt;Performance Evaluation + Impact of CUDA Graph&lt;/h2&gt;

&lt;p&gt;Figure 15 shows the speedup result when enabling torch.compile(mode=”max-autotune”) + CUDA Graph on the text decoder and vocoder. We achieve &lt;strong&gt;2x speedup for the text decoder and 30x speedup for vocoder, leading to 2.7x faster end-to-end inference time.&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;img alt=&quot;Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&quot; src=&quot;/assets/images/accelerating-generative-ai-4/fg16.png&quot; style=&quot;width:100%;&quot; /&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&quot; src=&quot;/assets/images/accelerating-generative-ai-4/fg17.png&quot; style=&quot;width:100%;&quot; /&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 15.&lt;/strong&gt; Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&lt;/p&gt;

&lt;p&gt;We also report the speedups for text decoder and vocoder using torch.compile without CUDA Graph, which is supported by torch.compile’s API (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(mode=&quot;max-autotune-no-cudagraphs&quot;)&lt;/code&gt;), to identify the impact of CUDA Graph on the performance. Without CUDA Graph, the speedup for text decoder and vocoder reduces to 1.17x and 18.4x. While still quite significant, it indicates the important role of CUDA Graph. We conclude that Seamless M4T-v2 is exposed to a lot of time launching CUDA kernels, especially when we use small batch size (e.g., 1) where the GPU kernel execution time is not long enough to amortize the GPU kernel launch time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg1.png&quot; alt=&quot;End-to-end inference speedup of applying torch.compile and CUDA graph incrementally&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 16.&lt;/strong&gt; End-to-end inference speedup of applying torch.compile and CUDA graph incrementally. &lt;strong&gt;a)&lt;/strong&gt; “Inc. Decoding”: Apply torch.compile only to the text decoder &lt;strong&gt;b)&lt;/strong&gt; “Inc. Decoding w/ CUDA Graph”: Apply torch.compile + CUDA Graph to the text decoder &lt;strong&gt;c)&lt;/strong&gt; “+KV Cache Reordering”: Additionally apply torch.compile to KV cache reordering operation upon b) &lt;strong&gt;d)&lt;/strong&gt; “+Vocoder”: Additionally apply torch.compile to the vocoder upon c) &lt;strong&gt;e)&lt;/strong&gt; “+Vocoder w/ CUDA Graph”: Additionally apply torch.compile + CUDA Graph to the vocoder upon d).&lt;/p&gt;

&lt;p&gt;Figure 16 represents the cumulative effect of applying torch.compile with and without CUDA Graph to the modules. The results indicate a significant improvement in the end-to-end inference speedup, demonstrating the effectiveness of these techniques in optimizing the overall latency. As a result, we gain &lt;strong&gt;2.7x&lt;/strong&gt; end-to-end inference speedup for Seamless M4T-v2 with batch_size=1.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank the PyTorch team and Seamless team for their tremendous support with this work.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yejin Lee, Carole-Jean Wu, Christian Puhrsch, Joel Schlosser, Driss Guessous, Jeffrey Wan, Joe Isaacson, Can Balioglu, Juan Pino</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is the fourth part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. To skip to the code, check out our github (seamless_communication, fairseq2). We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In part two, we showed how to accelerate Llama-7B by almost 10x using only native PyTorch optimizations. In part three, we showed how to accelerate text-to-image diffusion models up to 3x using only native Pytorch optimizations.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate PyTorch Models Using Quantization Techniques with Intel Extension for PyTorch</title>
      <link href="https://pytorch.org/blog/accelerate-pytorch-models/" rel="alternate" type="text/html" title="Accelerate PyTorch Models Using Quantization Techniques with Intel Extension for PyTorch" />
      <published>2024-01-18T00:00:00-08:00</published>
      <updated>2024-01-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerate-pytorch-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerate-pytorch-models/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;PyTorch is a Python-based framework for developing deep learning models. It is one of the most popular industry-standard AI frameworks and is used for a wide variety of computer vision and natural language processing applications. PyTorch was developed by Meta and is now part of The Linux Foundation. Intel works with the open source PyTorch project to optimize the PyTorch framework for Intel® hardware. The newest optimizations and features are first released in Intel® Extension for PyTorch before upstreaming them into PyTorch. The Intel extension provides quantization features to deliver good accuracy results for large deep learning models.&lt;/p&gt;

&lt;p&gt;This article introduces quantization, types of quantization, and demonstrates a code sample on how to accelerate PyTorch-based models by applying Intel Extension for PyTorch quantization.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantization&quot;&gt;What Is Quantization?&lt;/h2&gt;

&lt;p&gt;Quantization is a systematic reduction of the precision of all or several layers within the model. This means a higher-precision type (like single precision floating-point (FP32) that is mostly used in deep learning) is converted into a lower-precision type, such as FP16 (16 bits) or int8 (8 bits).&lt;/p&gt;

&lt;p&gt;This helps to achieve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lower memory bandwidth&lt;/li&gt;
  &lt;li&gt;Lower storage&lt;/li&gt;
  &lt;li&gt;Higher performance with minimum to zero accuracy loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization is especially important with large models such as those based on the Transformer architecture (like BERT or GPT).&lt;/p&gt;

&lt;p&gt;There are two types of quantization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static: This quantizes the weights and activations of the model, and is used when memory bandwidth and compute savings are important.&lt;/li&gt;
  &lt;li&gt;Dynamic: The weights are quantized ahead of time, but the activations are dynamically quantized during inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-perform-static-quantization-and-dynamic-quantization&quot;&gt;How to Perform Static Quantization and Dynamic Quantization&lt;/h2&gt;

&lt;p&gt;The Intel extension extends PyTorch with up-to-date features and optimizations for an extra performance boost on Intel hardware.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;Installation Instructions for Intel Extension for PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The extension can be loaded as a Python module or linked as a C++ library. Python users can enable it dynamically by importing &lt;strong&gt;intel_extension_for_pytorch&lt;/strong&gt;. The extension provides built-in quantization to deliver good statistical accuracy for most popular deep learning workloads including convolutional neural networks (CNN), natural language processing (NLP), and recommendation models. The quantization functionality in the Intel extension currently supports post-training quantization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To quantize the existing FP32 model to an int8 model using static quantization:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare the quantization configuration. For default static quantization configuration, use &lt;strong&gt;ipex.quantization.default_static_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the model for calibration using the &lt;strong&gt;ipex.quantization.prepare&lt;/strong&gt; method.&lt;/li&gt;
  &lt;li&gt;Perform calibration against the dataset. This calibration is specific for static quantization as it needs the representative dataset to determine the optimal quantization parameters, so the user should provide data to the model in batches to calibrate it.&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to int8 using the &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method. This function converts the FP32 model to int8 based on the applied calibration and configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To quantize the existing FP32 model to an int8 model using dynamic quantization, which is similar to static quantization:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare the quantization configuration. For default dynamic quantization configuration, use &lt;strong&gt;ipex.quantization.default_dynamic_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the FP32 model by using the &lt;strong&gt;ipex.quantization.prepare&lt;/strong&gt; method. Provide the parameters, such as FP32 model to quantize, the prepared configuration, example inputs, and information.&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to int8 using the &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method. The input model is the model prepared in Step 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;For static quantization, the model is calibrated with the &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10 dataset&lt;/a&gt;. The CIFAR-10 is a subset of the 80 million &lt;a href=&quot;https://groups.csail.mit.edu/vision/TinyImages/&quot;&gt;tiny images dataset&lt;/a&gt; collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.&lt;/p&gt;

&lt;p&gt;This dataset contains 60,000 images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and track). Every class has exactly 6,000 images. All images are 32 x 32 pixels and are colored. Also, the classes are completely mutually exclusive, which means there is no overlapping between classes.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;code sample&lt;/a&gt; demonstrates how to quantize (using static and dynamic quantization) a ResNet*-50 model using Intel Extension for PyTorch. The following steps are implemented in the code sample:&lt;/p&gt;

&lt;h4 id=&quot;download-and-prepare-the-dataset&quot;&gt;Download and Prepare the Dataset&lt;/h4&gt;

&lt;p&gt;Here, we use the CIFAR-10 dataset available in torchvision.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To make data fit the model:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Transform the data.&lt;/li&gt;
  &lt;li&gt;Change the size of the images from 32 x 32 pixels to 224 x 224 pixels.&lt;/li&gt;
  &lt;li&gt;Convert them to tensors.&lt;/li&gt;
  &lt;li&gt;Normalize them.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare transformations of the dataset as shown:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;transform = torchvision.transforms.Compose([
torchvision.transforms.Resize((224, 224)),
torchvision.transforms.ToTensor(),
torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Initialize the dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_dataset = torchvision.datasets.CIFAR10(root=DATA, train=False, transform=transform, download=Ture)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;prepare-the-data-loader&quot;&gt;Prepare the Data Loader&lt;/h4&gt;

&lt;p&gt;To load a dataset for static quantization calibration in specific size batches, create the loader as shown:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;calibration_data_loader = torch.utils.data.DataLoader(
dataset=test_dataset,
batch_size=128
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;create-the-model&quot;&gt;Create the Model&lt;/h4&gt;

&lt;p&gt;Use the pretrained ResNet-50 model available in the Torchvision library with default weights. The prepared model is FP32.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_fp32 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;apply-static-quantization&quot;&gt;Apply Static Quantization&lt;/h4&gt;

&lt;p&gt;Create a &lt;strong&gt;staticQuantize&lt;/strong&gt; function that implements the steps described previously.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To perform static quantization, we need:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;FP32 model loaded earlier&lt;/li&gt;
  &lt;li&gt;Example data&lt;/li&gt;
  &lt;li&gt;Calibration dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare the quantization configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config_static = ipex.quantization.default_static_qconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code sample, we are using the default quantization configuration, but you can also define your own. \&lt;/p&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Prepare the model using the declared configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prepared_model_static = prepare(model_fp32,
qconfig_static,
example_inputs=data,
inplace=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;Calibrate the model with the calibration dataset. Feed the model with successive batches of data from the dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for batch_idx, (data, target) in enumerate(calibration_data_loader):
prepared_model_static(data)
if batch_idx % 10 == 0:
print(&quot;Batch %d/%d complete, continue ...&quot; %(batch_idx+1, len(calibration_data_loader)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;5&quot;&gt;
  &lt;li&gt;Convert the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;converted_model_static = convert(prepared_model_static)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;apply-dynamic-quantization&quot;&gt;Apply Dynamic Quantization&lt;/h4&gt;

&lt;p&gt;Create the &lt;strong&gt;dynamicQuantize&lt;/strong&gt; function similar to the &lt;strong&gt;staticQuantize&lt;/strong&gt; function.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To perform dynamic quantization, we only need:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;The FP32 model loaded earlier&lt;/li&gt;
  &lt;li&gt;Example data&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare the quantization configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qconfig_dynamic = ipex.quantization.default_dynamic_qconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Prepare the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prepared_model_dynamic = prepare(model_fp32,
qconfig_dynamic,
example_inputs=data,
inplace=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;Convert the model from FP32 to int8.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;converted_model_dynamic = convert(prepared_model_dynamic)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this way, two functions are created to take advantage of the optimizations that quantization offers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;DynamicQuantize&lt;/strong&gt; for dynamic quantization of models&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;StaticQuantize&lt;/strong&gt; for static model quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Get started with Intel Extension for PyTorch quantization today and use it to achieve better accuracy results for deep learning workloads. Additionally, &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html?cid=sem&amp;amp;source=sa360&amp;amp;campid=2023_q2_iags_us_iagsoapie_iagsoapiee_awa_text-link_exact_cd_dpd-oneapi-intel_neural_compressor_3500107853_google_div_oos_non-pbm_intel&amp;amp;ad_group=ai_model_compression_exact&amp;amp;intel_term=neural+compressor&amp;amp;sa360id=43700076378213630&amp;amp;gclid=CjwKCAjw-IWkBhBTEiwA2exyO1pBoV7k3j16OANdyEOMVYDUvy4MZK3WQX6zzhymBxz7Pikqq0ndwBoCHvUQAvD_BwE&amp;amp;gclsrc=aw.ds#gs.2t5hw6&quot;&gt;Intel® Neural Compressor&lt;/a&gt; provides &lt;a href=&quot;https://intel.github.io/neural-compressor/latest/docs/source/quantization.html&quot;&gt;quantization&lt;/a&gt; to improve the speed of inference.&lt;/p&gt;

&lt;p&gt;Check out and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI and machine learning framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow.&lt;/p&gt;

&lt;p&gt;Learn about the unified, open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; programming model that forms the foundation of Intel’s &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI Software Portfolio&lt;/a&gt; to help you prepare, build, deploy, and scale your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the 4th gen Intel® Xeon® Scalable processors, visit the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel® AI platform overview&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/ai-solution-brief.html&quot;&gt;Accelerate AI Workloads with Intel® Advanced Matrix Extensions (Intel® AMX)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI and Machine Learning Development Tools and Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html#gs.2t503z&quot;&gt;AI Frameworks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/computer-vision.html&quot;&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/artificial-intelligence/hardware.html&quot;&gt;Intel Hardware for AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html?cid=sem&amp;amp;source=sa360&amp;amp;campid=2023_q2_iags_us_iagsoapie_iagsoapiee_awa_text-link_exact_cd_dpd-oneapi-intel_neural_compressor_3500107853_google_div_oos_non-pbm_intel&amp;amp;ad_group=ai_model_compression_exact&amp;amp;intel_term=neural+compressor&amp;amp;sa360id=43700076378213630&amp;amp;gclid=CjwKCAjw-IWkBhBTEiwA2exyO1pBoV7k3j16OANdyEOMVYDUvy4MZK3WQX6zzhymBxz7Pikqq0ndwBoCHvUQAvD_BwE&amp;amp;gclsrc=aw.ds#gs.2t5hw6&quot;&gt;Intel Neural Compressor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html#gs.h7kofh&quot;&gt;oneAPI Unified Programming Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;PyTorch Foundation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;PyTorch Optimizations from Intel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;PyTorch Quantization Code Sample&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://intel.github.io/neural-compressor/latest/docs/source/quantization.html&quot;&gt;Quantization Using Intel Neural Compressor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Triton Dequantization Kernels for GPTQ</title>
      <link href="https://pytorch.org/blog/accelerating-triton/" rel="alternate" type="text/html" title="Accelerating Triton Dequantization Kernels for GPTQ" />
      <published>2024-01-16T00:00:00-08:00</published>
      <updated>2024-01-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-triton</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-triton/">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;Leveraging a first principles approach, we showcase a step by step process undertaken to accelerate the current Triton GPTQ kernels by 3x (core GPTQ) and 6x (AutoGPTQ).  Example: 275us to 47us on a typical Llama style inference input.  The goal is to provide a helpful template for accelerating any given Triton kernel. We provide a background on Triton and GPTQ quantization and dequantization process, showcase the impact of coalesced memory access to improve shared and global memory throughput, highlight changes made to reduce warp stalling to improve total throughput, and an overview on integrating Triton kernels into PyTorch code.  Longer term, we hope to surpass the existing CUDA native GPTQ kernel with our Triton kernel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg1.png&quot; alt=&quot;Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg2.png&quot; alt=&quot;Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg3.png&quot; alt=&quot;Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100.&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block; margin-top: 60px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100.  More to come…&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;10-introduction-to-triton&quot;&gt;1.0 Introduction to Triton&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://openai.com/research/triton&quot;&gt;Triton framework&lt;/a&gt; provides a hardware agnostic way of programming and targeting GPUs, currently supporting both NVIDIA and AMD, with support for additional hardware vendors in progress. Triton is now a mainstay for PyTorch 2.0 as torch.compile decomposes eager PyTorch and re-assembles it into a high percentage of Triton kernels with PyTorch connecting code.&lt;/p&gt;

&lt;p&gt;As Triton becomes more widely adopted, it will be essential that programmers understand how to systematically step through the Triton stack (from the high level Python down to the low-level SASS) to address performance bottlenecks in order to optimize GPU efficiency for algorithms that go beyond torch.compile generated kernels.&lt;/p&gt;

&lt;p&gt;In this post, we will introduce some core concepts of the Triton programming language, how to identify common performance limiters in GPU kernels, and in parallel, tune a quantization kernel used in AutoGPTQ that can be used for high throughput inference applications.&lt;/p&gt;

&lt;h3 id=&quot;intro-to-gptq-quantization-and-dequantization&quot;&gt;Intro to GPTQ Quantization and Dequantization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot;&gt;GPTQ&lt;/a&gt;  is a quantization algorithm that is able to compress ultra-large (175B+) LLMs efficiently to int4 bit representation, via approximate second order information (Hessian inverse).  &lt;a href=&quot;https://github.com/PanQiWei/AutoGPTQ&quot;&gt;AutoGPTQ&lt;/a&gt; is a framework built on GPTQ, allowing for rapid dequantization and inference/serving of LLMs that have been quantized with GPTQ.&lt;/p&gt;

&lt;p&gt;As part of the AutoGPTQ stack, they provide a Triton GPTQ kernel to handle the dequantization of a model for inference.&lt;/p&gt;

&lt;p&gt;The basic process for INT quantization is shown below and involves determining the scale and zero point, and then computing the quantized 4bit Weight using the Scale and Zero point:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg4.jpg&quot; alt=&quot;The basic process for INT quantization&quot; style=&quot;width:100%;max-width:400px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We thus store the 4 Bit weights along with the meta information of Scale and ZeroPoint for each group of weights.&lt;/p&gt;

&lt;p&gt;To ‘dequant’ these weights, we do the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg5.png&quot; alt=&quot;To ‘dequant’ these weights&quot; style=&quot;width:100%;max-width:400px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then proceed to &lt;strong&gt;Matrix Multiply&lt;/strong&gt; the dequantized weights with the dense input feature matrix for this linear layer.&lt;/p&gt;

&lt;h2 id=&quot;20-identify-the-bottlenecks---optimizing-matrix-multiplication&quot;&gt;2.0 Identify the Bottlenecks - Optimizing Matrix Multiplication&lt;/h2&gt;

&lt;p&gt;As it turns out, making a fast matrix multiplication kernel is not trivial. A naively implemented matrix multiply will rarely reach peak throughput performance on highly parallel machines like GPUs. So – we need to tackle our compute and memory subsystems in our GPU in an hierarchical fashion to make sure we are maximally utilizing each resource.&lt;/p&gt;

&lt;p&gt;We start our optimization process, by running the unoptimized Triton Kernel, through the Nvidia Nsight Compute tool and taking a note of some important metrics and warnings:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg6.png&quot; alt=&quot;some important metrics and warnings&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig xy (todo)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg7.png&quot; alt=&quot;some important metrics and warnings&quot; style=&quot;width:100%;max-width:300px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We notice first that both compute and memory throughput are low, 7.40% and 21.19% respectively (fig xy) . Knowing that for typical inference matrix problem sizes, we are in the memory bound regime, we will attempt to optimize the kernel by applying code changes that target the memory subsystem of our A100 GPU.&lt;/p&gt;

&lt;p&gt;The three topics this post will cover are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;L2 Optimization&lt;/li&gt;
  &lt;li&gt;Vectorized Load&lt;/li&gt;
  &lt;li&gt;Warp Stalling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s walk through each topic, make the appropriate changes, and see its corresponding impact on our Triton Kernel. This Triton kernel is a fused dequantization kernel that dequantizes a packed int32 weight (we will refer to this as the B Matrix) Tensor into int4 weights, performs matrix multiplication with the activation tensor (refer to as the A matrix) in FP16 mode, and then  stores the results back to a matrix C.&lt;/p&gt;

&lt;p&gt;The above is referred to as W4A16 quantization. Keep in mind that the process we describe can and should be used for the development of any GPU kernel, as these are common bottlenecks in any unoptimized kernel.&lt;/p&gt;

&lt;h2 id=&quot;30-l2-optimization&quot;&gt;3.0 L2 Optimization&lt;/h2&gt;

&lt;p&gt;This optimization already exists in the AutoGPTQ kernel, but we’d like to dedicate a section to this to help readers better understand how mapping and execution order of thread blocks is handled in Triton.  Thus, we will step through a naive mapping and then a more optimal mapping to see its corresponding impact.&lt;/p&gt;

&lt;p&gt;Let’s build up our kernel naively, starting with a “linear” load from global memory and then compare it to a more optimized “swizzled” load. Linear vs Swizzled determines the execution order of our grid of work on the GPU. Let’s take a look at the hints that the &lt;a href=&quot;https://developer.nvidia.com/nsight-compute&quot;&gt;Nvidia Nsight Compute Tool&lt;/a&gt; provides regarding our kernels shared memory access pattern in the naive case:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg8.jpg&quot; alt=&quot;the hints from the Nvidia Nsight Compute Tool&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To tackle this issue we can use an approach referred to as “tile-swizzling.”  The idea of this method is to launch our thread blocks in a more L2 cache friendly order.&lt;/p&gt;

&lt;p&gt;Let’s take a step back and familiarize ourselves with some Triton semantics and make a simple CUDA analogy to understand the concept better. Triton kernels launch “programs”. These so-called programs map to the concept of a Thread Block in CUDA and it is the basic unit of parallelism in a Triton Kernel. Every program has with it associated a “pid” and all the threads in a program are guaranteed to be executing the same instruction.&lt;/p&gt;

&lt;p&gt;The Triton programs will be distributed onto your SMs in a naive-way if you do a simple linear mapping of “pid” to a 2D grid location of your output matrix C.&lt;/p&gt;

&lt;p&gt;This 2D grid location is determined by pid_m and pid_n in Triton. We would like to exploit data and cache locality in the L2 cache of our GPU, when we distribute our grid of work. To do this in Triton we can make the following changes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg9.png&quot; alt=&quot;To do this in Triton&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The code highlighted in red would be the naive “linear” tile ordering, and the code highlighted in green is the “swizzled” tile ordering. This type of launch promotes a sense of locality. Here is a visual to help understand this better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg10.jpg&quot; alt=&quot;a sense of locality&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After incorporating this change, the profiler no longer complains about uncoalesced memory accesses. Let’s take a look at how our memory throughput has changed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg11.jpg&quot; alt=&quot;how our memory throughput has changed&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This change was tested on a simple load store kernel.  Looking at the GPU speed of light statistics section in the profiler we also see a 112.07% increase in the memory throughput of the simple load kernel, which is what we were after with this optimization.  Again, this optimization already exists in the AutoGPTQ kernel, but is the boilerplate logic that every Triton Kernel programmer will have to write in the beginning of their kernel, before any of the exciting dequantization or matrix multiply logic. It is thus important to understand that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;This mapping is not unique&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Triton does not automatically handle this kind of optimization for the programmer, and careful thought must be taken to ensure your kernel is optimally handling shared memory accesses&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These are not obvious for those new to Triton, as much of the shared memory access optimization is handled by the Triton compiler. However, in the cases where these are not handled by the compiler, it is important to be able to understand what tools and methods are available to us to be able to influence memory behavior.&lt;/p&gt;

&lt;h2 id=&quot;40-vectorized-load&quot;&gt;4.0 Vectorized Load&lt;/h2&gt;

&lt;p&gt;Now, back to the original complaints of our unoptimized kernel. We want to optimize the global memory access pattern of our kernel. From the details page of the Nvidia Nsight compute tool, we see the following note, where the profiler is complaining about uncoalesced global memory accesses.&lt;/p&gt;

&lt;p&gt;Let’s dig deeper and take a look at the SASS (Assembly) Code load for an unoptimized memory read:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg12.png&quot; alt=&quot;an unoptimized memory read&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This load operation resulted in 32 global load operations that are 16 bit wide. This is not optimal.&lt;/p&gt;

&lt;p&gt;We would like to do our global memory loads in a vectorized way so that it results in the least amount of load instructions. To combat this we can give the Triton Compiler some help.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg13.png&quot; alt=&quot;code block&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The green highlighted lines above act as a compiler hint. It tells the compiler that these elements are contiguous in memory and that this load operation can be coalesced.&lt;/p&gt;

&lt;p&gt;Let’s see the effect in assembly after adding these lines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg14.png&quot; alt=&quot;the effect in assembly after adding these lines&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The load is now performed in 4 global load operations that are each 128 bit wide, instead of 32 16 bit global load operations. This means 28 fewer memory fetch instructions, and importantly a coalesced memory access. This can be seen from the fact that a single thread is not accessing consecutive memory addresses, which without the compiler hint, was the behavior.&lt;/p&gt;

&lt;p&gt;The resulting effect is 73x speedup in an isolated load operation, and after incorporating it in the full dequantization kernel we were able to see another 6% speedup. Another step in the right direction!&lt;/p&gt;

&lt;h2 id=&quot;50-warp-stalling&quot;&gt;5.0 Warp Stalling&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg15.png&quot; alt=&quot;performance limiter, warp stalling&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now putting all the changes back into our full dequantization kernel, we see the following performance limiter, warp stalling.&lt;/p&gt;

&lt;p&gt;These warp stalls are mostly caused by ‘Long Scoreboard’ stalls, accounting for 92.63% of the total.&lt;/p&gt;

&lt;p&gt;At a high level, &lt;a href=&quot;https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference&quot;&gt;long scoreboard stalls&lt;/a&gt; happen when a warp requires data that may not be ready yet in order to be in the “issued” state. In other words GPUs are throughput machines, and we need to hide the latency of load instructions with compute instructions. By loading more data and rearranging where the load instructions are in the script we can take care of this problem.&lt;/p&gt;

&lt;p&gt;In an ideal scenario, each warp scheduler would be able to issue 1 instruction every clock cycle. Note - Every SM on an A100 GPU has 4 warp schedulers.&lt;/p&gt;

&lt;p&gt;However – our kernel has bottlenecks and is spending 4.4 cycles in the stall state with the block size that AutoGPTQ Triton kernel deems as optimal given the presets it has.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we improve this?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We want to be able to increase our memory throughput so that we can increase the chance that when a warp issues an instruction, we won’t be waiting for loads to be stored in SRAM so that they can be used for computation. We played around with multiple parameters (such as number of pipeline stages, and number of warps) and the one that had the biggest impact was increasing the block size by a factor of 2 in the k dimension.&lt;/p&gt;

&lt;p&gt;These changes yield an immediate impact on both compute and memory throughput.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg16.jpg&quot; alt=&quot;an immediate impact on both compute and memory throughput&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also see the long scoreboard wait time at the step where we shift and scale the quantized weights drop significantly, which is what we identified as the original bottleneck in the source code. While there are still stalls at this line, only 68% of them are caused by long scoreboard stalls, compared to the original 92%. Ideally, we do not observe ANY stalls, so there is still work to be done here, but a reduction in the amount of stalls caused by long scoreboard tells us that our data is at this point ready to be used (in L1TEX) memory by an instruction that a warp wants to execute, at a higher frequency then the original kernel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg17.png&quot; alt=&quot;1.4x speedup in the execution time of our kernel&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The corresponding impact is a 1.4x speedup in the execution time of our kernel.&lt;/p&gt;

&lt;h2 id=&quot;60-results&quot;&gt;6.0 Results&lt;/h2&gt;

&lt;p&gt;By tackling all these problem areas methodically our resulting kernel is 6x faster on the Nvidia A100 GPU than if you were to use the Triton kernel AutoGPTQ provides out-of-the-box.&lt;/p&gt;

&lt;p&gt;Taking a relevant Llama inference sample data point, the &lt;a href=&quot;https://github.com/foundation-model-stack/foundation-model-stack/tree/triton/triton/kernels&quot;&gt;Triton kernel we’ve developed &lt;/a&gt;takes 47us to perform dequantization and matrix multiplication compared to the 275us taken by the AutoGPTQ kernel for the same matrix size.&lt;/p&gt;

&lt;p&gt;By replicating this step-by-step approach it should be possible to get similar speedups in other kernels, and help build understanding on common GPU bottlenecks and how to tackle them.&lt;/p&gt;

&lt;p&gt;It is important to note that while strides have been made in improving the performance of the AutoGPTQ Triton Kernel, we have still not closed the gap on the current exllamaV2 CUDA kernels found in AutoGPTQ.&lt;/p&gt;

&lt;p&gt;More research is required to understand how we can further optimize this kernel to match equivalent custom CUDA kernel performance.&lt;/p&gt;

&lt;h2 id=&quot;summary-and-future-work&quot;&gt;Summary and Future work&lt;/h2&gt;

&lt;p&gt;Triton extends PyTorch by allowing low level GPU optimizations to be done at a higher level of abstraction than CUDA programming, with the net result that adding optimized Triton kernels can help PyTorch models run faster.&lt;/p&gt;

&lt;p&gt;Our goal in this post was to show an example of accelerating the GPTQ dequant kernel and provide a template workflow for how the accelerations were achieved.&lt;/p&gt;

&lt;p&gt;For future work, SplitK work decomposition for the matrix multiplication is a potential speed up we’ll investigate.&lt;/p&gt;

&lt;h2 id=&quot;integrating-custom-triton-kernels-into-pytorch&quot;&gt;Integrating custom Triton Kernels into PyTorch&lt;/h2&gt;

&lt;p&gt;Given the acceleration shown above, a common question is how to actually use a custom kernel in a given PyTorch codebase.&lt;/p&gt;

&lt;p&gt;A triton kernel will contain at least two parts - the actual Triton kernel code which will be compiled by the Triton compiler:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg18.png&quot; alt=&quot;the actual Triton kernel code which will be compiled by the Triton compiler&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Along with the actual kernel code is a python wrapper, that may or may not subclass the PyTorch autograd class - depending if it’s going to support a backwards pass (i.e. for training purposes or only for inference purposes).&lt;/p&gt;

&lt;p&gt;You simply import the python class into your PyTorch code where you want to use it much like any other Python / PyTorch function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg19.png&quot; alt=&quot;import the python class into your PyTorch code&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, simply importing and then using ‘fast_qlinear’ would invoke the underlying Triton kernel with the speed-ups we’ve shown above applied to your PyTorch model.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to Jamie Yang and Hao Yu from IBM Research for their technical guidance in the collection of these results.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Less Wright, Adnan Hoque (IBM)</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem</title>
      <link href="https://pytorch.org/blog/finetune-llms/" rel="alternate" type="text/html" title="Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem" />
      <published>2024-01-10T00:00:00-08:00</published>
      <updated>2024-01-10T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/finetune-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/finetune-llms/">&lt;p&gt;We demonstrate how to finetune a 7B parameter model on a typical consumer GPU (NVIDIA T4 16GB) with LoRA and tools from the PyTorch and Hugging Face ecosystem with complete reproducible Google Colab notebook.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Large Language Models (LLMs) have shown impressive capabilities in industrial applications. Often, developers seek to tailor these LLMs for specific use-cases and applications to fine-tune them for better performance. However, LLMs are large by design and require a large number of GPUs to be fine-tuned.&lt;/p&gt;

&lt;p&gt;Let’s focus on a specific example by trying to fine-tune a Llama model on a free-tier Google Colab instance (1x NVIDIA T4 16GB). Llama-2 7B has 7 billion parameters, with a total of 28GB in case the model is loaded in full-precision. Given our GPU memory constraint (16GB), the model cannot even be loaded, much less trained on our GPU. This memory requirement can be divided by two with negligible performance degradation. You can read more about running models in half-precision and mixed precision for training &lt;a href=&quot;https://huggingface.co/docs/transformers/v4.15.0/performance#forward-vs-backward-execution-speed&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-makes-our-llama-fine-tuning-expensive&quot;&gt;What makes our Llama fine-tuning expensive?&lt;/h2&gt;

&lt;p&gt;In the case of full fine-tuning with Adam optimizer using a half-precision model and mixed-precision mode, we need to allocate per parameter:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2 bytes for the weight&lt;/li&gt;
  &lt;li&gt;2 bytes for the gradient&lt;/li&gt;
  &lt;li&gt;4 + 8 bytes for the Adam optimizer states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;→ With a total of 16 bytes per trainable parameter, this makes a total of &lt;strong&gt;112GB&lt;/strong&gt; (excluding the intermediate hidden states). Given that the largest GPU available today can have up to 80GB GPU VRAM, it makes fine-tuning challenging and less accessible to everyone. To bridge this gap, Parameter Efficient Fine-Tuning (PEFT) methods are largely adopted today by the community.&lt;/p&gt;

&lt;h2 id=&quot;parameter-efficient-fine-tuning-peft-methods&quot;&gt;Parameter Efficient Fine-Tuning (PEFT) methods&lt;/h2&gt;

&lt;p&gt;PEFT methods aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning.&lt;/p&gt;

&lt;p&gt;They can be differentiated by their conceptual framework: does the method fine-tune a subset of existing parameters, introduce new parameters, introduce trainable prompts, etc.? We recommend readers to have a look at the paper shared below that extensively compares existing PEFT methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg1.png&quot; alt=&quot;Venn diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image taken from the paper: &lt;a href=&quot;https://arxiv.org/pdf/2303.15647.pdf&quot;&gt;Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For this blog post, we will focus on Low-Rank Adaption for Large Language Models (LoRA), as it is one of the most adopted PEFT methods by the community.&lt;/p&gt;

&lt;h2 id=&quot;low-rank-adaptation-for-large-language-models-lora-using--peft&quot;&gt;Low-Rank Adaptation for Large Language Models (LoRA) using 🤗 PEFT&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;The LoRA method&lt;/a&gt; by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model(that we will denote by &lt;em&gt;base model&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices (called update matrices). These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.&lt;/p&gt;

&lt;p&gt;This approach has several advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.&lt;/li&gt;
  &lt;li&gt;The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.&lt;/li&gt;
  &lt;li&gt;LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.&lt;/li&gt;
  &lt;li&gt;The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.&lt;/li&gt;
  &lt;li&gt;LoRA does not add any inference latency when adapter weights are merged with the base model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg2.gif&quot; alt=&quot;Animated diagram that show how LoRA works in practice&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Animated diagram that show how LoRA works in practice - original content adapter from the figure 1 of LoRA &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;original paper&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Below is a code snippet showing how to train LoRA model using  Hugging Face PEFT library:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg3.png&quot; alt=&quot;code snippet showing how to train LoRA model using  Hugging Face PEFT library&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-base-model-can-be-in-any-dtype-leveraging-sota-llm-quantization-and-loading-the-base-model-in-4-bit-precision&quot;&gt;The base model can be in any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtype&lt;/code&gt;: leveraging SOTA LLM quantization and loading the base model in 4-bit precision&lt;/h2&gt;

&lt;p&gt;According to the LoRA formulation, the base model can be compressed in any data type (&lt;em&gt;‘dtype’&lt;/em&gt;) as long as the hidden states from the base model are in the same dtype as the output hidden states from the LoRA matrices.&lt;/p&gt;

&lt;p&gt;Compressing and quantizing large language models has recently become an exciting topic as SOTA models become larger and more difficult to serve and use for end users. Many people in the community proposed various approaches for effectively compressing LLMs with minimal performance degradation.&lt;/p&gt;

&lt;p&gt;This is where the &lt;a href=&quot;https://github.com/TimDettmers/bitsandbytes&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt;&lt;/a&gt; library comes in. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public.&lt;/p&gt;

&lt;h2 id=&quot;qlora-one-of-the-core-contributions-of-bitsandbytes-towards-the-democratization-of-ai&quot;&gt;QLoRA: One of the core contributions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt; towards the democratization of AI&lt;/h2&gt;

&lt;p&gt;Quantization of LLMs has largely focused on quantization for inference, but the &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot;&gt;QLoRA&lt;/a&gt; (Quantized model weights + Low-Rank Adapters) paper showed the breakthrough utility of using backpropagation through frozen, quantized weights at large model scales.&lt;/p&gt;

&lt;p&gt;With QLoRA we are matching 16-bit fine-tuning performance across all scales and models, while reducing fine-tuning memory footprint by more than 90%— thereby allowing fine-tuning of SOTA models on consumer-grade hardware.&lt;/p&gt;

&lt;p&gt;In this approach, LoRA is pivotal both for purposes of fine-tuning &lt;span style=&quot;text-decoration:underline;&quot;&gt;and&lt;/span&gt; the correction of minimal, residual quantization errors. Due to the significantly reduced size of the quantized model it becomes possible to generously place low-rank adaptors at every network layer, which together still make up just 0.2% of the original model’s weight memory footprint. Through such usage of LoRA, we achieve performance that has been shown to be equivalent to 16-bit full model finetuning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg4.png&quot; alt=&quot;System diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to generous use of LoRA, to achieve high-fidelity fine-tuning of 4-bit models, QLoRA uses 3 further algorithmic tricks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;4-bit NormalFloat (NF4) quantization, a custom data type exploiting the property of the normal distribution of model weights and distributing an equal number of weights (per block) to each quantization bin—thereby enhancing information density.&lt;/li&gt;
  &lt;li&gt;Double Quantization, quantization of the quantization constants (further savings).&lt;/li&gt;
  &lt;li&gt;Paged Optimizers, preventing memory spikes during gradient checkpointing from causing out-of-memory errors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An interesting aspect is the dequantization of 4-bit weights in the GPU cache, with matrix multiplication performed as a 16-bit floating point operation. In other words, we use a &lt;em&gt;low-precision storage data type&lt;/em&gt; (in our case 4-bit, but in principle interchangeable) and one normal precision &lt;em&gt;computation data type&lt;/em&gt;. This is important because the latter defaults to 32-bit for hardware compatibility and numerical stability reasons, &lt;span style=&quot;text-decoration:underline;&quot;&gt;but should be set to the optimal BFloat16 for newer hardware supporting it&lt;/span&gt; to achieve the best performance.&lt;/p&gt;

&lt;p&gt;To conclude, through combining these refinements to the quantization process and generous use of LoRA, we compress the model by over 90% and retain full model performance without the usual quantization degradation, while also retaining full fine-tuning capabilities with 16-bit LoRA adapters at every layer.&lt;/p&gt;

&lt;h2 id=&quot;using-qlora-in-practice&quot;&gt;Using QLoRA in practice&lt;/h2&gt;

&lt;p&gt;These SOTA quantization methods come packaged in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt; library and are conveniently integrated with HuggingFace 🤗 Transformers. For instance, to use LLM.int8 and QLoRA algorithms, respectively, simply pass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_in_8bit&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_in_4bit&lt;/code&gt; to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_pretrained&lt;/code&gt; method.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = &quot;facebook/opt-125m&quot;
# For LLM.int8()
# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)

# For QLoRA
model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about quantization features in this specific section of the documentation: &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/quantization&quot;&gt;https://huggingface.co/docs/transformers/main_classes/quantization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When using QLoRA with Adam optimizer using a 4-bit base model and mixed-precision mode, we need to allocate per parameter:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;~0.5 bytes for the weight&lt;/li&gt;
  &lt;li&gt;2 bytes for the gradient&lt;/li&gt;
  &lt;li&gt;4 + 8 bytes for the Adam optimizer states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giving a total of 14 bytes per trainable parameter times 0.0029 as we end up having only 0.29% trainable parameters with QLoRA, this makes the QLoRA training setup cost around 4.5GB to fit, but requires in practice ~7-10GB to include intermediate hidden states which are always in half-precision (7 GB for a sequence length of 512 and 10GB for a sequence length of 1024) in the Google Colab demo shared in the next section.&lt;/p&gt;

&lt;p&gt;Below is the code snippet showing how to train QLoRA model using Hugging Face PEFT:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg5.png&quot; alt=&quot;code snippet showing how to train QLoRA model using Hugging Face PEFT&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-trl-for-llm-training&quot;&gt;Using TRL for LLM training&lt;/h2&gt;

&lt;p&gt;Models such as ChatGPT, GPT-4, and Claude are powerful language models that have been fine-tuned using a method called Reinforcement Learning from Human Feedback (RLHF) to be better aligned with how we expect them to behave and would like to use them. The finetuning goes through 3 steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Fine-tuning (SFT)&lt;/li&gt;
  &lt;li&gt;Reward / preference modeling (RM)&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg6.png&quot; alt=&quot;Process diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;From InstructGPT paper: Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155 (2022).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here, we will only focus on the supervised fine-tuning step. We train the model on the new dataset following a process similar to that of pretraining. The objective is to predict the next token (causal language modeling). Multiple techniques can be applied to make the training more efficient:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Packing&lt;/strong&gt;: Instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with an End-Of-Sentence (EOS) token in between and cut chunks of the context size to fill the batch without any padding. This approach significantly improves training efficiency as each token processed by the model contributes to training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg7.png&quot; alt=&quot;Sample diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Train on completion only&lt;/strong&gt;: We want the model to be able to understand the prompt and generate an answer/. Instead of training the model on the whole input (prompt + answer), the training will be more efficient if we only train the model on completion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can perform supervised fine-tuning with these techniques using SFTTrainer:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=train_dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=1024,
    packing=True,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since SFTTrainer back-end is powered by 🤗&lt;a href=&quot;https://github.com/huggingface/accelerate&quot;&gt;accelerate&lt;/a&gt;, you can easily adapt the training to your hardware setup in one line of code!&lt;/p&gt;

&lt;p&gt;For example, with you have 2 GPUs, you can perform Distributed Data Parallel training with using the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;accelerate launch --num_processes=2 training_llama_script.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;putting-all-the-pieces-together&quot;&gt;Putting all the pieces together&lt;/h2&gt;

&lt;p&gt;We made a complete reproducible Google Colab notebook that you can check through&lt;a href=&quot;https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing&quot;&gt; this link&lt;/a&gt;. We use all the components shared in the sections above and fine-tune a llama-7b model on UltraChat dataset using QLoRA. As it can be observed through the screenshot below, when using a sequence length of 1024 and a batch size od 4,  the memory usage remains very  low (around 10GB).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg8.png&quot; alt=&quot;Memory usage diagram&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Younes Belkada, Marc Sun, Titus von Köller, Sourab Mangrulkar, Benjamin Bossan, Lysandre Debut, Steven Liu</name>
        
        
      </author>

      

      

      
        <summary type="html">We demonstrate how to finetune a 7B parameter model on a typical consumer GPU (NVIDIA T4 16GB) with LoRA and tools from the PyTorch and Hugging Face ecosystem with complete reproducible Google Colab notebook.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate AI models on GPU using Amazon SageMaker multi-model endpoints with TorchServe, saving up to 75% on inference costs</title>
      <link href="https://pytorch.org/blog/amazon-sagemaker-w-torchserve/" rel="alternate" type="text/html" title="Accelerate AI models on GPU using Amazon SageMaker multi-model endpoints with TorchServe, saving up to 75% on inference costs" />
      <published>2024-01-09T00:00:00-08:00</published>
      <updated>2024-01-09T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/amazon-sagemaker-w-torchserve</id>
      <content type="html" xml:base="https://pytorch.org/blog/amazon-sagemaker-w-torchserve/">&lt;p&gt;Multi-model endpoints (MMEs) are a powerful feature of &lt;a href=&quot;https://aws.amazon.com/sagemaker/&quot;&gt;Amazon SageMaker&lt;/a&gt; designed to simplify the deployment and operation of machine learning (ML) models. With MMEs, you can host multiple models on a single serving container and host all the models behind a single endpoint. The SageMaker platform automatically manages the loading and unloading of models and scales resources based on traffic patterns, reducing the operational burden of managing a large quantity of models. This feature is particularly beneficial for deep learning and generative AI models that require accelerated compute. The cost savings achieved through resource sharing and simplified model management makes SageMaker MMEs an excellent choice for you to host models at scale on AWS.&lt;/p&gt;

&lt;p&gt;Recently, generative AI applications have captured widespread attention and imagination. Customers want to deploy generative AI models on GPUs but at the same time are conscious of costs. SageMaker MMEs support GPU instances and is a great option for these types of applications. Today, we are excited to announce TorchServe support for SageMaker MMEs. This new model server support gives you the advantage of all the benefits of MMEs while still using the serving stack that TorchServe customers are most familiar with. In this post, we demonstrate how to host generative AI models, such as Stable Diffusion and Segment Anything Model, on SageMaker MMEs using TorchServe and build a language-guided editing solution that can help artists and content creators develop and iterate their artwork faster.&lt;/p&gt;

&lt;h2 id=&quot;solution-overview&quot;&gt;Solution overview&lt;/h2&gt;

&lt;p&gt;Language-guided editing is a common cross-industry generative AI use case. It can help artists and content creators work more efficiently to meet content demand by automating repetitive tasks, optimizing campaigns, and providing a hyper-personalized experience for the end customer. Businesses can benefit from increased content output, cost savings, improved personalization, and enhanced customer experience. In this post, we demonstrate how you can build language-assisted editing features using MME TorchServe that allow you to erase any unwanted object from an image and modify or replace any object in an image by supplying a text instruction.&lt;/p&gt;

&lt;p&gt;The user experience flow for each use case is as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To remove an unwanted object, the select the object from the image to highlight it. This action sends the pixel coordinates and the original image to a generative AI model, which generates a segmentation mask for the object. After confirming the correct object selection, you can send the original and mask images to a second model for removal. The detailed illustration of this user flow is demonstrated below.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;img alt=&quot;Dog on a bench with mouse pointer clicking the dog&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg1.png&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;Dog on a bench highlighted&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg2.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;A bench without the dog&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg3.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Select an object (“dog”) from the image
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Confirm the correct object is highlighted
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Erase the object from the image
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;To modify or replace an object, the select and highlight the desired object, following the same process as described above. Once you confirm the correct object selection, you can modify the object by supplying the original image, the mask, and a text prompt. The model will then change the highlighted object based on the provided instructions. A detailed illustration of this second user flow is as follows.&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;img alt=&quot;A vase with a cactus and mouse pointer&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg4.png&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;A vase highlighted&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg5.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;A rounded vase with a cactus&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg6.jpg&quot; style=&quot;width:100%; max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Select an object (“vase”) from the image
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Confirm the correct object is highlighted
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Provide a text prompt (“futuristic vase”) to modify the object
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To power this solution, we use three generative AI models: Segment Anything Model (SAM), Large Mask Inpainting Model (LaMa), and Stable Diffusion Inpaint (SD). Here are how these models been utilized in the user experience workflow:&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;To remove an unwanted object&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;To modify or replace an object&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;img alt=&quot;flow diagram&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg7.png&quot; style=&quot;width:100%;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;flow diagram&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg8.png&quot; style=&quot;width:100%;&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;ol&gt;
  &lt;li&gt;Segment Anything Model (SAM) is used to generate a segment mask of the object of interest. Developed by Meta Research, SAM is an open-source model that can segment any object in an image. This model has been trained on a massive dataset known as SA-1B, which comprises over 11 million images and 1.1 billion segmentation masks. For more information on SAM, refer to their &lt;a href=&quot;https://advimman.github.io/lama-project/&quot;&gt;website&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2109.07161&quot;&gt;research paper&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;LaMa is used to remove any undesired objects from an image. LaMa is a Generative Adversarial Network (GAN) model specializes in fill missing parts of images using irregular masks. The model architecture incorporates image-wide global context and a single-step architecture that uses Fourier convolutions, enabling it to achieve state-of-the-art results at a faster speed. For more details on LaMa, visit their &lt;a href=&quot;https://advimman.github.io/lama-project/&quot;&gt;website&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2109.07161&quot;&gt;research paper&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;SD 2 inpaint model from Stability AI is used to modify or replace objects in an image. This model allows us to edit the object in the mask area by providing a text prompt. The inpaint model is based on the text-to-image SD model, which can create high-quality images with a simple text prompt. It provides additional arguments such as original and mask images, allowing for quick modification and restoration of existing content. To learn more about Stable Diffusion models on AWS, refer to &lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/create-high-quality-images-with-stable-diffusion-models-and-deploy-them-cost-efficiently-with-amazon-sagemaker/&quot;&gt;Create high-quality images with Stable Diffusion models and deploy them cost-efficiently with Amazon SageMaker.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All three models are hosted on SageMaker MMEs, which reduces the operational burden from managing multiple endpoints. In addition to that, using MME eliminates concerns about certain models being underutilized because resources are shared. You can observe the benefit from improved instance saturation, which ultimately leads to cost savings. The following architecture diagram illustrates how all three models are served using SageMaker MMEs with TorchServe.&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;flow diagram&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg9.jpg&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We have published the code to implement this solution architecture in our &lt;a href=&quot;https://github.com/lxning/amazon-sagemaker-examples/tree/feat/torchserve-mme-gpu/inference/torchserve/mme-gpu&quot;&gt;GitHub repository&lt;/a&gt;. To follow along with the rest of the post, use the notebook file. It is recommended to run this example on a SageMaker notebook instance using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda_python3&lt;/code&gt; (Python 3.10.10) kernel.&lt;/p&gt;

&lt;h2 id=&quot;extend-the-torchserve-container&quot;&gt;Extend the TorchServe container&lt;/h2&gt;

&lt;p&gt;The first step is to prepare the model hosting container. SageMaker provides a managed PyTorch Deep Learning Container (DLC) that you can retrieve using the following code snippet:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Use SageMaker PyTorch DLC as base image
baseimage = sagemaker.image_uris.retrieve(
    framework=&quot;pytorch&quot;,
    region=region,
    py_version=&quot;py310&quot;,
    image_scope=&quot;inference&quot;,
    version=&quot;2.0.0&quot;,
    instance_type=&quot;ml.g5.2xlarge&quot;,
)
print(baseimage)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because the models require resources and additional packages that are not on the base PyTorch DLC, you need to build a Docker image. This image is then uploaded to &lt;a href=&quot;http://aws.amazon.com/ecr/&quot;&gt;Amazon Elastic Container Registry&lt;/a&gt; (Amazon ECR) so we can access directly from SageMaker. The custom installed libraries are listed in the Docker file:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ARG BASE_IMAGE

FROM $BASE_IMAGE

#Install any additional libraries
RUN pip install segment-anything-py==1.0
RUN pip install opencv-python-headless==4.7.0.68
RUN pip install matplotlib==3.6.3
RUN pip install diffusers
RUN pip install tqdm
RUN pip install easydict
RUN pip install scikit-image
RUN pip install xformers
RUN pip install tensorflow
RUN pip install joblib
RUN pip install matplotlib
RUN pip install albumentations==0.5.2
RUN pip install hydra-core==1.1.0
RUN pip install pytorch-lightning
RUN pip install tabulate
RUN pip install kornia==0.5.0
RUN pip install webdataset
RUN pip install omegaconf==2.1.2
RUN pip install transformers==4.28.1
RUN pip install accelerate
RUN pip install ftfy
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run the shell command file to build the custom image locally and push it to Amazon ECR:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%%capture build_output

reponame = &quot;torchserve-mme-demo&quot;
versiontag = &quot;genai-0.1&quot;

# Build our own docker image
!cd workspace/docker &amp;amp;&amp;amp; ./build_and_push.sh {reponame} {versiontag} {baseimage} {region} {account}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;prepare-the-model-artifacts&quot;&gt;Prepare the model artifacts&lt;/h2&gt;

&lt;p&gt;The main difference for the new MMEs with TorchServe support is how you prepare your model artifacts. The code repo provides a skeleton folder for each model (models folder) to house the required files for TorchServe. We follow the same four-step process to prepare each model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tar&lt;/code&gt; file. The following code is an example of the skeleton folder for the SD model:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;workspace
|--sd
   |-- custom_handler.py
   |-- model-config.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first step is to download the pre-trained model checkpoints in the models folder:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import diffusers
import torch
import transformers

pipeline = diffusers.StableDiffusionInpaintPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-2-inpainting&quot;, torch_dtype=torch.float16
)

sd_dir = &quot;workspace/sd/model&quot;
pipeline.save_pretrained(sd_dir)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The next step is to define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;custom_handler.py&lt;/code&gt; file. This is required to define the behavior of the model when it receives a request, such as loading the model, preprocessing the input, and postprocessing the output. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;handle&lt;/code&gt; method is the main entry point for requests, and it accepts a request object and returns a response object. It loads the pre-trained model checkpoints and applies the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preprocess&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;postprocess&lt;/code&gt; methods to the input and output data. The following code snippet illustrates a simple structure of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;custom_handler.py&lt;/code&gt; file. For more detail, refer to the &lt;a href=&quot;https://github.com/pytorch/serve/blob/4e2126277cff57e61e455097987c3be7d625f384/docs/custom_service.md?plain=1#L10&quot;&gt;TorchServe handler API.&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def initialize(self, ctx: Context):

def preprocess(self, data):

def inference(self, data):

def handle(self, data, context):
    requests = self.preprocess(data)
    responses = self.inference(requests)

    return responses
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The last required file for TorchServe is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model-config.yaml&lt;/code&gt;. The file defines the configuration of the model server, such as number of workers and batch size. The configuration is at a per-model level, and an example config file is shown in the following code. For a complete list of parameters, refer to the &lt;a href=&quot;https://github.com/pytorch/serve/blob/master/frontend/archive/src/main/java/org/pytorch/serve/archive/model/ModelConfig.java#L14&quot;&gt;GitHub repo&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minWorkers: 1
maxWorkers: 1
batchSize: 1
maxBatchDelay: 200
responseTimeout: 300
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The final step is to package all the model artifacts into a single .tar.gz file using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch-model-archiver&lt;/code&gt; module:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!torch-model-archiver --model-name sd --version 1.0 --handler workspace/sd/custom_handler.py --extra-files workspace/sd/model --config-file workspace/sam/model-config.yaml --archive-format no-archive!cd sd &amp;amp;&amp;amp; tar cvzf sd.tar.gz .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;create-the-multi-model-endpoint&quot;&gt;Create the multi-model endpoint&lt;/h2&gt;

&lt;p&gt;The steps to create a SageMaker MME are the same as before. In this particular example, you spin up an endpoint using the SageMaker SDK. Start by defining an &lt;a href=&quot;http://aws.amazon.com/s3&quot;&gt;Amazon Simple Storage Service&lt;/a&gt; (Amazon S3) location and the hosting container. This S3 location is where SageMaker will dynamically load the models base on invocation patterns. The hosting container is the custom container you built and pushed to Amazon ECR in the earlier step. See the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# This is where our MME will read models from on S3.
multi_model_s3uri = output_path
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you want to define a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MulitDataModel&lt;/code&gt; that captures all the attributes like model location, hosting container, and permission access:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;print(multi_model_s3uri)
model = Model(
    model_data=f&quot;{multi_model_s3uri}/sam.tar.gz&quot;,
    image_uri=container,
    role=role,
    sagemaker_session=smsess,
    env={&quot;TF_ENABLE_ONEDNN_OPTS&quot;: &quot;0&quot;},
)

mme = MultiDataModel(
    name=&quot;torchserve-mme-genai-&quot; + datetime.now().strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;),
    model_data_prefix=multi_model_s3uri,
    model=model,
    sagemaker_session=smsess,
)
print(mme)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deploy()&lt;/code&gt; function creates an endpoint configuration and hosts the endpoint:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mme.deploy(
    initial_instance_count=1,
    instance_type=&quot;ml.g5.2xlarge&quot;,
    serializer=sagemaker.serializers.JSONSerializer(),
    deserializer=sagemaker.deserializers.JSONDeserializer(),
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the example we provided, we also show how you can list models and dynamically add new models using the SDK. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add_model()&lt;/code&gt; function copies your local model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tar&lt;/code&gt; files into the MME S3 location:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Only sam.tar.gz visible!
list(mme.list_models())

models = [&quot;sd/sd.tar.gz&quot;, &quot;lama/lama.tar.gz&quot;]
for model in models:
    mme.add_model(model_data_source=model)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;invoke-the-models&quot;&gt;Invoke the models&lt;/h2&gt;

&lt;p&gt;Now that we have all three models hosted on an MME, we can invoke each model in sequence to build our language-assisted editing features. To invoke each model, provide a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target_model&lt;/code&gt; parameter in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predictor.predict()&lt;/code&gt; function. The model name is just the name of the model &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.tar&lt;/code&gt; file we uploaded. The following is an example code snippet for the SAM model that takes in a pixel coordinate, a point label, and dilate kernel size, and generates a segmentation mask of the object in the pixel location:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;img_file = &quot;workspace/test_data/sample1.png&quot;
img_bytes = None

with Image.open(img_file) as f:
    img_bytes = encode_image(f)

gen_args = json.dumps(dict(point_coords=[750, 500], point_labels=1, dilate_kernel_size=15))

payload = json.dumps({&quot;image&quot;: img_bytes, &quot;gen_args&quot;: gen_args}).encode(&quot;utf-8&quot;)

response = predictor.predict(data=payload, target_model=&quot;/sam.tar.gz&quot;)
encoded_masks_string = json.loads(response.decode(&quot;utf-8&quot;))[&quot;generated_image&quot;]
base64_bytes_masks = base64.b64decode(encoded_masks_string)

with Image.open(io.BytesIO(base64_bytes_masks)) as f:
    generated_image_rgb = f.convert(&quot;RGB&quot;)
    generated_image_rgb.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To remove an unwanted object from an image, take the segmentation mask generated from SAM and feed that into the LaMa model with the original image. The following images show an example.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;img alt=&quot;Dog on a bench&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg9b.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;White mask of dog on black background&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg10.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;Just a bench&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg3.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sample image
   &lt;/td&gt;
   &lt;td&gt;Segmentation mask from SAM
   &lt;/td&gt;
   &lt;td&gt;Erase the dog using LaMa
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;To modify or replace any object in an image with a text prompt, take the segmentation mask from SAM and feed it into SD model with the original image and text prompt, as shown in the following example.&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;img alt=&quot;Dog on a bench&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg9b.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img alt=&quot;White mask of dog on black background&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg10.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
   &lt;td&gt;
&lt;img alt=&quot;Hamster on a bench&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg11.jpg&quot; style=&quot;width:100%;max-width: 258px;&quot; /&gt;

   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sample image
   &lt;/td&gt;
   &lt;td&gt;Segmentation mask from SAM
   &lt;/td&gt;
   &lt;td&gt;Replace using SD model with text prompt
		&lt;br /&gt;
		“a hamster on a bench”
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;cost-savings&quot;&gt;Cost savings&lt;/h2&gt;

&lt;p&gt;The benefits of SageMaker MMEs increase based on the scale of model consolidation. The following table shows the GPU memory usage of the three models in this post. They are deployed on one &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g5.2xlarge&lt;/code&gt; instance by using one SageMaker MME.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Model&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;GPU Memory (MiB)&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Segment Anything Model
   &lt;/td&gt;
   &lt;td&gt;3,362
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Stable Diffusion In Paint
   &lt;/td&gt;
   &lt;td&gt;3,910
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Lama
   &lt;/td&gt;
   &lt;td&gt;852
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;You can see cost savings when hosting the three models with one endpoint, and for use cases with hundreds or thousands of models, the savings are much greater.&lt;/p&gt;

&lt;p&gt;For example, consider 100 Stable Diffusion models. Each of the models on its own could be served by an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ml.g5.2xlarge&lt;/code&gt; endpoint (4 GiB memory), costing &lt;span&gt;$&lt;/span&gt;1.52 per instance hour in the US East (N. Virginia) Region. To provide all 100 models using their own endpoint would cost &lt;span&gt;$&lt;/span&gt;218,880 per month. With a SageMaker MME, a single endpoint using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ml.g5.2xlarge&lt;/code&gt; instances can host four models simultaneously. This reduces production inference costs by 75% to only &lt;span&gt;$&lt;/span&gt;54,720 per month. The following table summarizes the differences between single-model and multi-model endpoints for this example. Given an endpoint configuration with sufficient memory for your target models, steady state invocation latency after all models have been loaded will be similar to that of a single-model endpoint.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Single-model endpoint&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Multi-model endpoint&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Total endpoint price per month
   &lt;/td&gt;
   &lt;td&gt;$218,880
   &lt;/td&gt;
   &lt;td&gt;$54,720
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Endpoint instance type
   &lt;/td&gt;
   &lt;td&gt;ml.g5.2xlarge
   &lt;/td&gt;
   &lt;td&gt;ml.g5.2xlarge
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU Memory capacity (GiB)
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
   &lt;td&gt;32
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GPU Memory capacity (GiB)
   &lt;/td&gt;
   &lt;td&gt;24
   &lt;/td&gt;
   &lt;td&gt;24
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Endpoint price per hour
   &lt;/td&gt;
   &lt;td&gt;$1.52
   &lt;/td&gt;
   &lt;td&gt;$1.52
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Number of instances per endpoint
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
   &lt;td&gt;2
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Endpoints needed for 100 models
   &lt;/td&gt;
   &lt;td&gt;100
   &lt;/td&gt;
   &lt;td&gt;25
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;clean-up&quot;&gt;Clean up&lt;/h2&gt;

&lt;p&gt;After you are done, please follow the instructions in the cleanup section of the notebook to delete the resources provisioned in this post to avoid unnecessary charges. Refer to &lt;a href=&quot;https://aws.amazon.com/sagemaker/pricing/&quot;&gt;Amazon SageMaker Pricing&lt;/a&gt; for details on the cost of the inference instances.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This post demonstrates the language-assisted editing capabilities made possible through the use of generative AI models hosted on SageMaker MMEs with TorchServe. The example we shared illustrates how we can use resource sharing and simplified model management with SageMaker MMEs while still utilizing TorchServe as our model serving stack. We utilized three deep learning foundation models: SAM, SD 2 Inpainting, and LaMa. These models enable us to build powerful capabilities, such as erasing any unwanted object from an image and modifying or replacing any object in an image by supplying a text instruction. These features can help artists and content creators work more efficiently and meet their content demands by automating repetitive tasks, optimizing campaigns, and providing a hyper-personalized experience. We invite you to explore the example provided in this post and build your own UI experience using TorchServe on a SageMaker MME.&lt;/p&gt;

&lt;p&gt;To get started, see &lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html#multi-model-support&quot;&gt;Supported algorithms, frameworks, and instances for multi-model endpoints using GPU backed instances&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;about-the-authors&quot;&gt;About the authors&lt;/h3&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;img alt=&quot;James Wu&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg12.jpg&quot; style=&quot;width: 150px;&quot; /&gt;
&lt;/td&gt;
&lt;td style=&quot;vertical-align: top;&quot;&gt;
&lt;strong&gt;James Wu&lt;/strong&gt; is a Senior AI/ML Specialist Solution Architect at AWS. helping customers design and build AI/ML solutions. James’s work covers a wide range of ML use cases, with a primary interest in computer vision, deep learning, and scaling ML across the enterprise. Prior to joining AWS, James was an architect, developer, and technology leader for over 10 years, including 6 years in engineering and 4 years in marketing &amp;amp; advertising industries.
&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;
&lt;img alt=&quot;Li Ning&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg13.jpg&quot; style=&quot;width: 150px;&quot; /&gt;
&lt;/td&gt;
&lt;td style=&quot;vertical-align: top;&quot;&gt;

&lt;strong&gt;Li Ning&lt;/strong&gt; is a senior software engineer at AWS with a specialization in building large-scale AI solutions. As a tech lead for TorchServe, a project jointly developed by AWS and Meta, her passion lies in leveraging PyTorch and AWS SageMaker to help customers embrace AI for the greater good. Outside of her professional endeavors, Li enjoys swimming, traveling, following the latest advancements in technology, and spending quality time with her family.
&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;
&lt;img alt=&quot;Ankith Gunapal&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg14.jpg&quot; style=&quot;width: 150px;&quot; /&gt;
&lt;/td&gt;
&lt;td style=&quot;vertical-align: top;&quot;&gt;
&lt;strong&gt;Ankith Gunapal&lt;/strong&gt; is an AI Partner Engineer at Meta (PyTorch). He is passionate about model optimization and model serving, with experience ranging from RTL verification, embedded software, computer vision, to PyTorch. He holds a Master’s in Data Science and a Master’s in Telecommunications. Outside of work, Ankith is also an electronic dance music producer.

&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;
&lt;img alt=&quot;Saurabh Trikande&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg15.jpg&quot; style=&quot;width: 150px;&quot; /&gt;
&lt;/td&gt;
&lt;td style=&quot;vertical-align: top;&quot;&gt;
&lt;strong&gt;Saurabh Trikande&lt;/strong&gt; is a Senior Product Manager for Amazon SageMaker Inference. He is passionate about working with customers and is motivated by the goal of democratizing machine learning. He focuses on core challenges related to deploying complex ML applications, multi-tenant ML models, cost optimizations, and making deployment of deep learning models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch and spending time with his family.

&lt;/td&gt;
&lt;/tr&gt;


&lt;tr&gt;
&lt;td&gt;
&lt;img alt=&quot;Subhash Talluri&quot; src=&quot;/assets/images/amazon-sagemaker-w-torchserve/fg16.jpg&quot; style=&quot;width: 150px;&quot; /&gt;
&lt;/td&gt;
&lt;td style=&quot;vertical-align: top;&quot;&gt;
&lt;strong&gt;Subhash Talluri&lt;/strong&gt; is a Lead AI/ML solutions architect of the Telecom Industry business unit at Amazon Web Services. He’s been leading development of innovative AI/ML solutions for Telecom customers and partners worldwide. He brings interdisciplinary expertise in engineering and computer science to help build scalable, secure, and compliant AI/ML solutions via cloud-optimized architectures on AWS.

&lt;/td&gt;
&lt;/tr&gt;

&lt;/table&gt;</content>

      
      
      
      
      

      <author>
          <name>James Wu, Ankith Gunapal, Li Ning, Subhash Talluri, and Saurabh Trikande</name>
        
        
      </author>

      

      

      
        <summary type="html">Multi-model endpoints (MMEs) are a powerful feature of Amazon SageMaker designed to simplify the deployment and operation of machine learning (ML) models. With MMEs, you can host multiple models on a single serving container and host all the models behind a single endpoint. The SageMaker platform automatically manages the loading and unloading of models and scales resources based on traffic patterns, reducing the operational burden of managing a large quantity of models. This feature is particularly beneficial for deep learning and generative AI models that require accelerated compute. The cost savings achieved through resource sharing and simplified model management makes SageMaker MMEs an excellent choice for you to host models at scale on AWS.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Generative AI Part III: Diffusion, Fast</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai-3/" rel="alternate" type="text/html" title="Accelerating Generative AI Part III: Diffusion, Fast" />
      <published>2024-01-03T00:00:00-08:00</published>
      <updated>2024-01-03T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai-3</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai-3/">&lt;p&gt;This post is the third part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;Segment Anything over 8x&lt;/a&gt; using only pure, native PyTorch. In part two, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;Llama-7B by almost 10x&lt;/a&gt; using only native PyTorch optimizations. In this blog, we’ll focus on speeding up text-to-image diffusion models by upto 3x.&lt;/p&gt;

&lt;p&gt;We will leverage an array of optimizations including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Running with the bfloat16 precision&lt;/li&gt;
  &lt;li&gt;scaled_dot_product_attention (SPDA)&lt;/li&gt;
  &lt;li&gt;torch.compile&lt;/li&gt;
  &lt;li&gt;Combining q,k,v projections for attention computation&lt;/li&gt;
  &lt;li&gt;Dynamic int8 quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will primarily focus on Stable Diffusion XL (SDXL), demonstrating a latency improvement of 3x. These techniques are PyTorch-native, which means you don’t have to rely on any third-party libraries or any C++ code to take advantage of them.&lt;/p&gt;

&lt;p&gt;Enabling these optimizations with the 🤗Diffusers library takes just a few lines of code. If you’re already feeling excited and cannot wait to jump to the code, check out the accompanying repository here: &lt;a href=&quot;https://github.com/huggingface/diffusion-fast&quot;&gt;https://github.com/huggingface/diffusion-fast&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg1.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(The discussed techniques are not SDXL-specific and can be used to speed up other text-to-image diffusion systems, as shown later.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Below, you can find some blog posts on similar topics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/accelerated-diffusers-pt-20/&quot;&gt;Accelerated Diffusers with PyTorch 2.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/blog/simple_sdxl_optimizations&quot;&gt;Exploring simple optimizations for SDXL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/accelerated-generative-diffusion-models/&quot;&gt;Accelerated Generative Diffusion Models with PyTorch 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;We will demonstrate the optimizations and their respective speed-up gains using the 🤗&lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;Diffusers library&lt;/a&gt;. Apart from that, we will make use of the following PyTorch-native libraries and environments:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Torch nightly (to benefit from the fastest kernels for efficient attention; 2.3.0.dev20231218+cu121)&lt;/li&gt;
  &lt;li&gt;🤗 PEFT (version: 0.7.1)&lt;/li&gt;
  &lt;li&gt;torchao (commit SHA: 54bcd5a10d0abbe7b0c045052029257099f83fd9)&lt;/li&gt;
  &lt;li&gt;CUDA 12.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For an easier reproduction environment, you can also refer to this &lt;a href=&quot;https://github.com/huggingface/sdxl-fast/blob/main/Dockerfile&quot;&gt;Dockerfile&lt;/a&gt;. The benchmarking numbers presented in this post come from a 400W 80GB A100 GPU (with its clock rate set to its maximum capacity).&lt;/p&gt;

&lt;p&gt;Since we use an A100 GPU (Ampere architecture) here, we can specify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.set_float32_matmul_precision(&quot;high&quot;)&lt;/code&gt; to benefit from the &lt;a href=&quot;https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/&quot;&gt;TF32 precision format&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;run-inference-using-a-reduced-precision&quot;&gt;Run inference using a reduced precision&lt;/h2&gt;

&lt;p&gt;Running SDXL in Diffusers just takes a few lines of code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionXLPipeline

## Load the pipeline in full-precision and place its model components on CUDA.
pipe = StableDiffusionXLPipeline.from_pretrained(&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;).to(&quot;cuda&quot;)

## Run the attention ops without efficiency.
pipe.unet.set_default_attn_processor()
pipe.vae.set_default_attn_processor()

prompt = &quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;
image = pipe(prompt, num_inference_steps=30).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But this isn’t very practical as it takes &lt;strong&gt;7.36 seconds&lt;/strong&gt; to generate a single image with 30 steps. This is our baseline which we will try to optimize one step at a time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg2.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, we’re running the pipeline with the full precision. We can immediately cut down the inference time by using a reduced precision such as &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;bfloat16&lt;/a&gt;. Besides, modern GPUs come with dedicated cores for running accelerated computation benefiting from reduced precision. To run the computations of the pipeline in the bfloat16 precision, we just need to specify the data type while initializing the pipeline:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionXLPipeline

pipe = StableDiffusionXLPipeline.from_pretrained(
	&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.bfloat16
).to(&quot;cuda&quot;)

## Run the attention ops without efficiency.
pipe.unet.set_default_attn_processor()
pipe.vae.set_default_attn_processor()
prompt = &quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;
image = pipe(prompt, num_inference_steps=30).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg3.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By using a reduced precision, we’re able to cut down the inference latency from &lt;strong&gt;7.36 seconds to 4.63 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some notes on the use of bfloat16&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using a reduced numerical precision (such as float16, bfloat16) to run inference doesn’t affect the generation quality but significantly improves latency.&lt;/li&gt;
  &lt;li&gt;The benefits of using the &lt;a href=&quot;https://cloud.google.com/tpu/docs/bfloat16&quot;&gt;bfloat16&lt;/a&gt; numerical precision as compared to float16 are hardware-dependent. Modern generations of GPUs tend to favor bfloat16.&lt;/li&gt;
  &lt;li&gt;Furthermore, in our experiments, we bfloat16 to be much more resilient when used with quantization in comparison to float16.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;(We later ran the experiments in float16 and found out that the recent versions of torchao do not incur numerical problems from float16.)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-sdpa-for-performing-attention-computations&quot;&gt;Use SDPA for performing attention computations&lt;/h2&gt;

&lt;p&gt;By default, Diffusers uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; (SDPA) for performing attention-related computations when using PyTorch 2. SDPA provides faster and more efficient kernels to run intensive attention-related operations. To run the pipeline SDPA, we simply don’t set any attention processor like so:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionXLPipeline

pipe = StableDiffusionXLPipeline.from_pretrained(
	&quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.bfloat16
).to(&quot;cuda&quot;)

prompt = &quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;
image = pipe(prompt, num_inference_steps=30).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;SDPA gives a nice boost from &lt;strong&gt;4.63 seconds to 3.31 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg4.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;compiling-the-unet-and-vae&quot;&gt;Compiling the UNet and VAE&lt;/h2&gt;

&lt;p&gt;We can ask PyTorch to perform some low-level optimizations (such as operator fusion and launching faster kernels with CUDA graphs) by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. For the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StableDiffusionXLPipeline&lt;/code&gt;, we compile the denoiser (UNet) and the VAE:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from diffusers import StableDiffusionXLPipeline
import torch

pipe = StableDiffusionXLPipeline.from_pretrained(
    &quot;stabilityai/stable-diffusion-xl-base-1.0&quot;, torch_dtype=torch.bfloat16
).to(&quot;cuda&quot;)

## Compile the UNet and VAE.
pipe.unet = torch.compile(pipe.unet, mode=&quot;max-autotune&quot;, fullgraph=True)
pipe.vae.decode = torch.compile(pipe.vae.decode, mode=&quot;max-autotune&quot;, fullgraph=True)

prompt = &quot;Astronaut in a jungle, cold color palette, muted colors, detailed, 8k&quot;

## First call to `pipe` will be slow, subsequent ones will be faster.
image = pipe(prompt, num_inference_steps=30).images[0]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using SDPA attention and compiling both the UNet and VAE reduces the latency from &lt;strong&gt;3.31 seconds to 2.54 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg5.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Notes on &lt;code&gt;torch.compile&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; offers different backends and modes. As we’re aiming for maximum inference speed, we opt for the inductor backend using the “max-autotune”. “max-autotune” uses CUDA graphs and optimizes the compilation graph specifically for latency. Using CUDA graphs greatly reduces the overhead of launching GPU operations. It saves time by using a mechanism to launch multiple GPU operations through a single CPU operation.&lt;/p&gt;

&lt;p&gt;Specifying &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fullgraph&lt;/code&gt; to be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; ensures that there are no graph breaks in the underlying model, ensuring the fullest potential of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;. In our case, the following compiler flags were also important to be explicitly set:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch._inductor.config.conv_1x1_as_mm = True
torch._inductor.config.coordinate_descent_tuning = True
torch._inductor.config.epilogue_fusion = False
torch._inductor.config.coordinate_descent_check_all_directions = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the full list of compiler flags, refer to &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/_inductor/config.py&quot;&gt;this file.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We also change the memory layout of the UNet and the VAE to “channels_last” when compiling them to ensure maximum speed:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pipe.unet.to(memory_format=torch.channels_last)
pipe.vae.to(memory_format=torch.channels_last)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the next section, we’ll show how to improve the latency even further.&lt;/p&gt;

&lt;h2 id=&quot;additional-optimizations&quot;&gt;Additional optimizations&lt;/h2&gt;

&lt;h3 id=&quot;no-graph-breaks-during-torchcompile&quot;&gt;No graph breaks during &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;Ensuring that the underlying model/method can be fully compiled is crucial for performance (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fullgraph=True&lt;/code&gt;). This means having no graph breaks. We did this for the UNet and VAE by changing how we access the returning variables. Consider the following example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg5b.jpg&quot; alt=&quot;code example&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;getting-rid-of-gpu-syncs-after-compilation&quot;&gt;Getting rid of GPU syncs after compilation&lt;/h3&gt;

&lt;p&gt;During the iterative reverse diffusion process, we &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/1d686bac8146037e97f3fd8c56e4063230f71751/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py#L1228&quot;&gt;call&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step()&lt;/code&gt; on the scheduler each time after the denoiser predicts the less noisy latent embeddings. Inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step()&lt;/code&gt;, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigmas&lt;/code&gt; variable is &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/1d686bac8146037e97f3fd8c56e4063230f71751/src/diffusers/schedulers/scheduling_euler_discrete.py#L476&quot;&gt;indexed&lt;/a&gt;. If the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigmas&lt;/code&gt; array is placed on the GPU, indexing causes a communication sync between the CPU and GPU. This causes a latency, and it becomes more evident when the denoiser has already been compiled.&lt;/p&gt;

&lt;p&gt;But if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sigmas &lt;/code&gt;array always stays on the CPU (refer to &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/35a969d297cba69110d175ee79c59312b9f49e1e/src/diffusers/schedulers/scheduling_euler_discrete.py#L240&quot;&gt;this line&lt;/a&gt;), this sync doesn’t take place, hence improved latency. In general, any CPU &amp;lt;-&amp;gt; GPU communication sync should be none or be kept to a bare minimum as it can impact inference latency.&lt;/p&gt;

&lt;h3 id=&quot;using-combined-projections-for-attention-ops&quot;&gt;Using combined projections for attention ops&lt;/h3&gt;

&lt;p&gt;Both the UNet and the VAE used in SDXL make use of Transformer-like blocks. A Transformer block consists of attention blocks and feed-forward blocks.&lt;/p&gt;

&lt;p&gt;In an attention block, the input is projected into three sub-spaces using three different projection matrices – Q, K, and V. In the naive implementation, these projections are performed separately on the input. But we can horizontally combine the projection matrices into a single matrix and perform the projection in one shot. This increases the size of the matmuls of the input projections and improves the impact of quantization (to be discussed next).&lt;/p&gt;

&lt;p&gt;Enabling this kind of computation in Diffusers just takes a single line of code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pipe.fuse_qkv_projections()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will make the attention operations for both the UNet and the VAE take advantage of the combined projections. For the cross-attention layers, we only combine the key and value matrices. To learn more, you can refer to the official documentation &lt;a href=&quot;https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/stable_diffusion_xl#diffusers.StableDiffusionXLPipeline.fuse_qkv_projections&quot;&gt;here&lt;/a&gt;. It’s worth noting that we &lt;a href=&quot;https://github.com/huggingface/diffusers/blob/35a969d297cba69110d175ee79c59312b9f49e1e/src/diffusers/models/attention_processor.py#L1356&quot;&gt;leverage&lt;/a&gt; PyTorch’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scaled_dot_product_attention&lt;/code&gt; here internally.&lt;/p&gt;

&lt;p&gt;These additional techniques improved the inference latency from &lt;strong&gt;2.54 seconds to 2.52 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg6.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dynamic-int8-quantization&quot;&gt;Dynamic int8 quantization&lt;/h2&gt;

&lt;p&gt;We selectively apply &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/dynamic_quantization.html&quot;&gt;dynamic int8 quantization&lt;/a&gt; to both the UNet and the VAE. This is because quantization adds additional conversion overhead to the model that is hopefully made up for by faster matmuls (dynamic quantization). If the matmuls are too small, these techniques may degrade performance.&lt;/p&gt;

&lt;p&gt;Through experimentation, we found that certain linear layers in the UNet and the VAE don’t benefit from dynamic int8 quantization. You can check out the full code for filtering those layers &lt;a href=&quot;https://github.com/huggingface/diffusion-fast/blob/0f169640b1db106fe6a479f78c1ed3bfaeba3386/utils/pipeline_utils.py#L16&quot;&gt;here&lt;/a&gt; (referred to as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dynamic_quant_filter_fn&lt;/code&gt; below).&lt;/p&gt;

&lt;p&gt;We leverage the ultra-lightweight pure PyTorch library &lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao&lt;/a&gt; to use its user-friendly APIs for quantization:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchao.quantization import apply_dynamic_quant

apply_dynamic_quant(pipe.unet, dynamic_quant_filter_fn)
apply_dynamic_quant(pipe.vae, dynamic_quant_filter_fn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since this quantization support is limited to linear layers only, we also turn suitable pointwise convolution layers into linear layers to maximize the benefit. We also specify the following compiler flags when using this option:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch._inductor.config.force_fuse_int_mm_with_mul = True
torch._inductor.config.use_mixed_mm = True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To prevent any numerical issues stemming from quantization, we run everything in the bfloat16 format.&lt;/p&gt;

&lt;p&gt;Applying quantization this way improved the latency from &lt;strong&gt;2.52 seconds to 2.43 seconds&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg7.png&quot; alt=&quot;SDXL Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;We welcome you to check out the following codebases to reproduce these numbers and extend the techniques to other text-to-image diffusion systems as well:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/diffusion-fast&quot;&gt;diffusion-fast&lt;/a&gt; (repository providing all the code to reproduce the numbers and plots above)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao library&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;Diffusers library&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/huggingface/peft&quot;&gt;PEFT library&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Other links&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/papers/2307.01952&quot;&gt;SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers/main/en/tutorials/fast_diffusion&quot;&gt;Fast diffusion documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;improvements-in-other-pipelines&quot;&gt;Improvements in other pipelines&lt;/h2&gt;

&lt;p&gt;We applied these techniques to other pipelines to test the generality of our approach. Below are our findings:&lt;/p&gt;

&lt;h3 id=&quot;ssd-1b&quot;&gt;&lt;a href=&quot;https://huggingface.co/segmind/SSD-1B&quot;&gt;SSD-1B&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg8.png&quot; alt=&quot;SSD-1B Chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;stable-diffusion-v1-5&quot;&gt;&lt;a href=&quot;https://huggingface.co/runwayml/stable-diffusion-v1-5&quot;&gt;Stable Diffusion v1-5&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg9.png&quot; alt=&quot;Stable Diffusion v1-5 chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pixart-alphapixart-xl-2-1024-ms&quot;&gt;&lt;a href=&quot;https://huggingface.co/PixArt-alpha/PixArt-XL-2-1024-MS&quot;&gt;PixArt-alpha/PixArt-XL-2-1024-MS&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;It’s worth noting that PixArt-Alpha uses a Transformer-based architecture as its denoiser for the reverse diffusion process instead of a UNet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-3/fg10.png&quot; alt=&quot;PixArt-alpha/PixArt-XL-2-1024-MS chart&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that for Stable Diffusion v1-5 and PixArt-Alpha, we didn’t explore the best shape combination criteria for applying dynamic int8 quantization. It might be possible to get better numbers with a better combination.&lt;/p&gt;

&lt;p&gt;Collectively, the methods we presented offer substantial speedup over the baseline without degradation in the generation quality. Furthermore, we believe that these methods should complement other optimization methods popular in the community (such as &lt;a href=&quot;https://github.com/horseee/DeepCache&quot;&gt;DeepCache&lt;/a&gt;, &lt;a href=&quot;https://github.com/chengzeyi/stable-fast&quot;&gt;Stable Fast&lt;/a&gt;, etc.).&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-next-steps&quot;&gt;Conclusion and next steps&lt;/h2&gt;

&lt;p&gt;In this post, we presented a basket of simple yet effective techniques that can help improve the inference latency of text-to-image Diffusion models in pure PyTorch. In summary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using a reduced precision to perform our computations&lt;/li&gt;
  &lt;li&gt;Scaled-dot product attention for running the attention blocks efficiently&lt;/li&gt;
  &lt;li&gt;torch.compile with “max-autotune” to improve for latency&lt;/li&gt;
  &lt;li&gt;Combining the different projections together for computing attention&lt;/li&gt;
  &lt;li&gt;Dynamic int8 quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We believe there’s a lot to be explored in terms of how we apply quantization to a text-to-image diffusion system. We didn’t exhaustively explore which layers in the UNet and the VAE tend to benefit from dynamic quantization. There might be opportunities to further speed things up with a better combination of the layers being targeted for quantization.&lt;/p&gt;

&lt;p&gt;We kept the text encoders of SDXL untouched other than just running them in bfloat16. Optimizing them might also lead to improvements in latency.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://madebyoll.in/&quot;&gt;Ollin Boer Bohan&lt;/a&gt; whose &lt;a href=&quot;https://huggingface.co/madebyollin/sdxl-vae-fp16-fix&quot;&gt;VAE&lt;/a&gt; was used throughout the benchmarking process as it is numerically more stable under reduced numerical precisions.&lt;/p&gt;

&lt;p&gt;Thanks to Hugo Larcher from Hugging Face for helping with infrastructure.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Sayak Paul and Patrick von Platen (Hugging Face 🤗)</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is the third part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In part two, we showed how to accelerate Llama-7B by almost 10x using only native PyTorch optimizations. In this blog, we’ll focus on speeding up text-to-image diffusion models by upto 3x.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Understanding GPU Memory 2: Finding and Removing Reference Cycles</title>
      <link href="https://pytorch.org/blog/understanding-gpu-memory-2/" rel="alternate" type="text/html" title="Understanding GPU Memory 2: Finding and Removing Reference Cycles" />
      <published>2023-12-19T00:00:00-08:00</published>
      <updated>2023-12-19T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/understanding-gpu-memory-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/understanding-gpu-memory-2/">&lt;p&gt;This is part 2 of the Understanding GPU Memory blog series. Our first post &lt;a href=&quot;/blog/understanding-gpu-memory-1/&quot;&gt;Understanding GPU Memory 1: Visualizing All Allocations over Time&lt;/a&gt; shows how to use the  memory snapshot tool. In this part, we will use the Memory Snapshot to visualize a GPU memory leak caused by reference cycles, and then locate and remove them in our code using the Reference Cycle Detector.&lt;/p&gt;

&lt;p&gt;Sometimes when we were using the Memory Snapshot, we saw plots of GPU memory that looked similar to this.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/memory_leak_oom.jpg&quot; alt=&quot;GPU memory&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this snapshot, each peak shows GPU tensors building up over time and then several tensors getting released at once. In addition, a CUDA OOM happens on the right side causing all the tensors to be released. Seeing the tensors accumulate like this is a &lt;strong&gt;clear indication of a problem, but it doesn’t immediately suggest why&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;tensors-in-reference-cycles&quot;&gt;Tensors in Reference Cycles&lt;/h2&gt;

&lt;p&gt;During early debugging, we dug in further to find that this **pattern happens a lot when your Python code has objects with reference cycles. ** Python will clean up non-cyclic objects immediately using reference counting. However objects in reference cycles are only cleaned up later by a cycle collector. If these cycles refer to a GPU tensor, the GPU tensor will stay alive until that cycle collector runs and removes the reference cycle. Let’s take a look at a simplified example.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/simple_reference_cycle.png&quot; alt=&quot;Simple reference cycle&quot; style=&quot;width:100%; max-width:400px; margin-right: auto; margin-left: auto; display: block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code Snippet behind the snapshot (full code in Appendix A):&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def leak(tensor_size, num_iter=100000, device=&quot;cuda:0&quot;):
      class Node:
        def __init__(self, T):
          self.tensor = T
          self.link = None

      for _ in range(num_iter):
        A = torch.zeros(tensor_size, device=device)
        B = torch.zeros(tensor_size, device=device)
        a, b = Node(A), Node(B)

        # A reference cycle will force refcounts to be non-zero.
        a.link, b.link = b, a
        # Python will eventually garbage collect a &amp;amp; b, but will
        # OOM on the GPU before that happens (since python
        # runtime doesn't know about CUDA memory usage).
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code example, the tensors A and B are created, where A has a link to B and vice versa. This forces a non-zero reference count when A and B go out of scope. When we run this for 100,000 iterations, we expect the automatic garbage collection to free the reference cycles when going out of scope. However, this will actually CUDA OOM.&lt;/p&gt;

&lt;h3 id=&quot;why-doesnt-automatic-garbage-collection-work&quot;&gt;Why doesn’t automatic garbage collection work?&lt;/h3&gt;

&lt;p&gt;The automatic garbage collection works well when there is a lot of extra memory as is common on CPUs because it amortizes the expensive garbage collection by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Tracing_garbage_collection#Generational_GC_(ephemeral_GC)&quot;&gt;Generational Garbage Collection&lt;/a&gt;. But to amortize the collection work, it defers some memory cleanup making the maximum memory usage higher, which is less suited to memory constrained environments. The Python runtime also has no insights into CUDA memory usage, so it cannot be triggered on high memory pressure either. It’s even more challenging as GPU training is almost always memory constrained because we will often raise the batch size to use any additional free memory.&lt;/p&gt;

&lt;p&gt;The CPython’s garbage collection frees unreachable objects held in reference cycles via the &lt;strong&gt;mark-and-sweep&lt;/strong&gt;. The garbage collection is automatically run when the number of objects exceeds certain thresholds. There are &lt;strong&gt;3 generations of thresholds&lt;/strong&gt; to help amortize the expensive costs of running garbage collection on every object. The later generations are less frequently run. This would explain why automatic collections will only clear several tensors on each peak, however there are still tensors that leak resulting in the CUDA OOM. Those tensors were held by reference cycles in later generations.&lt;/p&gt;

&lt;h2 id=&quot;explicitly-calling-gccollect&quot;&gt;Explicitly calling gc.collect()&lt;/h2&gt;

&lt;p&gt;One way to fix this is by explicitly calling the garbage collector frequently. Here we can see that the GPU memory for tensors out of scope gets cleaned up when we explicitly call the garbage collector every 100 iterations. This also controls the maximum GPU peak memory held by leaking tensors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/memory_leak_gc_collect.jpg&quot; alt=&quot;memory leak&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Although this works and fixes the CUDA OOM issue, calling gc.collect() too frequently can cause other issues including &lt;strong&gt;QPS regressions&lt;/strong&gt;. Therefore we cannot simply increase the frequency of garbage collection on every training job. &lt;strong&gt;It’s best to just avoid creating reference cycles in the first place&lt;/strong&gt;. More on this in section, Reference Cycle Detector.&lt;/p&gt;

&lt;h2 id=&quot;sneaky-memory-leak-in-callback&quot;&gt;Sneaky Memory Leak in Callback&lt;/h2&gt;

&lt;p&gt;Real examples are more complicated, so let’s look at a more realistic example that has a similar behavior. In this snapshot, we can observe the same behavior of tensors being accumulated and freed during automatic garbage collection, until we hit a CUDA OOM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/memory_leak_awaitable.jpg&quot; alt=&quot;memory leak&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Code Snippet behind this snapshot (full code sample in Appendix A):&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    class AwaitableTensor:
      def __init__(self, tensor_size):
        self._tensor_size = tensor_size
        self._tensor = None

      def wait(self):
        self._tensor = torch.zeros(self._tensor_size, device=&quot;cuda:0&quot;)
        return self._tensor

    class AwaitableTensorWithViewCallback:
      def __init__(self, tensor_awaitable, view_dim):
        self._tensor_awaitable = tensor_awaitable
        self._view_dim = view_dim
        # Add a view filter callback to the tensor.
        self._callback = lambda ret: ret.view(-1, self._view_dim)

      def wait(self):
        return self._callback(self._tensor_awaitable.wait())

    async def awaitable_leak(
      tensor_size=2**27, num_iter=100000,
    ):
      for _ in range(num_iter):
        A = AwaitableTensor(tensor_size)
        AwaitableTensorWithViewCallBack(A, 4).wait()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code, we define two classes. The class AwaitableTensor will create a tensor when waited upon. Another class AwaitableTensorWithViewCallback will apply a view filter on the AwaitableTensor via callback lambda.&lt;/p&gt;

&lt;p&gt;When running awaitable_leak, which creates tensor A (512 MB) and applies a view filter for 100,000 iterations, we expect that A should be reclaimed each time it goes out of scope because the reference count should reach 0. However, this will actually OOM!&lt;/p&gt;

&lt;p&gt;While we know there is a reference cycle here, it isn’t clear from the code where the cycle is created. &lt;strong&gt;To help with these situations, we have created a tool to locate and report these cycles.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference-cycle-detector&quot;&gt;Reference Cycle Detector&lt;/h2&gt;

&lt;p&gt;Introducing the &lt;strong&gt;Reference Cycle Detector&lt;/strong&gt;, which helps us find reference cycles keeping GPU tensors alive. The API is fairly simple:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;During model initialization:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Import:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from torch.utils.viz._cycles import warn_tensor_cycles&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Start:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;warn_tensor_cycles()&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Reference Cycle Detector will issue warnings every time that the cycle collector runs and finds a CUDA tensor that gets freed. The warning provides an &lt;strong&gt;object graph&lt;/strong&gt; showing how the reference cycle refers to the GPU tensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/awaitable_leak_cycle.png&quot; alt=&quot;object graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For instance in this object graph, we can easily observe that there is a &lt;strong&gt;circular dependency on the outer circle of the graph&lt;/strong&gt;, and &lt;strong&gt;highlighted in red is the GPU tensor kept alive&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Most cycles are pretty easy to fix once they are discovered. For instance here we can remove the reference to self created by self._view_dim in the callback.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/awaitable_code_snippet.png&quot; alt=&quot;code snippet&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve spent some time fixing cycles in existing models using these tools. For example in TorchRec, we’ve found and removed a reference cycle in &lt;a href=&quot;https://github.com/pytorch/torchrec/pull/1226&quot;&gt;PR#1226&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/torchrec_code_snippet.png&quot; alt=&quot;code snippet&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once we’ve removed the reference cycles, the code will &lt;strong&gt;no longer issue a CUDA OOM nor show any memory leaks&lt;/strong&gt; in their snapshots.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-other-benefits-of-using-the-reference-cycle-detector&quot;&gt;What are the other benefits of using the Reference Cycle Detector?&lt;/h3&gt;

&lt;p&gt;Removing these cycles will also directly &lt;strong&gt;lower the maximum GPU memory usage&lt;/strong&gt; as well as make it &lt;strong&gt;less likely for memory to fragment&lt;/strong&gt; because the allocator returns to the same state after each iteration.&lt;/p&gt;

&lt;h2 id=&quot;where-can-i-find-these-tools&quot;&gt;Where can I find these tools?&lt;/h2&gt;

&lt;p&gt;We hope that the Reference Cycle Detector will greatly improve your ability to find and remove memory leaks caused by reference cycles. The Reference Cycle Detector is available in the v2.1 release of PyTorch as experimental features and More information about the Reference Cycle Detector can be found in the &lt;a href=&quot;https://pytorch.org/docs/main/torch_cuda_memory.html&quot;&gt;PyTorch Memory docs here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;We look forward to hearing from you about any enhancements, bugs or memory stories that our tools helped to solve! As always, please feel free to open new issues on PyTorch’s Github page.&lt;/p&gt;

&lt;p&gt;We are also open to contributions from the OSS community, feel free to tag &lt;a href=&quot;https://github.com/aaronenyeshi&quot;&gt;Aaron Shi&lt;/a&gt; and &lt;a href=&quot;https://github.com/zdevito&quot;&gt;Zachary DeVito&lt;/a&gt; in any Github PRs for reviews.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Really appreciate the content reviewers, &lt;a href=&quot;mailto:marksaroufim@meta.com&quot;&gt;Mark Saroufim&lt;/a&gt;, &lt;a href=&quot;mailto:gchanan@meta.com&quot;&gt;Gregory Chanan&lt;/a&gt;, and &lt;a href=&quot;mailto:adnanaziz@meta.com&quot;&gt;Adnan Aziz&lt;/a&gt; for reviewing this post and improving its readability.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;appendix-a---code-sample&quot;&gt;Appendix A - Code Sample&lt;/h3&gt;

&lt;p&gt;This code snippet was used to generate the plots and examples shown. Here are the arguments to reproduce the sections:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Introduction: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python sample.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Explicitly calling gc.collect(): &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python sample.py --gc_collect_interval=100&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Sneaky Memory Leak in Callback: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python sample.py --workload=awaitable&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ref Cycle Detector: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python sample.py --workload=awaitable --warn_tensor_cycles&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;sample.py:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# (c) Meta Platforms, Inc. and affiliates. 
import argparse
import asyncio
import gc
import logging
import socket
from datetime import datetime, timedelta

import torch

logging.basicConfig(
   format=&quot;%(levelname)s:%(asctime)s %(message)s&quot;,
   level=logging.INFO,
   datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = &quot;%b_%d_%H_%M_%S&quot;

# Keep a max of 100,000 alloc/free events in the recorded history
# leading up to the snapshot.
MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000

def start_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Starting snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

def stop_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Stopping snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(enabled=None)

def export_memory_snapshot() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not exporting memory snapshot&quot;)
       return

   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f&quot;{host_name}_{timestamp}&quot;

   try:
       logger.info(f&quot;Saving snapshot to local file: {file_prefix}.pickle&quot;)
       torch.cuda.memory._dump_snapshot(f&quot;{file_prefix}.pickle&quot;)
   except Exception as e:
       logger.error(f&quot;Failed to capture memory snapshot {e}&quot;)
       return

# This function will leak tensors due to the reference cycles.
def simple_leak(tensor_size, gc_interval=None, num_iter=30000, device=&quot;cuda:0&quot;):
    class Node:
        def __init__(self, T):
            self.tensor = T
            self.link = None

    for i in range(num_iter):
        A = torch.zeros(tensor_size, device=device)
        B = torch.zeros(tensor_size, device=device)
        a, b = Node(A), Node(B)
        # A reference cycle will force refcounts to be non-zero, when
        # a and b go out of scope.
        a.link, b.link = b, a
        # Python will eventually gc a and b, but may OOM on the CUDA
        # device before that happens (since python runtime doesn't
        # know about CUDA memory usage).

        # Since implicit gc is not called frequently enough due to
        # generational gc, adding an explicit gc is necessary as Python
        # runtime does not know about CUDA memory pressure.
        # https://en.wikipedia.org/wiki/Tracing_garbage_collection#Generational_GC_(ephemeral_GC)
        if gc_interval and i % int(gc_interval) == 0:
            gc.collect()

async def awaitable_leak(
    tensor_size, gc_interval=None, num_iter=100000, device=&quot;cuda:0&quot;
):
    class AwaitableTensor:
        def __init__(self, tensor_size, device) -&amp;gt; None:
            self._tensor_size = tensor_size
            self._device = device
            self._tensor = None

        def wait(self) -&amp;gt; torch.Tensor:
            self._tensor = torch.zeros(self._tensor_size, device=self._device)
            return self._tensor

    class AwaitableTensorWithViewCallBack:
        def __init__(
            self,
            tensor_awaitable: AwaitableTensor,
            view_dim: int,
        ) -&amp;gt; None:
            self._tensor_awaitable = tensor_awaitable
            self._view_dim = view_dim
            # Add a view filter callback to the tensor.
            self._callback = lambda ret: ret.view(-1, self._view_dim)

        def wait(self) -&amp;gt; torch.Tensor:
            return self._callback(self._tensor_awaitable.wait())

    for i in range(num_iter):
        # Create an awaitable tensor
        a_tensor = AwaitableTensor(tensor_size, device)

        # Apply a view filter callback on the awaitable tensor.
        AwaitableTensorWithViewCallBack(a_tensor, 4).wait()

        # a_tensor will go out of scope.

        if gc_interval and i % int(gc_interval) == 0:
            gc.collect()

if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser(description=&quot;A memory_leak binary instance&quot;)
    parser.add_argument(
        &quot;--gc_collect_interval&quot;,
        default=None,
        help=&quot;Explicitly call GC every given interval. Default is off.&quot;,
    )
    parser.add_argument(
        &quot;--workload&quot;,
        default=&quot;simple&quot;,
        help=&quot;Toggle which memory leak workload to run. Options are simple, awaitable.&quot;,
    )
    parser.add_argument(
        &quot;--warn_tensor_cycles&quot;,
        action=&quot;store_true&quot;,
        default=False,
        help=&quot;Toggle whether to enable reference cycle detector.&quot;,
    )
    args = parser.parse_args()

    if args.warn_tensor_cycles:
        from tempfile import NamedTemporaryFile

        from torch.utils.viz._cycles import observe_tensor_cycles

        logger.info(&quot;Enabling warning for Python reference cycles for CUDA Tensors.&quot;)

        def write_and_log(html):
            with NamedTemporaryFile(&quot;w&quot;, suffix=&quot;.html&quot;, delete=False) as f:
                f.write(html)
                logger.warning(
                    &quot;Reference cycle includes a CUDA Tensor see visualization of cycle %s&quot;,
                    f.name,
                )

        observe_tensor_cycles(write_and_log)
    else:
        # Start recording memory snapshot history
        start_record_memory_history()

    # Run the workload with a larger tensor size.
    # For smaller sizes, we will not CUDA OOM as gc will kick in often enough
    # to reclaim reference cycles before an OOM occurs.
    size = 2**26  # 256 MB
    try:
        if args.workload == &quot;awaitable&quot;:
            size *= 2
            logger.info(f&quot;Running tensor_size: {size*4/1024/1024} MB&quot;)
            asyncio.run(
                awaitable_leak(tensor_size=size, gc_interval=args.gc_collect_interval)
            )
        elif args.workload == &quot;simple&quot;:
            logger.info(f&quot;Running tensor_size: {size*4/1024/1024} MB&quot;)
            simple_leak(tensor_size=size, gc_interval=args.gc_collect_interval)
        else:
            raise Exception(&quot;Unknown workload.&quot;)
    except Exception:
        logger.exception(f&quot;Failed to allocate {size*4/1024/1024} MB&quot;)

    # Create the memory snapshot file
    export_memory_snapshot()

    # Stop recording memory snapshot history
    stop_record_memory_history()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Aaron Shi, Zachary DeVito</name>
        
        
      </author>

      

      

      
        <summary type="html">This is part 2 of the Understanding GPU Memory blog series. Our first post Understanding GPU Memory 1: Visualizing All Allocations over Time shows how to use the memory snapshot tool. In this part, we will use the Memory Snapshot to visualize a GPU memory leak caused by reference cycles, and then locate and remove them in our code using the Reference Cycle Detector.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Training Production AI Models with PyTorch 2.0</title>
      <link href="https://pytorch.org/blog/training-production-ai-models/" rel="alternate" type="text/html" title="Training Production AI Models with PyTorch 2.0" />
      <published>2023-12-18T00:00:00-08:00</published>
      <updated>2023-12-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/training-production-ai-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/training-production-ai-models/">&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch 2.0&lt;/a&gt; (abbreviated as PT2) can significantly improve the training and inference performance of an AI model using a compiler called &lt;em&gt;torch.compile&lt;/em&gt; while being 100% backward compatible with PyTorch 1.x. There have been reports on how PT2 improves the performance of common &lt;em&gt;benchmarks&lt;/em&gt; (e.g., &lt;a href=&quot;https://huggingface.co/docs/diffusers/optimization/torch2.0&quot;&gt;huggingface’s diffusers&lt;/a&gt;). In this blog, we discuss our experiences in applying PT2 to &lt;em&gt;production&lt;/em&gt; AI models at Meta.&lt;/p&gt;

&lt;h2 id=&quot;2-background&quot;&gt;2. Background&lt;/h2&gt;

&lt;h3 id=&quot;21-why-is-automatic-performance-optimization-important-for-production&quot;&gt;2.1 Why is automatic performance optimization important for production?&lt;/h3&gt;

&lt;p&gt;Performance is particularly important for production—e.g, even a 5% reduction in the training time of a heavily used model can translate to substantial savings in GPU cost and data-center &lt;em&gt;power&lt;/em&gt;. Another important metric is &lt;em&gt;development efficiency&lt;/em&gt;, which measures how many engineer-months are required to bring a model to production. Typically, a significant part of this bring-up effort is spent on &lt;em&gt;manual&lt;/em&gt; performance tuning such as rewriting GPU kernels to improve the training speed. By providing &lt;em&gt;automatic&lt;/em&gt; performance optimization, PT2 can improve &lt;em&gt;both&lt;/em&gt; cost and development efficiency.&lt;/p&gt;

&lt;h3 id=&quot;22-how-pt2-improves-performance&quot;&gt;2.2 How PT2 improves performance&lt;/h3&gt;

&lt;p&gt;As a compiler, PT2 can view &lt;em&gt;multiple&lt;/em&gt; operations in the training graph captured from a model (unlike in PT1.x, where only one operation is executed at a time). Consequently, PT2 can exploit a number of performance optimization opportunities, including:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Fusing multiple operations into a single GPU kernel:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;A typical type of performance overhead in running a GPU program is the CPU overhead of launching small GPU kernels. By fusing multiple operations into a single GPU kernel, PT2 can significantly reduce the kernel-launching overhead on the CPU. For instance, consider the PyTorch program in Figure 1(a). When it is executed on GPU with PT1, it has three GPU kernels (two for the two sin() ops and one for the addition op). With PT2, there is only one kernel generated, which fuses all three ops.&lt;/li&gt;
      &lt;li&gt;After fusing some operations, certain operations in the graph may become dead and hence can be optimized away. This can save both compute and memory bandwidth on the GPU. For instance, in Figure 1(b), one of the duplicated sin() ops can be optimized away.&lt;/li&gt;
      &lt;li&gt;In addition, fusion can also reduce GPU device memory reads/writes (by composing pointwise kernels) and help improve hardware utilization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig1.jpg&quot; alt=&quot;Fig.1  How PT2 improves performance with fusion and dead-code elimination.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: How PT2 improves performance with fusion and dead-code elimination.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reducing the type conversion overhead for using lower-precision data types:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;PyTorch 1.x supports &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html&quot;&gt;Automatic Mixed Precision (AMP)&lt;/a&gt;.  While AMP can reduce the compute time of an op, it introduces type conversion overhead before and after the op. PT2 can increase AMP performance by optimizing away unnecessary type conversion code, significantly reducing its overhead. As an example, Figure 2(a) converts three 32-bit input tensors (a32, b32, c32) to bf16 before doing the matrix multiplications. Nevertheless, in this example, a32 and c32 are actually the same tensor (a_float32). So, there is no need to convert a_float32 twice, as shown in the code generated by torch.compile in Figure 2(b). Note that while both this example and the previous one optimize away redundant computations, they are different in the sense that the type conversion code in this example is &lt;em&gt;implicit&lt;/em&gt; via torch.autocast, unlike in the previous example where the torch.sin(x).cuda() is &lt;em&gt;explicit&lt;/em&gt; in user code.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig2.jpg&quot; alt=&quot;Fig.2  How PT2 reduces type conversion overhead when using AMP.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: How PT2 reduces type conversion overhead when using AMP.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Reusing buffers on the GPU:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;With a global view, the scheduler in torch.compile can reuse buffers on the GPU, thereby reducing both memory allocation time and memory consumption. Figure 3 shows the driver program that calls the Triton kernels generated for the program in Figure 2(a). We can see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;buf1&lt;/code&gt; is reused as&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt; buf4&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig3.jpg&quot; alt=&quot;Fig.3  Reuse of buffers.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 3&lt;/strong&gt;: Reuse of buffers.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Autotuning:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;PT2 has options to enable autotuning (via Triton) on matrix-multiply ops, pointwise ops, and reduction ops. Tunable parameters include block size, number of stages, and number of warps. With autotuning, the most performant implementation of an op can be found empirically.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-production-environment-considerations&quot;&gt;3. Production environment considerations&lt;/h2&gt;

&lt;p&gt;In this section, we describe a number of important considerations in applying PT2 to production.&lt;/p&gt;

&lt;h3 id=&quot;31--ensuring-no-model-quality-degradation-with-torchcompile&quot;&gt;3.1  Ensuring no model quality degradation with torch.compile&lt;/h3&gt;

&lt;p&gt;Applying torch.compile to a model will cause numerical changes because of (1) reordering of floating-point ops during various optimizations such as fusion and (2) use of lower precision data types like bf16 if AMP is enabled.  Therefore 100% bitwise compatibility with PT 1.x is not expected. Nevertheless, we still need to make sure that the model quality (measured in some form of numeric scores) is preserved after applying torch.compile.  Typically, each production model will have its own range of acceptable scores (e.g., percentage change must be within 0.01%).&lt;/p&gt;

&lt;p&gt;In case of a model-quality drop caused by torch.compile, we need to do a deep-dive debug.&lt;/p&gt;

&lt;p&gt;One useful technique for debugging a torch.compile-related numeric issue is to apply torch.compile with different backends, in particular “eager” and “aot_eager”, in addition to “inductor”:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If the numeric issue happens with the “eager” backend, then the forward graph constructed by torch.compile is likely incorrect;&lt;/li&gt;
  &lt;li&gt;If the numeric issue doesn’t happen with “eager” but happens with “aot_eager”, then the backward graph constructed by torch.compile is likely incorrect;&lt;/li&gt;
  &lt;li&gt;If the numeric issue doesn’t happen with either “eager” or “aot_eager” but happens with “inductor”, then the code generation inside the inductor is likely incorrect.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;32-autotuning-in-production&quot;&gt;3.2 Autotuning in production&lt;/h3&gt;

&lt;p&gt;By default, the autotuning in torch.inductor is done &lt;em&gt;online&lt;/em&gt; while the model is executed. For some production models, we find that the autotuning time can take several hours, which is not acceptable for production. Therefore, we add &lt;em&gt;offline autotuning&lt;/em&gt; which works as depicted in Figure 4. The very first time that a model is run, the details (e.g., input tensor shape, data type etc) on all ops that require tuning will be logged to a database. Then, a tuning process for these ops is run overnight to search for the most performant implementation of each op;  the search result is updated to a persistent cache (implemented as a source file of torch.inductor). Next time when the model is run again, the tuned implementation of each op will be found in the cache and chosen for execution.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig4.jpg&quot; alt=&quot;Fig.4  The offline autotuning used in production.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 4&lt;/strong&gt;: The offline autotuning used in production.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;33-profiling-support-for-torchcompile&quot;&gt;3.3 Profiling support for torch.compile&lt;/h3&gt;

&lt;p&gt;As we previously discussed in this &lt;a href=&quot;https://pytorch.org/blog/performance-deb&quot;&gt;blog&lt;/a&gt;, a profiler is essential for debugging the performance of production models. We have enhanced the profiler to display torch.compile related events on the timeline. The most useful ones are marking which parts of the model are running compiled code so that we can quickly validate if the parts of the model that are supposed to be compiled are actually compiled by torch.compile. For example, the trace in Figure 5 has two compiled regions (with the label “CompiledFunction”). Other useful events are time spent on the compilation and that spent on accessing the compiler’s code-cache.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig5.jpg&quot; alt=&quot;Fig.5  A trace with two compiled regions.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 5&lt;/strong&gt;: A trace with two compiled regions.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;34-controlling-just-in-time-compilation-time&quot;&gt;3.4 Controlling just-in-time compilation time&lt;/h3&gt;

&lt;p&gt;torch.compile uses just-in-time compilation. The compilation happens when the first batch of data is trained. In our production setting, there is an upper limit on how much time is allowed for a training job to reach its first batch, aka &lt;em&gt;Time-To-First-Batch (TTFB)&lt;/em&gt;.  We need to make sure that enabling torch.compile will not increase TTFB to over the limit. This could be challenging because production models are large and~~ ~~torch.compile can take substantial compilation time. We enable &lt;em&gt;parallel compilation&lt;/em&gt; to keep the compile time under control (this is controlled by the global variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compile_threads&lt;/code&gt; inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch/_inductor/config.py&lt;/code&gt;, which is already set to the CPU count on OSS Linux). A model is decomposed into one or more computational graphs; each graph is decomposed into multiple Triton kernels. If parallel compilation is enabled, all the Triton kernels in the same graph can be compiled simultaneously (nevertheless, kernels from different graphs are still compiled in serial). Figure 6 illustrates how parallel compilation helps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig6.jpg&quot; alt=&quot;Fig.6  Using parallel compilation in production.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 6&lt;/strong&gt;: Using parallel compilation in production.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-results&quot;&gt;4. Results&lt;/h2&gt;

&lt;p&gt;In this section, we use three production models to evaluate PT2. First we show the training time speedups with PT2, using different optimization configs. Second, we show the importance of parallel compilation on the compilation time.&lt;/p&gt;

&lt;h3 id=&quot;41-training-time-speedup-with-torchcompile&quot;&gt;4.1 Training-time speedup with torch.compile&lt;/h3&gt;

&lt;p&gt;Figure 7 reports the training-time speedup with PT2. For each model, we show four cases: (i) no-compile with bf16, (ii) compile with fp32, (iii) compile with bf16, (iv) compile with bf16 and autotuning. The y-axis is the speedup over the baseline, which is no-compile with fp32.  Note that no-compile with bf16 is actually slower than no-compile with fp32, due to the type conversion overhead. In contrast, compiling with bf16 achieves much larger speedups by reducing much of this overhead. Overall, given that these models are already heavily optimized by hand, we are excited to see that torch.compile can still provide 1.14-1.24x speedup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig7.jpg&quot; alt=&quot;Fig.7 Training-time speedup with torch.compile (note: the baseline, no-compile/fp32, is  omitted in this figure).&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 7&lt;/strong&gt;: Training-time speedup with torch.compile (note: the baseline, no-compile/fp32, is omitted in this figure).&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;42-compilation-time-reduction-with-parallel-compilation&quot;&gt;4.2 Compilation-time reduction with parallel compilation&lt;/h3&gt;

&lt;p&gt;Figure 8 shows the compilation time with and without parallel compilation. While there is still room for improvement on the serial compilation time, parallel compilation has reduced the compilation overhead on TTFB to an acceptable level. Models B and C benefit more from parallel compilation than Model A does because they have more distinct Triton kernels per graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/training-production-ai-models/blog-fig8.jpg&quot; alt=&quot;Fig.8 PT2 compilation time.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 8&lt;/strong&gt;: PT2 compilation time.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-concluding-remarks&quot;&gt;5. Concluding Remarks&lt;/h2&gt;

&lt;p&gt;In this blog, we demonstrate that PT2 can significantly accelerate the training of large and complex production AI models with reasonable compilation time. In our next blog, we will discuss how PT2 can do general graph transformations.&lt;/p&gt;

&lt;h2 id=&quot;6-acknowledgements&quot;&gt;6. Acknowledgements&lt;/h2&gt;

&lt;p&gt;Many thanks to &lt;a href=&quot;mailto:marksaroufim@meta.com&quot;&gt;Mark Saroufim&lt;/a&gt;, &lt;a href=&quot;mailto:adnanaziz@fb.com&quot;&gt;Adnan Aziz&lt;/a&gt;, and &lt;a href=&quot;mailto:gchanan@meta.com&quot;&gt;Gregory Chanan&lt;/a&gt; for their detailed and insightful reviews.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>CK Luk, Daohang Shi, Yuzhen Huang, Jackie (Jiaqi) Xu, Jade Nie, Zhou Wang, Lu Fang, Flavio Sales Truzzi, Devashish Shankar, Dima Ivashchenko, Chunzhi Yang, Nicolas Macchioni, David Berard, Yu Guo, Xiaodong Wang, Bert Maher, Yanbo Liang, Edward Yang, Brian Hirsh, Michael Voznesensky, Animesh Jain, Michael Anderson</name>
        
        
      </author>

      

      

      
        <summary type="html">1. Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Empowering Models with Performance: The Art of Generalized Model Transformation Approach</title>
      <link href="https://pytorch.org/blog/empowering-models-performance/" rel="alternate" type="text/html" title="Empowering Models with Performance: The Art of Generalized Model Transformation Approach" />
      <published>2023-12-15T00:00:00-08:00</published>
      <updated>2023-12-15T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/empowering-models-performance</id>
      <content type="html" xml:base="https://pytorch.org/blog/empowering-models-performance/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;PyTorch 2.0&lt;/a&gt; (PT2) offers a compiled execution mode which rewrites Python bytecode to extract sequences of PyTorch operations, translating them into a Graph IR. The IR is then just-in-time compiled through a customizable back end, improving training performance without user interference. Often, production models may go through multiple stages of optimization/lowering to hit performance targets. Therefore, having a compiled mode is desirable as it can separate the work of improving model performance from direct modification of the PyTorch model implementation. Thus, the compiled mode becomes more important, enabling Pytorch users to enhance model performance without modifying the PyTorch code implementation. This feature is particularly valuable for optimizing complex models, including large-scale and production-ready ones.&lt;/p&gt;

&lt;p&gt;In our previous &lt;a href=&quot;https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/&quot;&gt;blog post&lt;/a&gt; , we outlined how heuristic model transformation rules are employed to optimize intricate production models. While these rules enabled substantial performance gains for some pilot models, they lacked universal adaptability; they don’t consistently perform well across different models or sometimes even within different sections of a single model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/empowering-models-performance/fig1.jpg&quot; alt=&quot;Fig.1 PT1 Graph mode vs PT2 Compile mode.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 1&lt;/strong&gt;: PT1 Graph mode vs PT2 Compile mode.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;In this blog post, we propose a more generalized model transformation solution, serving as a plugin to the PT2 compiler as shown in Fig.1 which is more general, performant and user-friendly, bringing performance improvements to both model training and inference without manual efforts. As illustrated in Fig.2, by incorporating the previously user-defined transformations into the compiler, we have streamlined the production stack. These changes bring advantages to a broader range of PyTorch models, extending beyond just Meta models,  which has already been incorporated in PT2 and is ready for use to benefit all Pytorch models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/empowering-models-performance/fig2.jpg&quot; alt=&quot;Fig.2 Simplified stack with PT2 compile mode.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 2&lt;/strong&gt;: Simplified stack with PT2 compile mode.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h2 id=&quot;guiding-principle-atomic-rules&quot;&gt;Guiding Principle: Atomic Rules&lt;/h2&gt;

&lt;p&gt;Traditionally, people might use predefined heuristic rules to replace a model subgraph with another more performant subgraph toreduce launch overhead, minimize memory bw, and fully occupy SMs. However, this approach doesn’t scale well as it is hard to craft a set of rules that fits all models perfectly.&lt;/p&gt;

&lt;p&gt;Instead of grappling with bulky, complex rules, we can actually break them down into smaller, more digestible pieces – what we call ‘&lt;strong&gt;atomic rules&lt;/strong&gt;’. These tiny powerhouses of efficiency target the transformation of individual operators, to conduct one step of the fusion/transformation. This makes them easy to handle and apply, offering a straightforward path to optimizing models. So, with these atomic rules in hand, optimizing any model for top-tier performance becomes a breeze!&lt;/p&gt;

&lt;p&gt;We will walk through some simple examples to demonstrate how we use a chain of atomic rules to replace complicated heuristic rules.&lt;/p&gt;

&lt;h3 id=&quot;case-1-horizontal-fusion-of-computation-chains-started-with-accesses-to-embedding-tables&quot;&gt;Case 1: Horizontal fusion of computation chains started with accesses to embedding tables&lt;/h3&gt;

&lt;p&gt;Horizontal fusion means fusing parallel operators into one so as to reduce the number of kernels to be launched and improve performance. In our previous blog (&lt;a href=&quot;https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/#32-horizontal-fusion-of-computation-chains-started-with-accesses-to-embedding-tables&quot;&gt;Section 3.2&lt;/a&gt;), we described model transformations that fused layernorm and activation functions after embedding bags, as shown in the figure provided. However, this method, had limitations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;It only worked with layernorm and activation functions after embedding.&lt;/li&gt;
  &lt;li&gt;It was restricted to models with specific architecture rules, causing various issues in our production stack, including parameter changes and inference disruptions.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To improve, we can use three atomic rules as shown in Fig.3 to replace the complicated heuristic rule:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fuse layernorms that follow the same split nodes horizontally.&lt;/li&gt;
  &lt;li&gt;Then, fuse tanh functions following the same split nodes horizontally.&lt;/li&gt;
  &lt;li&gt;Lastly, fuse vertical split-cat nodes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These atomic rules offer a clean and streamlined way for model simplification and optimization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/empowering-models-performance/fig3.jpg&quot; alt=&quot;Fig.3 Before, we optimized the model in one go by replacing subgraphs. Now, with atomic rules, we optimize step-by-step, covering more cases.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 3&lt;/strong&gt;: Before, we optimized the model in one go by replacing subgraphs. Now, with atomic rules, we optimize step-by-step, covering more cases.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3 id=&quot;case-2-fuse-horizontal-mlp&quot;&gt;Case 2: Fuse horizontal MLP&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Multilayer_perceptron&quot;&gt;MLP&lt;/a&gt;s (Multilayer Perceptrons) are fundamental components of deep neural networks, often consisting of linear, normalization, and activation functions. In complex models, there’s often a need to fuse many horizontal MLPs. Traditional methods find and replace parallel MLPs with a fused module as shown in Fig.4,  but this isn’t always straightforward. Some models might not have normalization, or they might use different activation functions, making it hard to apply a one-size-fits-all rule.&lt;/p&gt;

&lt;p&gt;This is where our atomic rules come in handy. These simplified rules target individual operators one at a time, making the process easier and more manageable. We use the following atomic rules for horizontal MLP fusion:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fusing horizontal linear operators&lt;/li&gt;
  &lt;li&gt;Fusing horizontal layernorms.&lt;/li&gt;
  &lt;li&gt;Fusing horizontal activation functions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/empowering-models-performance/fig4.jpg&quot; alt=&quot;Fig.4 Pseudocode for fusing MLP. Traditional optimizations need manual Python code changes.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 4&lt;/strong&gt;: Pseudocode for fusing MLP. Traditional optimizations need manual Python code changes.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;The beauty of these rules is that they’re not limited to one case. They can be applied broadly. Since PyTorch models are built with torch operators, focusing on a smaller set of operators simplifies the process. This approach is not only more manageable but also more general compared to writing a specific large pattern replacement rule, making it easier to optimize various models efficiently.&lt;/p&gt;

&lt;h2 id=&quot;compile-time-graph-search&quot;&gt;Compile-time Graph Search&lt;/h2&gt;

&lt;p&gt;Our principle is to use chained atomic rules to replace heuristic rules. While this approach covers a wider range of cases, it does entail a longer time for graph search and pattern matching. The next question is: how can we minimize compilation time while performing compile-time graph searches efficiently?&lt;/p&gt;

&lt;p&gt;We design a two-step greedy algorithm as illustrated in Fig. 5. The first step in this process is to identify the target nodes, which we follow certain rules, e.g., identifying all linear operations with the same input shapes. Once identified, we use a Breadth-First Search (BFS) strategy to separate these nodes into different sets, so that nodes within a set don’t have data dependency. The nodes within each of these sets are independent and can be fused horizontally.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/empowering-models-performance/fig5.jpg&quot; alt=&quot;Fig.5 Process of model transformation with graph IR.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p style=&quot;line-height: 1.05&quot;&gt;&lt;small&gt;&lt;em&gt;&lt;strong&gt;Fig. 5&lt;/strong&gt;: Process of model transformation with graph IR.&lt;/em&gt;&lt;/small&gt;&lt;/p&gt;

&lt;p&gt;With our approach, the search time is roughly 60 seconds for one of our largest internal models, which is manageable for on-the-fly tasks.&lt;/p&gt;

&lt;h2 id=&quot;in-the-end&quot;&gt;In the End&lt;/h2&gt;

&lt;p&gt;In our tests with internal ranking models, we observed approximately 5% to 15% training performance improvement across five models on top of the performance gain brought by torch.compile. We have enabled the optimization in PT2 compiler stack and landed it as default when users choose Inductor as the backend (&lt;a href=&quot;https://github.com/pytorch/pytorch/blob/53acdb66f7ed31919cf69cf62e6ee0f13287be7e/torch/_inductor/config.py#L90&quot;&gt;config&lt;/a&gt;). We expect our generalized transformation approach could benefit models beyond Meta, and look forward to more discussion and improvement through this compiler level transformation framework.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Many thanks to  Mark Saroufim, Gregory Chanan, Adnan Aziz, and Rocky Liu for their detailed and insightful reviews.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Jackie (Jiaqi) Xu, Yanbo Liang, Jason Ansel, Chunzhi Yang, Jade Nie, Yuzhen Huang, CK Luk, Xiaodong Wang, Lu Fang, Menglu Yu, Jinwon Lee, Daohang Shi, Flavio Sales Truzzi</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Understanding GPU Memory 1: Visualizing All Allocations over Time</title>
      <link href="https://pytorch.org/blog/understanding-gpu-memory-1/" rel="alternate" type="text/html" title="Understanding GPU Memory 1: Visualizing All Allocations over Time" />
      <published>2023-12-14T00:00:00-08:00</published>
      <updated>2023-12-14T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/understanding-gpu-memory-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/understanding-gpu-memory-1/">&lt;p&gt;During your time with PyTorch on GPUs, you may be familiar with this common error message:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 79.32 GiB of which 401.56 MiB is free.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this series, we show how to use memory tooling, including the Memory Snapshot, the Memory Profiler, and the Reference Cycle Detector to debug out of memory errors and improve memory usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig1.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Memory Snapshot&lt;/strong&gt; tool provides a fine-grained GPU memory visualization for debugging GPU OOMs. Captured memory snapshots will show memory events including allocations, frees and OOMs, along with their stack traces.&lt;/p&gt;

&lt;p&gt;In a snapshot, each tensor’s memory allocation is color coded separately. The x axis is over time, and the y axis is the amount of GPU memory in MB. The snapshot is interactive, so we can observe the stack trace for any allocation by mousing over. Try it yourself at &lt;a href=&quot;https://github.com/pytorch/pytorch.github.io/blob/site/assets/images/understanding-gpu-memory-1/snapshot.html&quot;&gt;https://github.com/pytorch/pytorch.github.io/blob/site/assets/images/understanding-gpu-memory-1/snapshot.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this snapshot, there are 3 peaks showing the memory allocations over 3 training iterations (this is configerable). When looking at the peaks, it is &lt;strong&gt;easy to see the rise of memory in the forward&lt;/strong&gt; &lt;strong&gt;pass&lt;/strong&gt; and the &lt;strong&gt;fall during the backward pass&lt;/strong&gt; as the gradients are computed. It is also possible to see that the program has the &lt;strong&gt;same pattern of memory use iteration to iteration&lt;/strong&gt;. One thing that stands out is the many &lt;strong&gt;tiny spikes in memory&lt;/strong&gt;, by mousing over them, we see that they are buffers used temporarily by convolution operators.&lt;/p&gt;

&lt;h3 id=&quot;capturing-memory-snapshots&quot;&gt;Capturing Memory Snapshots&lt;/h3&gt;

&lt;p&gt;The API to capture memory snapshots is fairly simple and available in torch.cuda.memory:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Start:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._record_memory_history(max_entries=100000)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Save:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._dump_snapshot(file_name)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Stop:&lt;/strong&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.cuda.memory._record_memory_history(enabled=None)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Code Snippet&lt;/strong&gt; (for full code sample, see &lt;strong&gt;Appendix A&lt;/strong&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   # Start recording memory snapshot history, initialized with a buffer
   # capacity of 100,000 memory events, via the `max_entries` field.
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

   # Run your PyTorch Model.
   # At any point in time, save a snapshot to file for later.
   for _ in range(5):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # In this sample, we save the snapshot after running 5 iterations.
   #   - Save as many snapshots as you'd like.
   #   - Snapshots will save last `max_entries` number of memory events
   #     (100,000 in this example).
   try:
       torch.cuda.memory._dump_snapshot(f&quot;{file_prefix}.pickle&quot;)
   except Exception as e:
       logger.error(f&quot;Failed to capture memory snapshot {e}&quot;)

   # Stop recording memory snapshot history.
   torch.cuda.memory._record_memory_history(enabled=None)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To visualize the snapshot file, we have a tool hosted at &lt;a href=&quot;https://pytorch.org/memory_viz&quot;&gt;https://pytorch.org/memory_viz&lt;/a&gt;. There, you can drag and drop your saved snapshot file and it will plot each allocation over time. &lt;strong&gt;Privacy Note:&lt;/strong&gt; The tool will not save your snapshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig2.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Alternatively, you can generate an HTML from a .pickle by using the script at pytorch/torch/cuda/_memory_viz.py, here is an example:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python torch/cuda/_memory_viz.py trace_plot snapshot.pickle -o snapshot.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;debugging-cuda-ooms&quot;&gt;Debugging CUDA OOMs&lt;/h2&gt;

&lt;p&gt;Let’s look at how we can use the memory snapshot tool to answer:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Why did a &lt;strong&gt;CUDA OOM&lt;/strong&gt; happen?&lt;/li&gt;
  &lt;li&gt;Where is the &lt;strong&gt;GPU Memory being used&lt;/strong&gt;?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;resnet50-with-a-bug&quot;&gt;ResNet50 with a bug&lt;/h3&gt;

&lt;p&gt;We’ve taken a look at a properly working model in the first snapshot. Now, let’s take a look at a training example with a bug, see snapshot:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig3.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice how the &lt;strong&gt;second iteration uses far more memory&lt;/strong&gt; than the first iteration. If this model were much larger, it could have &lt;strong&gt;CUDA OOM’d in the second iteration&lt;/strong&gt; without much more insight into why.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig4.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When examining this snapshot further, we can clearly see that several tensors are staying alive from the first iteration to the second and later iterations. If we mouse over one of these tensors, it would show a &lt;strong&gt;stack trace suggesting that these were gradient tensors&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;And indeed if we go to the code, we can see that &lt;strong&gt;it doesn’t clear the gradient tensors&lt;/strong&gt;, when it could have &lt;strong&gt;cleared them before the forward&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Before:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        for _ in range(num_iters):
          pred = model(inputs)
          loss_fn(pred, labels).backward()
          optimizer.step()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        for _ in range(num_iters):
          pred = model(inputs)
          loss_fn(pred, labels).backward()
          optimizer.step()
          # Add this line to clear grad tensors
          optimizer.zero_grad(set_to_none=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can simply add an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;optimizer.zero_grad(set_to_none=True)&lt;/code&gt; instruction to clear the gradient tensors from iteration to iteration (more details about why we need to zero the gradients here: &lt;a href=&quot;https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html&quot;&gt;https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This is a simplification of a bug we’ve found in more complicated programs using this tool. We encourage you to try out the Memory Snapshot on your GPU memory problems and let us know how it goes.&lt;/p&gt;

&lt;h3 id=&quot;resnet50-after-bug-fix&quot;&gt;ResNet50 after bug fix&lt;/h3&gt;

&lt;p&gt;After applying the fix, the snapshot seems to be clearing the gradients now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig5.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We now have the snapshot of a properly working ResNet50 model. Try out the code yourself (see code sample in &lt;strong&gt;Appendix A&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;But you may be wondering, &lt;strong&gt;why is there still an increase in memory after the first iteration?&lt;/strong&gt; To answer this, let’s visit the &lt;strong&gt;Memory Profiler&lt;/strong&gt; in the next section.&lt;/p&gt;

&lt;h2 id=&quot;categorized-memory-usage&quot;&gt;Categorized Memory Usage&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Memory Profiler&lt;/strong&gt; is an added feature of the PyTorch Profiler that &lt;strong&gt;categorizes&lt;/strong&gt; memory usage over time. We still rely on the Memory Snapshot for stack traces for deep dives into memory allocations.&lt;/p&gt;

&lt;p&gt;To generate a memory timeline, here is a code snippet (full code sample in &lt;strong&gt;Appendix B&lt;/strong&gt;):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   # Initialize the profiler context with record_shapes, profile_memory,
   # and with_stack set to True.
   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       # Run the PyTorch Model inside the profile context.
       for _ in range(5):
           prof.step()
           with record_function(&quot;## forward ##&quot;):
               pred = model(inputs)

           with record_function(&quot;## backward ##&quot;):
               loss_fn(pred, labels).backward()

           with record_function(&quot;## optimizer ##&quot;):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

   # Construct the memory timeline HTML plot.
   prof.export_memory_timeline(f&quot;{file_prefix}.html&quot;, device=&quot;cuda:0&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For further reference, see &lt;a href=&quot;https://pytorch.org/docs/main/profiler.html&quot;&gt;https://pytorch.org/docs/main/profiler.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Memory Profiler automatically generates categories based on the graph of tensor operations recorded during profiling.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/understanding-gpu-memory-1/fig6.png&quot; alt=&quot;Memory Timeline&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this Memory Timeline collected using the Memory Profiler, we have the same training example as before. We can observe the &lt;strong&gt;gradients in blue are now being cleared&lt;/strong&gt; from iteration to iteration. We can also notice that the &lt;strong&gt;optimizer state in yellow is allocated after the first iteration&lt;/strong&gt;, and is kept constant for the rest of the job.&lt;/p&gt;

&lt;p&gt;This optimizer state is the reason behind the increase of GPU memory from the first iteration to the second. Try out the code yourself (see code sample in &lt;strong&gt;Appendix B&lt;/strong&gt;). The Memory Profiler helps to improve training &lt;strong&gt;memory understanding&lt;/strong&gt; so that model authors can figure out which categories are using the most GPU memory.&lt;/p&gt;

&lt;h2 id=&quot;where-can-i-find-these-tools&quot;&gt;Where can I find these tools?&lt;/h2&gt;

&lt;p&gt;We hope that these tools will greatly improve your ability to debug CUDA OOMs and to understand your memory usage by category.&lt;/p&gt;

&lt;p&gt;The Memory Snapshot and the Memory Profiler are available in the v2.1 release of PyTorch as experimental features.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;More information about the Memory Snapshot can be found in the &lt;a href=&quot;https://pytorch.org/docs/main/torch_cuda_memory.html&quot;&gt;PyTorch Memory docs here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;More details about the Memory Profiler can be found in the &lt;a href=&quot;https://pytorch.org/docs/main/profiler.html&quot;&gt;PyTorch Profiler docs here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feedback&quot;&gt;Feedback&lt;/h2&gt;

&lt;p&gt;We look forward to hearing from you about any enhancements, bugs or memory stories that our tools helped to solve! As always, please feel free to open new issues on PyTorch’s Github page.&lt;/p&gt;

&lt;p&gt;We are also open to contributions from the OSS community, feel free to tag &lt;a href=&quot;https://github.com/aaronenyeshi&quot;&gt;Aaron Shi&lt;/a&gt; and &lt;a href=&quot;https://github.com/zdevito&quot;&gt;Zachary DeVito&lt;/a&gt; in any Github PRs for reviews.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Really appreciate the content reviewers, &lt;a href=&quot;mailto:marksaroufim@meta.com&quot;&gt;Mark Saroufim&lt;/a&gt; and &lt;a href=&quot;mailto:gchanan@meta.com&quot;&gt;Gregory Chanan&lt;/a&gt;, for reviewing this post and improving its readability.&lt;/p&gt;

&lt;p&gt;Really appreciate the code reviews and feedback from &lt;a href=&quot;mailto:adnanaziz@meta.com&quot;&gt;Adnan Aziz&lt;/a&gt; and &lt;a href=&quot;mailto:ltian@meta.com&quot;&gt;Lei Tian&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;appendix-a---resnet50-memory-snapshot-code-example&quot;&gt;Appendix A - ResNet50 Memory Snapshot Code Example&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# (c) Meta Platforms, Inc. and affiliates. 
import logging
import socket
from datetime import datetime, timedelta

import torch

from torchvision import models

logging.basicConfig(
   format=&quot;%(levelname)s:%(asctime)s %(message)s&quot;,
   level=logging.INFO,
   datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = &quot;%b_%d_%H_%M_%S&quot;

# Keep a max of 100,000 alloc/free events in the recorded history
# leading up to the snapshot.
MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT: int = 100000

def start_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Starting snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(
       max_entries=MAX_NUM_OF_MEM_EVENTS_PER_SNAPSHOT
   )

def stop_record_memory_history() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not recording memory history&quot;)
       return

   logger.info(&quot;Stopping snapshot record_memory_history&quot;)
   torch.cuda.memory._record_memory_history(enabled=None)

def export_memory_snapshot() -&amp;gt; None:
   if not torch.cuda.is_available():
       logger.info(&quot;CUDA unavailable. Not exporting memory snapshot&quot;)
       return

   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f&quot;{host_name}_{timestamp}&quot;

   try:
       logger.info(f&quot;Saving snapshot to local file: {file_prefix}.pickle&quot;)
       torch.cuda.memory._dump_snapshot(f&quot;{file_prefix}.pickle&quot;)
   except Exception as e:
       logger.error(f&quot;Failed to capture memory snapshot {e}&quot;)
       return

# Simple Resnet50 example to demonstrate how to capture memory visuals.
def run_resnet50(num_iters=5, device=&quot;cuda:0&quot;):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   # Start recording memory snapshot history
   start_record_memory_history()

   for _ in range(num_iters):
       pred = model(inputs)
       loss_fn(pred, labels).backward()
       optimizer.step()
       optimizer.zero_grad(set_to_none=True)

   # Create the memory snapshot file
   export_memory_snapshot()

   # Stop recording memory snapshot history
   stop_record_memory_history()

if __name__ == &quot;__main__&quot;:
    # Run the resnet50 model
    run_resnet50()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;appendix-b---resnet50-memory-profiler-code-example&quot;&gt;Appendix B - ResNet50 Memory Profiler Code Example&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# (c) Meta Platforms, Inc. and affiliates. 
import logging
import socket
from datetime import datetime, timedelta

import torch

from torch.autograd.profiler import record_function
from torchvision import models

logging.basicConfig(
   format=&quot;%(levelname)s:%(asctime)s %(message)s&quot;,
   level=logging.INFO,
   datefmt=&quot;%Y-%m-%d %H:%M:%S&quot;,
)
logger: logging.Logger = logging.getLogger(__name__)
logger.setLevel(level=logging.INFO)

TIME_FORMAT_STR: str = &quot;%b_%d_%H_%M_%S&quot;

def trace_handler(prof: torch.profiler.profile):
   # Prefix for file names.
   host_name = socket.gethostname()
   timestamp = datetime.now().strftime(TIME_FORMAT_STR)
   file_prefix = f&quot;{host_name}_{timestamp}&quot;

   # Construct the trace file.
   prof.export_chrome_trace(f&quot;{file_prefix}.json.gz&quot;)

   # Construct the memory timeline file.
   prof.export_memory_timeline(f&quot;{file_prefix}.html&quot;, device=&quot;cuda:0&quot;)

def run_resnet50(num_iters=5, device=&quot;cuda:0&quot;):
   model = models.resnet50().to(device=device)
   inputs = torch.randn(1, 3, 224, 224, device=device)
   labels = torch.rand_like(model(inputs))
   optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
   loss_fn = torch.nn.CrossEntropyLoss()

   with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=0, warmup=0, active=6, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,
       on_trace_ready=trace_handler,
   ) as prof:
       for _ in range(num_iters):
           prof.step()
           with record_function(&quot;## forward ##&quot;):
               pred = model(inputs)

           with record_function(&quot;## backward ##&quot;):
               loss_fn(pred, labels).backward()

           with record_function(&quot;## optimizer ##&quot;):
               optimizer.step()
               optimizer.zero_grad(set_to_none=True)

if __name__ == &quot;__main__&quot;:
    # Warm up
    run_resnet50()
    # Run the resnet50 model
    run_resnet50()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name>Aaron Shi, Zachary DeVito</name>
        
        
      </author>

      

      

      
        <summary type="html">During your time with PyTorch on GPUs, you may be familiar with this common error message:</summary>
      

      
      
    </entry>
  
</feed>



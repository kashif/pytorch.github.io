<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-10-25T08:25:50-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Intel GPU Support Now Available in PyTorch 2.5</title>
      <link href="https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/" rel="alternate" type="text/html" title="Intel GPU Support Now Available in PyTorch 2.5" />
      <published>2024-10-25T00:00:00-07:00</published>
      <updated>2024-10-25T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/intel-gpu-support-pytorch-2-5</id>
      <content type="html" xml:base="https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/">&lt;p&gt;Support for Intel GPUs is now available in PyTorch® 2.5, providing improved functionality and performance for Intel GPUs which including &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html&quot;&gt;Intel® Arc™ discrete graphics&lt;/a&gt;, &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/processors/core-ultra.html&quot;&gt;Intel® Core™ Ultra processors&lt;/a&gt; with built-in Intel® Arc™ graphics and &lt;a href=&quot;https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html&quot;&gt;Intel® Data Center GPU Max Series&lt;/a&gt;. This integration brings Intel GPUs and the SYCL* software stack into the official PyTorch stack, ensuring a consistent user experience and enabling more extensive AI application scenarios, particularly in the AI PC domain.&lt;/p&gt;

&lt;p&gt;Developers and customers building for and using Intel GPUs will have a better user experience by directly obtaining continuous software support from native PyTorch, unified software distribution, and consistent product release time.&lt;/p&gt;

&lt;p&gt;Furthermore, Intel GPU support provides more choices to users. Now PyTorch provides a consistent GPU programming paradigm on both front ends and back ends. Developers can now run and deploy workloads on Intel GPUs with minimal coding efforts.&lt;/p&gt;

&lt;h2 id=&quot;overview-of-intel-gpu-support&quot;&gt;&lt;strong&gt;Overview of Intel GPU support&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Intel GPU support in PyTorch provides eager mode and graph mode support in the PyTorch built-in front end. Eager mode now has an implementation of commonly used Aten operators with the SYCL programming language. Graph mode (torch.compile) now has an enabled Intel GPU back end to implement the optimization for Intel GPUs and to integrate Triton. &lt;/p&gt;

&lt;p&gt;Essential components of Intel GPU support were added to PyTorch, including runtime, Aten operators, oneDNN, TorchInductor, Triton and Intel GPU tool chains integration. Meanwhile, quantization and distributed are being actively developed in preparation for the PyTorch 2.6 release.&lt;/p&gt;

&lt;h2 id=&quot;features&quot;&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In addition to providing key features for Intel® Client GPUs and Intel® Data Center GPU Max Series for inference and training, PyTorch keeps the same user experience as other hardware the PyTorch supports. If you migrate code from CUDA*, you can run the existing application code on an Intel GPU with minimal code changes for the device name (from cuda to xpu). For example:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;# CUDA Code&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;tensor&lt;/strong&gt; &lt;strong&gt;=&lt;/strong&gt; &lt;strong&gt;torch.tensor([&lt;/strong&gt;1.0&lt;strong&gt;,&lt;/strong&gt; 2.0&lt;strong&gt;]).to(&lt;/strong&gt;“cuda”&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;# Code for Intel GPU&lt;/em&gt;&lt;br /&gt;
&lt;strong&gt;tensor&lt;/strong&gt; &lt;strong&gt;=&lt;/strong&gt; &lt;strong&gt;torch.tensor([&lt;/strong&gt;1.0&lt;strong&gt;,&lt;/strong&gt; 2.0&lt;strong&gt;]).to(&lt;/strong&gt;“xpu”&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;PyTorch 2.5 features with an Intel GPU include: &lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inference and training workflows.&lt;/li&gt;
  &lt;li&gt;Enhance both torch.compile and eager mode functionalities (more Ops), together with performance improvement, and fully run three Dynamo Hugging Face*, TIMM* and TorchBench* benchmarks for eager and compile modes. &lt;/li&gt;
  &lt;li&gt;Data types such as FP32, BF16, FP16, and automatic mixed precision (AMP).&lt;/li&gt;
  &lt;li&gt;Runs on Intel® Client GPUs and Intel® Data Center GPU Max Series.&lt;/li&gt;
  &lt;li&gt;Supports Linux (Ubuntu, SUSE Linux and Red Hat Linux) and Windows 10/11.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;get-started&quot;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Get a tour of the environment setup, PIP wheels installation, and examples on Intel® Client GPUs and Intel® Data Center GPU Max Series from &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;Getting Started Guide&lt;/a&gt;. Support for Intel GPUs can be experienced through PyTorch PIP wheels installation by nightly and preview binary releases.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Try Intel® Client GPUs through Intel® Arc™ Graphics family (Codename DG2), Intel® Core™ Ultra processor family with Intel® Graphics (Codename Meteor Lake), and Intel® Core™ Ultra mobile processor family with Intel® Graphics (Codename Lunar Lake).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Try Intel Data Center GPU Max Series through &lt;a href=&quot;https://cloud.intel.com/&quot;&gt;Intel® Tiber™ AI Cloud&lt;/a&gt;.&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;To learn how to create a free Standard account, see &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;Get Started&lt;/a&gt;. Then do the following:&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;Sign in to the &lt;a href=&quot;https://console.cloud.intel.com/docs/guides/get_started.html&quot;&gt;cloud console&lt;/a&gt;.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;From the &lt;a href=&quot;https://console.cloud.intel.com/training&quot;&gt;Training&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;section, open the  &lt;a href=&quot;https://console.cloud.intel.com/training/detail/7db2a900-e47d-4b70-8968-cefa08432c1d&quot;&gt;PyTorch on Intel® GPUs&lt;/a&gt;  notebook and click “Launch Jupyter Notebook.”&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Ensure that the &lt;strong&gt;PyTorch 2.5&lt;/strong&gt; kernel is selected for the notebook.&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance&quot;&gt;&lt;strong&gt;Performance&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The performance of Intel GPU on PyTorch was continuously optimized to achieve decent result on three Dynamo Hugging Face, TIMM and TorchBench benchmarks for eager and compile modes.&lt;/p&gt;

&lt;p&gt;The latest performance data measured on top of PyTorch Dynamo Benchmarking Suite using Intel® Data Center GPU Max Series 1100 single card showcase the FP16/BF16 significant speedup ratio over FP32 on eager mode in Figure 1, and Torch.compile mode speedup ratio over eager mode in Figure 2. Both inference and training reached the similar significant improvements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-gains-over-fp32-eager.png&quot; alt=&quot;Figure 2: FP16/BF16 Performance Gains Over FP32 Eager&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: FP16/BF16 Performance Gains Over FP32 Eager&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-gains-over-fp32-eager-2.png&quot; alt=&quot;Figure 3: Torch.compile Performance Gains Over Eager Mode&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3: Torch.compile Performance Gains Over Eager Mode&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Intel GPU on PyTorch 2.5 brings Intel® Client GPUs (Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Arc™ Graphics for dGPU parts) and Intel® Data Center GPU Max Series into the PyTorch ecosystem for AI workload acceleration. Especially, Client GPUs is added to the GPU-supported list for AI PC use scenarios on Windows and Linux environment.&lt;/p&gt;

&lt;p&gt;We warmly welcome the community to evaluate and provide feedback on these enhancements to  &lt;a href=&quot;https://github.com/pytorch/pytorch?tab=readme-ov-file#intel-gpu-support&quot;&gt;Intel GPU support on PyTorch&lt;/a&gt;. &lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;PyTorch Docs: Getting Started on Intel GPU&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.intel.com/&quot;&gt;Intel® Tiber™ AI Cloud&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;&lt;strong&gt;Acknowledgments&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;We want thank PyTorch open source community for their technical discussions and insights: &lt;a href=&quot;https://github.com/atalman&quot;&gt;Andrey Talman&lt;/a&gt;, &lt;a href=&quot;https://github.com/alband&quot;&gt;Alban Desmaison&lt;/a&gt;, &lt;a href=&quot;https://github.com/malfet&quot;&gt;Nikita Shulga&lt;/a&gt;, &lt;a href=&quot;https://github.com/seemethere&quot;&gt;Eli Uriegas&lt;/a&gt;, &lt;a href=&quot;https://github.com/jansel&quot;&gt;Jason Ansel&lt;/a&gt;, and &lt;a href=&quot;https://github.com/desertfire&quot;&gt;Bin Bao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We also thank collaborators from PyTorch for their professional support and guidance.&lt;/p&gt;

&lt;h2 id=&quot;performance-configuration&quot;&gt;&lt;strong&gt;Performance Configuration&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The configurations in the table are collected with &lt;a href=&quot;https://github.com/intel/svr-info&quot;&gt;svr-info&lt;/a&gt;. Test by Intel on September 12, 2024.&lt;/p&gt;

&lt;h2 id=&quot;table-1&quot;&gt;Table 1&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Component&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Details&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel® Max Series GPU 1100 in Intel® Tiber™ Developer Cloud&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Time&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Thu Sep 12 08:21:27 UTC 2024&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro SYS-521GE-TNRT&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Baseboard&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro X13DEG-OA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Chassis&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Supermicro Other&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;CPU Model&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel(R) Xeon(R) Platinum 8468V&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Microarchitecture&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;SPR_XCC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Sockets&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Cores per Socket&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;48&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Hyperthreading&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;CPUs&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;192&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Intel Turbo Boost&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Base Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.4GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;All-core Maximum Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.4GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Maximum Frequency&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2.9GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;NUMA Nodes&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Prefetchers&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled, AMP: Disabled, Homeless: Disabled, LLC: Disabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;PPINs&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5e3f862ef7ba9d50, 6c85812edfcc84b1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Accelerators&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;DLB 2, DSA 2, IAA 2, QAT (on CPU) 2, QAT (on chipset) 0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Installed Memory&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1024GB (16x64GB DDR5 4800 MT/s [4800 MT/s])&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Hugepagesize&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2048 kB&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Transparent Huge Pages&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;madvise&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Automatic NUMA Balancing&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Enabled&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;NIC&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;2 x Ethernet Controller X710 for 10GBASE-T, 4 x MT2892 Family [ConnectX-6 Dx]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Disk&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1 x 894.3G Micron_7450_MTFDKBG960TFR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;BIOS&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;1.4a&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Microcode&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;0x2b0004b1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;OS&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Ubuntu 22.04.2 LTS&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Kernel&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;5.15.0-73-generic&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;TDP&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;330W&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Power &amp;amp; Perf Policy&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Normal (6)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Frequency Governor&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;performance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Frequency Driver&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;acpi-cpufreq&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Max C-State&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;table-2&quot;&gt;Table 2&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Component&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Details&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Single Card&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Intel® Max Series GPU 1100 series on 4th Gen Intel® Xeon® processors of Intel Tiber Developer Cloud&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Workload &amp;amp; version&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Timm ac34701, TorchBench 03cde49, Torchvision d23a6e1, Torchaudio b3f6f51, Transformers 243e186&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Software Stack&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;intel-for-pytorch-gpu-dev 0.5.3, intel-pti-dev 0.9.0, Intel xpu backend for Triton cc981fe&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Framework&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Pytorch 4a3dabd67f8ce63f2fc45f278421cca3cc532cfe&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;GPU driver&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;agama-ci-devel-803.61&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;GFX FW Version&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;PVC2_1.23374&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Notices &amp;amp; Disclaimers&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AI disclaimer:&lt;/strong&gt;&lt;br /&gt;
AI features may require software purchase, subscription or enablement by a software or platform provider, or may have specific configuration or compatibility requirements. Details at  &lt;a href=&quot;http://www.intel.com/AIPC&quot;&gt;www.intel.com/AIPC&lt;/a&gt;. Results may vary.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>PyTorch Team at Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Support for Intel GPUs is now available in PyTorch® 2.5, providing improved functionality and performance for Intel GPUs which including Intel® Arc™ discrete graphics, Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Data Center GPU Max Series. This integration brings Intel GPUs and the SYCL* software stack into the official PyTorch stack, ensuring a consistent user experience and enabling more extensive AI application scenarios, particularly in the AI PC domain.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ExecuTorch Beta: On-Device AI and LLMs, Stability, and Acceleration with Partners</title>
      <link href="https://pytorch.org/blog/executorch-beta/" rel="alternate" type="text/html" title="ExecuTorch Beta: On-Device AI and LLMs, Stability, and Acceleration with Partners" />
      <published>2024-10-24T00:00:00-07:00</published>
      <updated>2024-10-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/executorch-beta</id>
      <content type="html" xml:base="https://pytorch.org/blog/executorch-beta/">&lt;ul&gt;
  &lt;li&gt;ExecuTorch has achieved Beta status with the release of v0.4, providing stable APIs and runtime, as well as extensive kernel coverage.&lt;/li&gt;
  &lt;li&gt;ExecuTorch is the recommended on-device inference engine for Llama 3.2 1B/3B models, offering enhanced performance and memory efficiency for both original and quantized models.&lt;/li&gt;
  &lt;li&gt;There has been a significant increase in adoption and ecosystem growth for ExecuTorch, and the focus is now on improving reliability, performance, and coverage for non-CPU backends as the next steps.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Current On-Device AI Market&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The on-device AI market has been rapidly expanding, and is revolutionizing the way we interact with technology. It is unlocking new experiences, enabling personalization, and reducing latency. Traditionally, computer vision and speech recognition have been the primary use-cases for on-device AI, particularly in IoT, industrial applications, and mobile devices. However, the emergence of Large Language Models (LLMs) has made Generative AI the fastest growing sector in AI, subsequently highlighting the importance of on-device Generative AI. IDC &lt;a href=&quot;https://www.idc.com/getdoc.jsp?containerId=prUS52478124&quot;&gt;forecasts&lt;/a&gt; by 2028, close to 1 billion GenAI capable smartphones being shipped worldwide.&lt;/p&gt;

&lt;p&gt;LLMs are not only getting smaller but more powerful. This has led to the creation of a new class of applications that leverage multiple models for intelligent agents and streamlined workflows. The community is rapidly adopting and contributing to these new models, with quantized versions being created within hours of model release. Several leading technology companies are investing heavily in small LLMs, even deploying Low-Rank Adaptation (LoRA) at scale on-device to transform user experiences.&lt;/p&gt;

&lt;p&gt;However, this rapid progress comes at a cost. The fragmentation of our on-device AI landscape creates complexity and inefficiency when going from model authoring to edge deployment. This is where PyTorch’s &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch&lt;/a&gt; comes in – our Beta announcement marks an important milestone in addressing these challenges and empowering developers to create innovative, AI-powered applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What’s New Today&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It’s been exactly one year since we &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;first open sourced ExecuTorch&lt;/a&gt;, six months since &lt;a href=&quot;https://pytorch.org/blog/executorch-alpha/&quot;&gt;Alpha release&lt;/a&gt;, and today, we’re excited to announce three main developments:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Beta&lt;/strong&gt;. ExecuTorch has reached Beta status starting from v0.4! It is now widely adopted and used in production environments across Meta. Through this adoption process we’ve identified and addressed feature gaps, improved stability, and expanded kernel and accelerator coverage. These improvements make us confident to promote ExecuTorch from &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;Alpha&lt;/a&gt; to &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.4.0&quot;&gt;Beta&lt;/a&gt; status, and we are happy to welcome the community to adopt it in their own production settings. Here are three concrete enhancements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Developers can write application code and include the latest ExecuTorch as a dependency, updating when needed with a clean API contract. This is possible due to our API stabilization efforts, as well as our &lt;a href=&quot;https://pytorch.org/executorch/main/api-life-cycle.html&quot;&gt;explicit API lifecycle&lt;/a&gt; and backwards &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/runtime/COMPATIBILITY.md&quot;&gt;compatibility policy&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Running ExecuTorch on CPUs reached the necessary performance, portability and coverage. In particular, we have implemented more than 85% of all &lt;a href=&quot;https://pytorch.org/executorch/main/ir-ops-set-definition.html&quot;&gt;core ATen operators&lt;/a&gt; as part of our &lt;a href=&quot;https://pytorch.org/executorch/stable/kernel-library-overview.html&quot;&gt;portable CPU kernels library&lt;/a&gt; to ensure running a model on ExecuTorch just works in most cases and making missing ops an exception rather than the norm. Moreover, we integrated and extensively tested our &lt;a href=&quot;https://pytorch.org/executorch/main/native-delegates-executorch-xnnpack-delegate.html&quot;&gt;XNNPACK&lt;/a&gt; delegate for high performance on a wide range of CPU architectures. It is used in a number of production cases today.&lt;/li&gt;
  &lt;li&gt;In addition to the low-level ExecuTorch components for greater portability, we built extensions and higher-level abstractions to support more common use-cases such as &lt;a href=&quot;https://pytorch.org/executorch/main/devtools-overview.html&quot;&gt;developer tooling&lt;/a&gt; to support on-device debugging and profiling, and &lt;a href=&quot;https://pytorch.org/executorch/main/extension-module.html&quot;&gt;Module.h&lt;/a&gt; extension to simplify deployment for mobile devices.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;2. On-Device Large-Language Models (LLMs).&lt;/strong&gt; There has been a growing interest in the community to deploy Large Language Models (LLMs) on edge devices, as it offers improved privacy and offline capabilities. However, these models are quite large, pushing the limits of what is possible. Fortunately, ExecuTorch can support these models, and we’ve enhanced the overall framework with numerous optimizations.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ExecuTorch is the &lt;a href=&quot;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-inference-with-lightweight-models-&quot;&gt;recommended framework&lt;/a&gt; to run latest Llama models on-device with &lt;a href=&quot;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2/#-inference-with-lightweight-models-&quot;&gt;excellent performance&lt;/a&gt; today. The Llama 3.2 1B/3B models are well-suited for mobile deployment, and it is especially true with the official &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/&quot;&gt;quantized 1B/3B model releases&lt;/a&gt; from Meta, as it provides a great balance between performance, accuracy, and size. When deploying Llama 3.2 1B/3B quantized models, decode latency improved by 2.5x and prefill latency improved by 4.2x on average, while model size decreased by 56% and memory usage reduced by 41% on average when benchmarked on Android OnePlus 12 device (we’ve also verified similar relative performance on Samsung S24+ for 1B and 3B, and Samsung S22 for 1B). For Llama 3.2 1B quantized model, for example, ExecuTorch is able to achieve 50.2 tokens/s for decoding and 260 tokens/s for prefill on the OnePlus 12, using the latest CPU kernels from XNNPACK and &lt;a href=&quot;https://community.arm.com/arm-community-blogs/b/ai-and-ml-blog/posts/llm-inference-llama-quantized-models-executorch-kleidiai&quot;&gt;Kleidi libraries&lt;/a&gt;. These quantized models allow developers to integrate LLMs into memory and power-constrained devices while still maintaining quality and safety.&lt;/li&gt;
  &lt;li&gt;One of the value propositions of ExecuTorch is being able to use accelerators on mobile devices seamlessly. In fact, ExecuTorch also showcased accelerators to achieve even greater performance running Llama across &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/apple_ios/LLaMA/docs/delegates/mps_README.md&quot;&gt;Apple MPS backend&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/android/LlamaDemo/docs/delegates/qualcomm_README.md&quot;&gt;Qualcomm AI Accelerator&lt;/a&gt;, and &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/demo-apps/android/LlamaDemo/docs/delegates/mediatek_README.md&quot;&gt;MediaTek AI Accelerator&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;There has been growing community and industry interest in multimodal and beyond text-only LLMs, evidenced by Meta’s &lt;a href=&quot;https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&quot;&gt;Llama 3.2 11B/90B vision models&lt;/a&gt; and open-source models like Llava. We have so far &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llava&quot;&gt;enabled Llava 1.5 7B model on phones via ExecuTorch&lt;/a&gt;, making many optimizations, notably reducing runtime memory from 11GB all the way down to 5GB.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Ecosystem and Community Adoption&lt;/strong&gt;&lt;br /&gt;
Now that ExecuTorch is in Beta, it is mature enough to be used in production. It is being increasingly used at Meta across various product surfaces. For instance, ExecuTorch already powers various ML inference use cases across Meta’s Ray-Ban Meta Smart Glasses and Quest 3 VR headsets as well as Instagram and WhatsApp.&lt;/p&gt;

&lt;p&gt;We also &lt;a href=&quot;https://huggingface.co/docs/transformers/main/en/main_classes/executorch&quot;&gt;partnered with Hugging Face&lt;/a&gt; to provide native ExecuTorch support for models being exported using torch.export. This collaboration ensures exported artifacts can directly be lowered and run efficiently on various mobile and edge devices. Models like gemma-2b and phi3-mini are already supported and more foundational models support is &lt;a href=&quot;https://github.com/huggingface/transformers/issues/32253&quot;&gt;in progress&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With stable APIs and Gen AI support, we’re excited to build and grow ExecuTorch with the community. The on-device AI community is growing rapidly and finding ways to adopt ExecuTorch across various fields. For instance, ExecuTorch is being used in a mobile app built by &lt;a href=&quot;https://digica.com/&quot;&gt;Digica&lt;/a&gt; to streamline inventory management in hospitals. As another example, Software Mansion developed an app, &lt;a href=&quot;https://blog.swmansion.com/eraserai-how-to-create-efficient-app-for-edge-device-04f09aa8072f&quot;&gt;EraserAI&lt;/a&gt;, to remove unwanted objects from a photo with EfficientSAM running on-device with ExecuTorch via Core ML delegate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Towards General Availability (GA):&lt;/strong&gt;&lt;br /&gt;
Since the original release of ExecuTorch alpha, we’ve seen a growing interest within the community in using ExecuTorch in various production environments. To that end, we have made great progress towards more stabilized and matured APIs and have made a significant investment in community support, adoption and contribution to ExecuTorch. As are are getting close to GA, we are investing our efforts in the following areas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Non-CPU backends:&lt;/strong&gt; Bringing non-CPU backends to even greater robustness, coverage and performance is our next goal. From day one of our original launch, we have partnered with Apple (for Core ML and MPS), Arm (for EthosU NPU) and Qualcomm (for Hexagon NPU) on accelerator integration with ExecuTorch, and we’ve since then expanded our partnership to MediaTek (NPU) and Cadence (XTensa DSP). We’re also building &lt;a href=&quot;https://pytorch.org/executorch/stable/native-delegates-executorch-vulkan-delegate.html&quot;&gt;Vulkan GPU&lt;/a&gt; integration in-house. In terms of feature coverage, we’ve successfully implemented the core functionalities with our partners, ensured seamless integration with our developer tooling, and showcased successful LLM integration with many of the accelerators. Our next big step is to thoroughly validate the performance and reliability of the system in real-world, production use-cases. This stage will help us fine-tune the experience and ensure the stability needed for smooth operations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Benchmarking infra&lt;/strong&gt;: As part of our ongoing testing efforts, we’ve developed a benchmarking infrastructure along with a &lt;a href=&quot;https://hud.pytorch.org/benchmark/llms?repoName=pytorch%2Fexecutorch&quot;&gt;public dashboard&lt;/a&gt; to showcase our progress toward on-device model inference benchmarking. This allows us to transparently track and display model coverage across various backends, giving our community real-time insights into how we’re advancing towards our goals.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re excited to share these developments with you and look forward to continued improvements in collaboration with our partners and the community! We welcome community contribution to help us make ExecuTorch the clear choice for deploying AI and LLM models on-device. We invite you to start using ExecuTorch in your on-device projects, or even better consider &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/CONTRIBUTING.md&quot;&gt;contributing&lt;/a&gt; to it. You can also report any issues on our &lt;a href=&quot;https://github.com/pytorch/executorch/issues&quot;&gt;GitHub&lt;/a&gt; page.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">ExecuTorch has achieved Beta status with the release of v0.4, providing stable APIs and runtime, as well as extensive kernel coverage. ExecuTorch is the recommended on-device inference engine for Llama 3.2 1B/3B models, offering enhanced performance and memory efficiency for both original and quantized models. There has been a significant increase in adoption and ecosystem growth for ExecuTorch, and the focus is now on improving reliability, performance, and coverage for non-CPU backends as the next steps.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">TorchRec and FBGEMM 1.0 Stable Release</title>
      <link href="https://pytorch.org/blog/torchrec-fbgemm-1/" rel="alternate" type="text/html" title="TorchRec and FBGEMM 1.0 Stable Release" />
      <published>2024-10-23T00:00:00-07:00</published>
      <updated>2024-10-23T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchrec-fbgemm-1</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchrec-fbgemm-1/">&lt;p&gt;We are happy to announce the stable release, 1.0, for &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/FBGEMM&quot;&gt;FBGEMM&lt;/a&gt;. TorchRec is the PyTorch native recommendation systems library, powered by FBGEMM’s (Facebook GEneral Matrix Multiplication) efficient, low-level kernels.&lt;/p&gt;

&lt;h2 id=&quot;torchrec&quot;&gt;TorchRec&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/blog/introducing-torchrec/&quot;&gt;Initially open sourced in 2022&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; provides common primitives for creating state-of-the-art personalization models:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simple, optimized APIs for distributed training across hundreds of GPUs&lt;/li&gt;
  &lt;li&gt;Advanced sharding techniques for embeddings&lt;/li&gt;
  &lt;li&gt;Modules common in authoring recommendation systems&lt;/li&gt;
  &lt;li&gt;Frictionless path to distributed inference with APIs for quantization and sharding of TorchRec models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since then, TorchRec has matured significantly, with wide internal adoption across many Meta production recommendation models for training and inference, alongside new features such as: &lt;a href=&quot;https://github.com/pytorch/torchrec/releases?page=1&quot;&gt;variable batched embeddings, embedding offloading, zero collision hashing, etc.&lt;/a&gt; Furthermore, TorchRec has a presence outside of Meta, such as &lt;a href=&quot;https://docs.databricks.com/en/machine-learning/train-recommender-models.html&quot;&gt;in recommendation models at Databricks&lt;/a&gt; and in the &lt;a href=&quot;https://github.com/twitter/the-algorithm-ml&quot;&gt;Twitter algorithm&lt;/a&gt;. As a result, standard TorchRec features have been marked as &lt;strong&gt;stable&lt;/strong&gt;, with PyTorch style BC guarantees, and can be seen on the &lt;a href=&quot;https://pytorch.org/torchrec/&quot;&gt;revamped TorchRec documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fbgemm&quot;&gt;FBGEMM&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://pytorch.org/FBGEMM/&quot;&gt;FBGEMM is a library that provides high-performance kernels for CPUs and GPUs&lt;/a&gt;. Since 2018, FBGEMM has supported the efficient execution of Meta-internal and external AI/ML workloads by expanding its scope from &lt;a href=&quot;https://arxiv.org/abs/2101.05615&quot;&gt;performance-critical kernels for inference on CPUs&lt;/a&gt; to more complex sparse operators for both training and inference – and recently for Generative AI – on CPUs and GPUs.&lt;/p&gt;

&lt;p&gt;FBGEMM has been empowering TorchRec through its backend high-performance kernel implementations for recommendation workloads, ranging from embedding bag kernels to jagged tensor operations. Together with TorchRec, we released FBGEMM 1.0, which guarantees the functionality and backward-compatibility of several stable APIs serving its core features with &lt;a href=&quot;https://pytorch.org/FBGEMM/&quot;&gt;enhanced documentation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/&quot;&gt;DLRM (Deep Learning Recommendation Model)&lt;/a&gt; is the standard neural network architecture for powering recommendations at Meta, with categorical features being processed through embeddings, while continuous (dense) features are processed with a bottom multilayer perceptron. The following diagram depicts the basic architecture of DLRM, with a second order interaction layer between the dense and sparse features and a top MLP for generating the prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchrec-1.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TorchRec provides standardized modules with significant optimizations in fusing embedding lookups. EBC is a traditional PyTorch embedding module implementation, containing a collection of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.EmbeddingBags.&lt;/code&gt; FusedEBC, powered by FBGEMM for high performance operations on embedding tables with a fused optimizer and UVM caching/management for alleviating memory constraints, is the optimized version present in sharded TorchRec modules for distributed training and inference. The below benchmark demonstrates the vast performance improvements of FusedEBC in comparison to a traditional PyTorch embedding module implementation (EBC) and the ability for FusedEBC to handle much larger embeddings than what is available on GPU memory with UVM caching.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/torchrec-2.png&quot; alt=&quot;performance chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;torchrec-data-types&quot;&gt;TorchRec Data Types&lt;/h2&gt;

&lt;p&gt;TorchRec provides standard &lt;a href=&quot;https://pytorch.org/torchrec/datatypes-api-reference.html&quot;&gt;data types&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/torchrec/modules-api-reference.html&quot;&gt;modules&lt;/a&gt; for easy handling of distributed embeddings. Here is a simple example setting up a collection of embedding tables through TorchRec:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec import EmbeddingBagCollection
from torchrec import KeyedJaggedTensor
from torchrec import JaggedTensor

ebc = torchrec.EmbeddingBagCollection(
    device=&quot;cpu&quot;,
    tables=[
        torchrec.EmbeddingBagConfig(
            name=&quot;product_table&quot;,
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=[&quot;product&quot;],
            pooling=torchrec.PoolingType.SUM,
        ),
        torchrec.EmbeddingBagConfig(
            name=&quot;user_table&quot;,
            embedding_dim=64,
            num_embeddings=4096,
            feature_names=[&quot;user&quot;],
            pooling=torchrec.PoolingType.SUM,
        )
    ]
)

product_jt = JaggedTensor(
    values=torch.tensor([1, 2, 1, 5]), lengths=torch.tensor([3, 1])
)
user_jt = JaggedTensor(values=torch.tensor([2, 3, 4, 1]), lengths=torch.tensor([2, 2]))

kjt = KeyedJaggedTensor.from_jt_dict({&quot;product&quot;: product_jt, &quot;user&quot;: user_jt})

print(&quot;Call EmbeddingBagCollection Forward: &quot;, ebc(kjt))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;sharding&quot;&gt;Sharding&lt;/h2&gt;

&lt;p&gt;TorchRec provides a planner class that automatically generates an optimized sharding plan across many GPUs. Here we demonstrate generating a sharding plan across two GPUs:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology

planner = EmbeddingShardingPlanner(
    topology=Topology(
        world_size=2,
        compute_device=&quot;cuda&quot;,
    )
)

plan = planner.collective_plan(ebc, [sharder], pg)

print(f&quot;Sharding Plan generated: {plan}&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;model-parallel&quot;&gt;Model Parallel&lt;/h2&gt;

&lt;p&gt;TorchRec’s main distributed training API is &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module&quot;&gt;DistributedModelParallel&lt;/a&gt;, which calls the planner to generate a sharding plan (demonstrated above) and shards TorchRec modules according to that plan. We demonstrate using &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module&quot;&gt;DistributedModelParallel&lt;/a&gt; to our EmbeddingBagCollection for sharding embeddings distributed training:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(&quot;cuda&quot;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;TorchRec provides simple APIs for quantizing and sharding embeddings for a model for distributed inference. The usage is demonstrated below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from torchrec.inference.modules import (
    quantize_inference_model,
    shard_quant_model,
)
quant_model = quantize_inference_model(ebc)
sharded_model, _ = shard_quant_model(
    quant_model, compute_device=device, sharding_device=device
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;TorchRec and FBGEMM are now stable, with optimized features for large scale recommendation systems.&lt;/p&gt;

&lt;p&gt;For setting up TorchRec and FBGEMM, check out the &lt;a href=&quot;https://pytorch.org/torchrec/setup-torchrec.html&quot;&gt;getting started guide&lt;/a&gt;. &lt;br /&gt;
 &lt;br /&gt;
We also recommend the comprehensive, end-to-end &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torchrec_intro_tutorial.html#&quot;&gt;tutorial for introducing the features in TorchRec and FBGEMM&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Paul Zhang, Zain Huda, Sarunya Pumma, Shintaro Iwasaki, Supadchaya Puangpontip, Benson Ma</name>
        
        
      </author>

      

      

      
        <summary type="html">We are happy to announce the stable release, 1.0, for TorchRec and FBGEMM. TorchRec is the PyTorch native recommendation systems library, powered by FBGEMM’s (Facebook GEneral Matrix Multiplication) efficient, low-level kernels.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.5 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-5/" rel="alternate" type="text/html" title="PyTorch 2.5 Release Blog" />
      <published>2024-10-17T00:00:00-07:00</published>
      <updated>2024-10-17T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-5</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-5/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.5 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.5.0&quot;&gt;release note&lt;/a&gt;)! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.&lt;/p&gt;

&lt;p&gt;This release is composed of 4095 commits from 504 contributors since PyTorch 2.4. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.5. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;As well, please check out our new ecosystem projects releases with &lt;a href=&quot;https://github.com/pytorch/torchrec&quot;&gt;TorchRec&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch-labs/torchfix/releases/tag/v0.6.0&quot;&gt;TorchFix&lt;/a&gt;.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;Beta
   &lt;/td&gt;
   &lt;td&gt;Prototype
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CuDNN backend for SDPA
   &lt;/td&gt;
   &lt;td&gt;FlexAttention
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;torch.compile regional compilation without recompilations
   &lt;/td&gt;
   &lt;td&gt;Compiled Autograd
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchDynamo added support for exception handling &amp;amp; MutableMapping types
   &lt;/td&gt;
   &lt;td&gt;Flight Recorder
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchInductor CPU backend optimization
   &lt;/td&gt;
   &lt;td&gt;Max-autotune Support on CPU with GEMM Template
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;TorchInductor on Windows
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;FP16 support on CPU path for both eager mode and TorchInductor CPP backend
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Autoload Device Extension
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;Enhanced Intel GPU support
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;BETA FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;beta-cudnn-backend-for-sdpa&quot;&gt;[Beta] CuDNN backend for SDPA&lt;/h3&gt;

&lt;p&gt;The cuDNN “Fused Flash Attention” backend  was landed for &lt;em&gt;torch.nn.functional.scaled_dot_product_attention&lt;/em&gt;. On NVIDIA H100 GPUs this can provide up to 75% speed-up over FlashAttentionV2. This speedup is enabled by default for all users of SDPA on H100 or newer GPUs.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchcompile-regional-compilation-without-recompilations&quot;&gt;[Beta] &lt;em&gt;torch.compile&lt;/em&gt; regional compilation without recompilations&lt;/h3&gt;

&lt;p&gt;Regional compilation without recompilations, via &lt;em&gt;torch._dynamo.config.inline_inbuilt_nn_modules&lt;/em&gt; which default to True in 2.5+. This option allows users to compile a repeated &lt;em&gt;nn.Module&lt;/em&gt; (e.g. a transformer layer in LLM) without recompilations. Compared to compiling the full model, this option can result in smaller compilation latencies with 1%-5% performance degradation compared to full model compilation.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/regional_compilation.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchinductor-cpu-backend-optimization&quot;&gt;[Beta] TorchInductor CPU backend optimization&lt;/h3&gt;

&lt;p&gt;This feature advances Inductor’s CPU backend optimization, including CPP backend code generation and FX fusions with customized CPU kernels. The Inductor CPU backend supports vectorization of common data types and all Inductor IR operations, along with the static and symbolic shapes. It is compatible with both Linux and Windows OS and supports the default Python wrapper, the CPP wrapper, and AOT-Inductor mode.&lt;/p&gt;

&lt;p&gt;Additionally, it extends the max-autotune mode of the GEMM template (prototyped in 2.5), offering further performance gains. The backend supports various FX fusions, lowering to customized kernels such as oneDNN for Linear/Conv operations and SDPA. The Inductor CPU backend consistently achieves performance speedups across three benchmark suites—TorchBench, Hugging Face, and timms—outperforming eager mode in 97.5% of the 193 models tested.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;PROTOTYPE FEATURES&lt;/h2&gt;

&lt;h3 id=&quot;prototype-flexattention&quot;&gt;[Prototype] FlexAttention&lt;/h3&gt;

&lt;p&gt;We’ve introduced a flexible API that enables implementing various attention mechanisms such as Sliding Window, Causal Mask, and PrefixLM with just a few lines of idiomatic PyTorch code. This API leverages torch.compile to generate a fused FlashAttention kernel, which eliminates extra memory allocation and achieves performance comparable to handwritten implementations. Additionally, we automatically generate the backwards pass using PyTorch’s autograd machinery. Furthermore, our API can take advantage of sparsity in the attention mask, resulting in significant improvements over standard attention implementations.&lt;/p&gt;

&lt;p&gt;For more information and examples, please refer to the &lt;a href=&quot;https://pytorch.org/blog/flexattention/&quot;&gt;official blog post&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch-labs/attention-gym&quot;&gt;Attention Gym&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-compiled-autograd&quot;&gt;[Prototype] Compiled Autograd&lt;/h3&gt;

&lt;p&gt;Compiled Autograd is an extension to the PT2 stack allowing the capture of the entire backward pass. Unlike the backward graph traced by AOT dispatcher, Compiled Autograd tracing is deferred until backward execution time, which makes it impervious to forward pass graph breaks, and allows it to record backward hooks into the graph.&lt;/p&gt;

&lt;p&gt;Please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/compiled_autograd_tutorial.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;prototype-flight-recorder&quot;&gt;[Prototype] Flight Recorder&lt;/h3&gt;

&lt;p&gt;Flight recorder is a new debugging tool that helps debug stuck jobs. The tool works by continuously capturing information about collectives as they run. Upon detecting a stuck job, the information can be used to quickly identify misbehaving ranks/machines along with code stack traces.&lt;/p&gt;

&lt;p&gt;For more information please refer to the following &lt;a href=&quot;https://pytorch.org/tutorials/prototype/flight_recorder_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-max-autotune-support-on-cpu-with-gemm-template&quot;&gt;[Prototype] Max-autotune Support on CPU with GEMM Template&lt;/h3&gt;

&lt;p&gt;Max-autotune mode for the Inductor CPU backend in torch.compile profiles multiple implementations of operations at compile time and selects the best-performing one. This is particularly beneficial for GEMM-related operations, using a C++ template-based GEMM implementation as an alternative to the ATen-based approach with oneDNN and MKL libraries. We support FP32, BF16, FP16, and INT8 with epilogue fusions for x86 CPUs. We’ve seen up to 7% geomean speedup on the dynamo benchmark suites and up to 20% boost in next-token latency for LLM inference.&lt;/p&gt;

&lt;p&gt;For more information please refer to the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/max_autotune_on_CPU_tutorial.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-torchinductor-cpu-on-windows&quot;&gt;[Prototype] TorchInductor CPU on Windows&lt;/h3&gt;

&lt;p&gt;Inductor CPU backend in torch.compile now works on Windows. We support MSVC (cl), clang (clang-cl) and Intel compiler (icx-cl) for Windows inductor currently.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/inductor_windows_cpu.html&quot;&gt;tutorial&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h3 id=&quot;prototype-fp16-support-on-cpu-path-for-both-eager-mode-and-torchinductor-cpp-backend&quot;&gt;[Prototype] FP16 support on CPU path for both eager mode and TorchInductor CPP backend&lt;/h3&gt;

&lt;p&gt;Float16 is a commonly used reduced floating point type for performance improvement in neural network inference/training. Since this release, float16 for both eager and TorchInductor is supported on the CPU path.&lt;/p&gt;

&lt;h3 id=&quot;prototype-autoload-device-extension&quot;&gt;[Prototype] Autoload Device Extension&lt;/h3&gt;

&lt;p&gt;PyTorch now supports autoloading for out-of-tree device extensions, streamlining integration by eliminating the need for manual imports. This feature, enabled through the torch.backends entrypoint, simplifies usage by ensuring seamless extension loading, while allowing users to disable it via an environment variable if needed.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://pytorch.org/tutorials/prototype/python_extension_autoload.html&quot;&gt;tutorial&lt;/a&gt; for more information.&lt;/p&gt;

&lt;h3 id=&quot;prototype-enhanced-intel-gpu-support&quot;&gt;[Prototype] Enhanced Intel GPU support&lt;/h3&gt;

&lt;p&gt;Intel GPUs support enhancement is now available for both Intel® Data Center GPU Max Series and Intel® Client GPUs (Intel® Core™ Ultra processors with built-in Intel® Arc™ graphics and Intel® Arc™ Graphics for dGPU parts), which is to make it easier to accelerate your Machine Learning workflows on Intel GPUs in PyTorch 2.5 release. We also enabled the initial support of PyTorch on Windows for Intel® Client GPUs in this release.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Expanded PyTorch hardware backend support matrix to include both Intel Data Center and Client GPUs.  &lt;/li&gt;
  &lt;li&gt;The implementation of SYCL* kernels to enhance coverage and execution of Aten operators on Intel GPUs to boost performance in PyTorch eager mode.&lt;/li&gt;
  &lt;li&gt;Enhanced Intel GPU backend of torch.compile to improve inference and training performance for a wide range of deep learning workloads.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These features are available through PyTorch preview and nightly binary PIP wheels. For more information regarding Intel GPU support, please refer to &lt;a href=&quot;https://pytorch.org/docs/main/notes/get_start_xpu.html&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.5 (release note)! This release features a new CuDNN backend for SDPA, enabling speedups by default for users of SDPA on H100s or newer GPUs. As well, regional compilation of torch.compile offers a way to reduce the cold start up time for torch.compile by allowing users to compile a repeated nn.Module (e.g. a transformer layer in LLM) without recompilations. Finally, TorchInductor CPP backend offers solid performance speedup with numerous enhancements like FP16 support, CPP wrapper, AOT-Inductor mode, and max-autotune mode.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The Path to Achieve PyTorch Performance Boost on Windows CPU</title>
      <link href="https://pytorch.org/blog/performance-boost-windows/" rel="alternate" type="text/html" title="The Path to Achieve PyTorch Performance Boost on Windows CPU" />
      <published>2024-10-15T00:00:00-07:00</published>
      <updated>2024-10-15T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/performance-boost-windows</id>
      <content type="html" xml:base="https://pytorch.org/blog/performance-boost-windows/">&lt;p&gt;The challenge of PyTorch’s lower CPU performance on Windows compared to Linux has been a significant issue. There are multiple factors leading to this performance disparity. Through our investigation, we’ve identified several reasons for poor CPU performance on Windows, two primary issues have been pinpointed: the inefficiency of the Windows default malloc memory allocator and the absence of &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;&gt;SIMD&lt;/a&gt; for vectorization optimizations on the Windows platform. In this article, we show how PyTorch CPU performance on Windows has improved from the previous releases and where it stands as of PyTorch 2.4.1.&lt;/p&gt;

&lt;h2 id=&quot;memory-allocation-optimization-in-pytorch-212-and-later&quot;&gt;Memory Allocation Optimization in PyTorch 2.1.2 and later&lt;/h2&gt;

&lt;p&gt;In versions prior to PyTorch 2.1.2, PyTorch relied on the operating system’s default malloc function for memory allocation. The default malloc memory allocation on the Windows platform was less efficient compared to the malloc implementation mechanism on the Linux platform, leading to increased memory allocation times and reduced performance. To address this, we have substituted the default Windows malloc with mimalloc, a more efficient memory allocator developed by Microsoft. This update, included with the release of PyTorch 2.1.2 and later, has significantly enhanced the CPU performance of PyTorch on Windows, as shown in Figure 1.1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg1.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PyTorch CPU Performance Improvement on Windows with Memory Allocation Optimization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.1: Relative throughput improvement achieved by upgrading from Windows PyTorch version 2.0.1 to 2.1.2 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The graph illustrates that with the release of PyTorch 2.1.2, there has been a notable enhancement in CPU performance on the Windows platform. The degree of improvement varies across different models, which can be attributed to the diverse mix of operations they perform and their corresponding memory access patterns. While the BERT model shows a modest performance gain, models like ResNet50 and MobileNet-v3 Large benefit from more pronounced improvements.&lt;/p&gt;

&lt;p&gt;On a high-performance CPU, memory allocation becomes a performance bottleneck. This is also why addressing this issue has led to such significant performance improvements.&lt;/p&gt;

&lt;p&gt;As shown in the graphs below, we see that PyTorch CPU performance on Windows can significantly be improved. However, there is still a noticeable gap when compared to its performance on Linux. The absence of vectorization optimizations in the Windows variant of PyTorch CPU is a key factor to the remaining performance gap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg2.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.0.1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.2: Relative performance of Windows vs Linux with PyTorch version 2.0.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg3.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%; margin-top: 50px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.1.2&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1.3: Relative performance of Windows vs Linux with PyTorch version 2.1.2 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;vectorization-optimization-in-pytorch-241-and-later&quot;&gt;Vectorization Optimization in PyTorch 2.4.1 and later&lt;/h2&gt;

&lt;p&gt;Prior to PyTorch 2.4.1, the Windows build of PyTorch lacked &lt;a href=&quot;https://en.wikipedia.org/wiki/Single_instruction,_multiple_data&quot;&gt;SIMD&lt;/a&gt; for vectorization optimizations, a feature that the Linux build leveraged for improved performance. This discrepancy was due to the &lt;a href=&quot;https://github.com/shibatch/sleef&quot;&gt;SLEEF&lt;/a&gt; Library’s integration issues on Windows, which is a SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT and is essential for efficient trigonometric calculations. Through a collaborative effort with engineers from ARM and Qualcomm, these challenges were resolved, enabling the integration of SIMD into PyTorch for Windows. The PyTorch 2.4.1 update has thus significantly enhanced PyTorch’s CPU performance on Windows, as shown in Figure 2.1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg4.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PyTorch CPU Performance Improvement on Windows with Vertorization Optimization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2.1: Relative throughput improvement achieved by upgrading from PyTorch CPU version 2.1.2 to 2.4.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;As shown in the graph below, we see that PyTorch CPU performance on Windows ahieved the performance on Linux.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg5.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on PyTorch 2.4.1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2.2: Relative performance of Windows vs Linux with PyTorch version 2.4.1 (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;CONCLUSION&lt;/h2&gt;

&lt;p&gt;From PyTorch 2.0.1 to PyTorch 2.4.1, the CPU performance gap between Windows and Linux has been continuously narrowing. We compared the ratio of CPU performance on Windows to CPU performance on Linux across different versions, and the results are shown in the following graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/performance-boost-windows/fg6.png&quot; alt=&quot;performance comparison chart&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Windows vs Linux Performance on different version of PyTorch&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Performance Ratio for Windows to Linux with different version of PyTorch (higher is better).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The graph shows that with PyTorch 2.4.1, CPU performance on Windows has nearly converged with that on Linux, and on some models, it has even surpassed Linux. For example, in the case of DistillBERT and RoBERTa models, the CPU performance ratio of Windows to Linux has achieved a remarkable 102%. However, certain models, including MobileNet-v3, still show a performance discrepancy. Intel engineers will continue to collaborate with Meta engineers, to reduce the performance gap of PyTorch CPU between Windows and Linux.&lt;/p&gt;

&lt;h2 id=&quot;how-to-take-advantage-of-the-optimizations&quot;&gt;HOW TO TAKE ADVANTAGE OF THE OPTIMIZATIONS&lt;/h2&gt;

&lt;p&gt;Install PyTorch CPU 2.4.1 or later on Windows from the &lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;official repository&lt;/a&gt;, and you may automatically experience a performance boost with memory allocation and vectorizations.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;ACKNOWLEDGMENTS&lt;/h2&gt;

&lt;p&gt;The results presented in this blog post was achieved through the collaborative effort of the Intel PyTorch team and Meta. We would like to express our sincere gratitude to &lt;a href=&quot;https://github.com/xuhancn&quot;&gt;Xu Han&lt;/a&gt;, &lt;a href=&quot;https://github.com/jgong5&quot;&gt;Jiong Gong&lt;/a&gt;, &lt;a href=&quot;https://github.com/zhuhaozhe&quot;&gt;Haozhe Zhu&lt;/a&gt;, &lt;a href=&quot;https://github.com/mingfeima&quot;&gt;Mingfei Ma&lt;/a&gt;, &lt;a href=&quot;https://github.com/chuanqi129&quot;&gt;Chuanqi Wang&lt;/a&gt;, &lt;a href=&quot;https://github.com/Guobing-Chen&quot;&gt;Guobing Chen&lt;/a&gt; and &lt;a href=&quot;https://github.com/EikanWang&quot;&gt;Eikan Wang&lt;/a&gt;. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here. Thanks to &lt;a href=&quot;https://github.com/peterjc123&quot;&gt;Jiachen Pu&lt;/a&gt; from community for his participation in the issue discussion and suggesting the use of &lt;a href=&quot;https://github.com/microsoft/mimalloc&quot;&gt;mimalloc&lt;/a&gt;. We’d also like to express our gratitude to Microsoft for providing such an easily integrated and performant mallocation library. Thanks to &lt;a href=&quot;https://github.com/blapie&quot;&gt;Pierre Blanchard&lt;/a&gt; , &lt;a href=&quot;https://github.com/nSircombe&quot;&gt;Nathan Sircombe&lt;/a&gt; from ARM and &lt;a href=&quot;https://github.com/alexreinking&quot;&gt;Alex Reinking&lt;/a&gt; from Adobe for their contribution in overcome the compatibility issues with the &lt;a href=&quot;https://github.com/shibatch/sleef&quot;&gt;sleef&lt;/a&gt; integrated to PyTorch Windows. Finally we want to thank &lt;a href=&quot;https://github.com/jingxu10&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;https://github.com/WeizhuoZhang-intel&quot;&gt;Weizhuo Zhang&lt;/a&gt; and &lt;a href=&quot;https://github.com/ZhaoqiongZ&quot;&gt;Zhaoqiong Zheng&lt;/a&gt; for their contributions to this blog.&lt;/p&gt;

&lt;h3 id=&quot;product-and-performance-information&quot;&gt;Product and Performance Information&lt;/h3&gt;

&lt;p&gt;The configurations in the table are collected with &lt;a href=&quot;https://github.com/intel/svr-info&quot;&gt;svr-info&lt;/a&gt;. Test by Intel on August 30, 2024.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Specification&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Configuration1&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Configuration2&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Name
   &lt;/td&gt;
   &lt;td&gt;ThinkBook 14 G5+ IRH
   &lt;/td&gt;
   &lt;td&gt;ThinkBook 14 G5+ IRH
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Time
   &lt;/td&gt;
   &lt;td&gt;Fri Aug 30 02:43:02 PM UTC 2024
   &lt;/td&gt;
   &lt;td&gt;Fri Aug 30 02:43:02 PM UTC 2024
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;System
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Baseboard
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Chassis
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
   &lt;td&gt;LENOVO
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPU Model
   &lt;/td&gt;
   &lt;td&gt;13th Gen Intel(R) Core(TM) i7-13700H
   &lt;/td&gt;
   &lt;td&gt;13th Gen Intel(R) Core(TM) i7-13700H
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Microarchitecture
   &lt;/td&gt;
   &lt;td&gt;Unknown Intel
   &lt;/td&gt;
   &lt;td&gt;Unknown Intel
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sockets
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Cores per Socket
   &lt;/td&gt;
   &lt;td&gt;14
   &lt;/td&gt;
   &lt;td&gt;14
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hyperthreading
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;CPUs
   &lt;/td&gt;
   &lt;td&gt;20
   &lt;/td&gt;
   &lt;td&gt;20
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Intel Turbo Boost
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
   &lt;td&gt;Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Base Frequency
   &lt;/td&gt;
   &lt;td&gt;2.4GHz
   &lt;/td&gt;
   &lt;td&gt;2.4GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;All-core Maximum Frequency
   &lt;/td&gt;
   &lt;td&gt;4.7GHz
   &lt;/td&gt;
   &lt;td&gt;4.7GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Maximum Frequency
   &lt;/td&gt;
   &lt;td&gt;4.8GHz
   &lt;/td&gt;
   &lt;td&gt;4.8GHz
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;NUMA Nodes
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
   &lt;td&gt;1
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Prefetchers
   &lt;/td&gt;
   &lt;td&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled
   &lt;/td&gt;
   &lt;td&gt;L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;PPINs
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
   &lt;td&gt;-
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Accelerators
   &lt;/td&gt;
   &lt;td&gt;DLB, DSA, IAA, QAT
   &lt;/td&gt;
   &lt;td&gt;DLB, DSA, IAA, QAT
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Installed Memory
   &lt;/td&gt;
   &lt;td&gt;32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s])
   &lt;/td&gt;
   &lt;td&gt;32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s])
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hugepagesize
   &lt;/td&gt;
   &lt;td&gt;2048kb
   &lt;/td&gt;
   &lt;td&gt;2048kb
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Transparent Huge Pages
   &lt;/td&gt;
   &lt;td&gt;madvise
   &lt;/td&gt;
   &lt;td&gt;madvise
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Automatic NUMA Balancing
   &lt;/td&gt;
   &lt;td&gt;Disabled
   &lt;/td&gt;
   &lt;td&gt;Disabled
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;NIC
   &lt;/td&gt;
   &lt;td&gt;“1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation”
   &lt;/td&gt;
   &lt;td&gt;“1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation”
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Disk
   &lt;/td&gt;
   &lt;td&gt;Micron MTFDKBA512TFH 500G
   &lt;/td&gt;
   &lt;td&gt;Micron MTFDKBA512TFH 500G
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;BIOS
   &lt;/td&gt;
   &lt;td&gt;LBCN22WW
   &lt;/td&gt;
   &lt;td&gt;LBCN22WW
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Microcode
   &lt;/td&gt;
   &lt;td&gt;0x411c
   &lt;/td&gt;
   &lt;td&gt;0x411c
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;OS
   &lt;/td&gt;
   &lt;td&gt;Windows 11 Desktop
   &lt;/td&gt;
   &lt;td&gt;Ubuntu 23.10
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Kernel
   &lt;/td&gt;
   &lt;td&gt;OS Build 19045.4412
   &lt;/td&gt;
   &lt;td&gt;6.5.0-27-generic
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TDP
   &lt;/td&gt;
   &lt;td&gt;200 watts
   &lt;/td&gt;
   &lt;td&gt;200 watts
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Power &amp;amp; Perf Policy
   &lt;/td&gt;
   &lt;td&gt;Normal Powersave (7)
   &lt;/td&gt;
   &lt;td&gt;Normal Powersave (7)
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Frequency Governor
   &lt;/td&gt;
   &lt;td&gt;performance
   &lt;/td&gt;
   &lt;td&gt;performance
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Frequency Driver
   &lt;/td&gt;
   &lt;td&gt;intel_pstate
   &lt;/td&gt;
   &lt;td&gt;intel_pstate
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Max C-State
   &lt;/td&gt;
   &lt;td&gt;9
   &lt;/td&gt;
   &lt;td&gt;9
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;notices-and-disclaimers&quot;&gt;Notices and Disclaimers&lt;/h2&gt;

&lt;p&gt;Performance varies by use, configuration and other factors. Learn more on the &lt;a href=&quot;https://edc.intel.com/content/www/us/en/products/performance/benchmarks/overview/&quot;&gt;Performance Index site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Performance results are based on testing as of dates shown in &lt;a href=&quot;#product-and-performance-information&quot;&gt;configurations&lt;/a&gt; and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation.&lt;/p&gt;

&lt;p&gt;Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel Corporation</name>
        
        
      </author>

      

      

      
        <summary type="html">The challenge of PyTorch’s lower CPU performance on Windows compared to Linux has been a significant issue. There are multiple factors leading to this performance disparity. Through our investigation, we’ve identified several reasons for poor CPU performance on Windows, two primary issues have been pinpointed: the inefficiency of the Windows default malloc memory allocator and the absence of SIMD for vectorization optimizations on the Windows platform. In this article, we show how PyTorch CPU performance on Windows has improved from the previous releases and where it stands as of PyTorch 2.4.1. Memory Allocation Optimization in PyTorch 2.1.2 and later In versions prior to PyTorch 2.1.2, PyTorch relied on the operating system’s default malloc function for memory allocation. The default malloc memory allocation on the Windows platform was less efficient compared to the malloc implementation mechanism on the Linux platform, leading to increased memory allocation times and reduced performance. To address this, we have substituted the default Windows malloc with mimalloc, a more efficient memory allocator developed by Microsoft. This update, included with the release of PyTorch 2.1.2 and later, has significantly enhanced the CPU performance of PyTorch on Windows, as shown in Figure 1.1. PyTorch CPU Performance Improvement on Windows with Memory Allocation Optimization Figure 1.1: Relative throughput improvement achieved by upgrading from Windows PyTorch version 2.0.1 to 2.1.2 (higher is better). The graph illustrates that with the release of PyTorch 2.1.2, there has been a notable enhancement in CPU performance on the Windows platform. The degree of improvement varies across different models, which can be attributed to the diverse mix of operations they perform and their corresponding memory access patterns. While the BERT model shows a modest performance gain, models like ResNet50 and MobileNet-v3 Large benefit from more pronounced improvements. On a high-performance CPU, memory allocation becomes a performance bottleneck. This is also why addressing this issue has led to such significant performance improvements. As shown in the graphs below, we see that PyTorch CPU performance on Windows can significantly be improved. However, there is still a noticeable gap when compared to its performance on Linux. The absence of vectorization optimizations in the Windows variant of PyTorch CPU is a key factor to the remaining performance gap. Windows vs Linux Performance on PyTorch 2.0.1 Figure 1.2: Relative performance of Windows vs Linux with PyTorch version 2.0.1 (higher is better). Windows vs Linux Performance on PyTorch 2.1.2 Figure 1.3: Relative performance of Windows vs Linux with PyTorch version 2.1.2 (higher is better). Vectorization Optimization in PyTorch 2.4.1 and later Prior to PyTorch 2.4.1, the Windows build of PyTorch lacked SIMD for vectorization optimizations, a feature that the Linux build leveraged for improved performance. This discrepancy was due to the SLEEF Library’s integration issues on Windows, which is a SIMD Library for Evaluating Elementary Functions, vectorized libm and DFT and is essential for efficient trigonometric calculations. Through a collaborative effort with engineers from ARM and Qualcomm, these challenges were resolved, enabling the integration of SIMD into PyTorch for Windows. The PyTorch 2.4.1 update has thus significantly enhanced PyTorch’s CPU performance on Windows, as shown in Figure 2.1. PyTorch CPU Performance Improvement on Windows with Vertorization Optimization Figure 2.1: Relative throughput improvement achieved by upgrading from PyTorch CPU version 2.1.2 to 2.4.1 (higher is better). As shown in the graph below, we see that PyTorch CPU performance on Windows ahieved the performance on Linux. Windows vs Linux Performance on PyTorch 2.4.1 Figure 2.2: Relative performance of Windows vs Linux with PyTorch version 2.4.1 (higher is better). CONCLUSION From PyTorch 2.0.1 to PyTorch 2.4.1, the CPU performance gap between Windows and Linux has been continuously narrowing. We compared the ratio of CPU performance on Windows to CPU performance on Linux across different versions, and the results are shown in the following graph. Windows vs Linux Performance on different version of PyTorch Figure 3: Performance Ratio for Windows to Linux with different version of PyTorch (higher is better). The graph shows that with PyTorch 2.4.1, CPU performance on Windows has nearly converged with that on Linux, and on some models, it has even surpassed Linux. For example, in the case of DistillBERT and RoBERTa models, the CPU performance ratio of Windows to Linux has achieved a remarkable 102%. However, certain models, including MobileNet-v3, still show a performance discrepancy. Intel engineers will continue to collaborate with Meta engineers, to reduce the performance gap of PyTorch CPU between Windows and Linux. HOW TO TAKE ADVANTAGE OF THE OPTIMIZATIONS Install PyTorch CPU 2.4.1 or later on Windows from the official repository, and you may automatically experience a performance boost with memory allocation and vectorizations. ACKNOWLEDGMENTS The results presented in this blog post was achieved through the collaborative effort of the Intel PyTorch team and Meta. We would like to express our sincere gratitude to Xu Han, Jiong Gong, Haozhe Zhu, Mingfei Ma, Chuanqi Wang, Guobing Chen and Eikan Wang. Their expertise and dedication have been instrumental in achieving the optimizations and performance improvements discussed here. Thanks to Jiachen Pu from community for his participation in the issue discussion and suggesting the use of mimalloc. We’d also like to express our gratitude to Microsoft for providing such an easily integrated and performant mallocation library. Thanks to Pierre Blanchard , Nathan Sircombe from ARM and Alex Reinking from Adobe for their contribution in overcome the compatibility issues with the sleef integrated to PyTorch Windows. Finally we want to thank Jing Xu, Weizhuo Zhang and Zhaoqiong Zheng for their contributions to this blog. Product and Performance Information The configurations in the table are collected with svr-info. Test by Intel on August 30, 2024. Specification Configuration1 Configuration2 Name ThinkBook 14 G5+ IRH ThinkBook 14 G5+ IRH Time Fri Aug 30 02:43:02 PM UTC 2024 Fri Aug 30 02:43:02 PM UTC 2024 System LENOVO LENOVO Baseboard LENOVO LENOVO Chassis LENOVO LENOVO CPU Model 13th Gen Intel(R) Core(TM) i7-13700H 13th Gen Intel(R) Core(TM) i7-13700H Microarchitecture Unknown Intel Unknown Intel Sockets 1 1 Cores per Socket 14 14 Hyperthreading Enabled Enabled CPUs 20 20 Intel Turbo Boost Enabled Enabled Base Frequency 2.4GHz 2.4GHz All-core Maximum Frequency 4.7GHz 4.7GHz Maximum Frequency 4.8GHz 4.8GHz NUMA Nodes 1 1 Prefetchers L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled L2 HW: Enabled, L2 Adj.: Enabled, DCU HW: Enabled, DCU IP: Enabled PPINs - - Accelerators DLB, DSA, IAA, QAT DLB, DSA, IAA, QAT Installed Memory 32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s]) 32GB (8x4GB LPDDR4 7400 MT/s [5200 MT/s]) Hugepagesize 2048kb 2048kb Transparent Huge Pages madvise madvise Automatic NUMA Balancing Disabled Disabled NIC “1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation” “1. Raptor Lake PCH CNVi WiFi 2. Intel Corporation” Disk Micron MTFDKBA512TFH 500G Micron MTFDKBA512TFH 500G BIOS LBCN22WW LBCN22WW Microcode 0x411c 0x411c OS Windows 11 Desktop Ubuntu 23.10 Kernel OS Build 19045.4412 6.5.0-27-generic TDP 200 watts 200 watts Power &amp;amp; Perf Policy Normal Powersave (7) Normal Powersave (7) Frequency Governor performance performance Frequency Driver intel_pstate intel_pstate Max C-State 9 9 Notices and Disclaimers Performance varies by use, configuration and other factors. Learn more on the Performance Index site. Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure. Your costs and results may vary. Intel technologies may require enabled hardware, software or service activation. Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Foundation Technical Advisory Council Elects New Leadership</title>
      <link href="https://pytorch.org/blog/tac-elects-new-leadership/" rel="alternate" type="text/html" title="PyTorch Foundation Technical Advisory Council Elects New Leadership" />
      <published>2024-10-08T00:00:00-07:00</published>
      <updated>2024-10-08T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/tac-elects-new-leadership</id>
      <content type="html" xml:base="https://pytorch.org/blog/tac-elects-new-leadership/">&lt;p&gt;We are pleased to announce the first-ever Chair and Vice Chair of the PyTorch Foundation’s Technical Advisory Council (TAC): &lt;strong&gt;Luca Antiga&lt;/strong&gt; as the Chair and &lt;strong&gt;Jiong Gong&lt;/strong&gt; as Vice Chair. Both leaders bring extensive experience and deep commitment to the PyTorch community, and they are set to guide the TAC in its mission to foster an open, diverse, and innovative PyTorch technical community.&lt;/p&gt;

&lt;h2 id=&quot;meet-the-new-leadership&quot;&gt;Meet the New Leadership&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tac-elects-new-leadership/luca-antiga.jpg&quot; alt=&quot;Luca Antiga&quot; style=&quot;max-width:350px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Luca Antiga is the CTO at Lightning AI since 2022. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings.&lt;/p&gt;

&lt;p&gt;“I am looking forward to taking on the role of the chair of the PyTorch TAC,” says Luca. “As the TAC chair, I will ensure effective, timely topic selection and enhance visibility of technical needs from the board members and from the ecosystem at large. I will strive for directional, cohesive messaging throughout the transition of PyTorch from Meta to the Linux Foundation.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tac-elects-new-leadership/jiong-gong.jpg&quot; alt=&quot;Jiong Gong&quot; style=&quot;max-width:350px; margin-top: 40px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Jiong Gong is a Principal Engineer and SW Architect for PyTorch Optimization from Intel. He serves as one of the PyTorch CPU module maintainers and is an active contributor to the TorchInductor CPU backend.&lt;/p&gt;

&lt;p&gt;“I plan to further strengthen the collaboration between PyTorch developers and hardware vendors, promoting innovation and performance optimization across various hardware platforms, enhancing PyTorch ecosystem and streamlining the decision-making process,” says Jiong. “I am honored to serve as the vice chair of the TAC.”&lt;/p&gt;

&lt;h2 id=&quot;what-does-the-tac-do&quot;&gt;What Does the TAC Do?&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation’s TAC provides a forum for technical communication, leadership,  and collaboration for the PyTorch Foundation. The committee members are members of the PyTorch Foundation.  The committee holds open meetings once a month that anyone in the community can attend.  The committee provides thought leadership on technical topics, knowledge sharing, and a forum to discuss issues with other technical experts in the community.&lt;/p&gt;

&lt;h2 id=&quot;new-tac-webpage&quot;&gt;New TAC Webpage&lt;/h2&gt;

&lt;p&gt;Stay connected with the PyTorch Foundation’s Technical Advisory Council (TAC) by visiting our new &lt;a href=&quot;/tac&quot;&gt;TAC webpage&lt;/a&gt;. Here you can find the TAC members, where to view upcoming meeting agendas, access presentations, attend public meetings, watch meeting recordings and participate in discussions on key technical topics.&lt;/p&gt;

&lt;p&gt;Plus stay tuned on our blog for regular updates from the PyTorch Foundation TAC leadership.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We are pleased to announce the first-ever Chair and Vice Chair of the PyTorch Foundation’s Technical Advisory Council (TAC): Luca Antiga as the Chair and Jiong Gong as Vice Chair. Both leaders bring extensive experience and deep commitment to the PyTorch community, and they are set to guide the TAC in its mission to foster an open, diverse, and innovative PyTorch technical community. Meet the New Leadership Luca Antiga is the CTO at Lightning AI since 2022. He is an early contributor to PyTorch core and co-authored “Deep Learning with PyTorch” (published by Manning). He started his journey as a researcher in Bioengineering, and later co-founded Orobix, a company focused on building and deploying AI in production settings. “I am looking forward to taking on the role of the chair of the PyTorch TAC,” says Luca. “As the TAC chair, I will ensure effective, timely topic selection and enhance visibility of technical needs from the board members and from the ecosystem at large. I will strive for directional, cohesive messaging throughout the transition of PyTorch from Meta to the Linux Foundation.” Jiong Gong is a Principal Engineer and SW Architect for PyTorch Optimization from Intel. He serves as one of the PyTorch CPU module maintainers and is an active contributor to the TorchInductor CPU backend. “I plan to further strengthen the collaboration between PyTorch developers and hardware vendors, promoting innovation and performance optimization across various hardware platforms, enhancing PyTorch ecosystem and streamlining the decision-making process,” says Jiong. “I am honored to serve as the vice chair of the TAC.” What Does the TAC Do? The PyTorch Foundation’s TAC provides a forum for technical communication, leadership, and collaboration for the PyTorch Foundation. The committee members are members of the PyTorch Foundation. The committee holds open meetings once a month that anyone in the community can attend. The committee provides thought leadership on technical topics, knowledge sharing, and a forum to discuss issues with other technical experts in the community. New TAC Webpage Stay connected with the PyTorch Foundation’s Technical Advisory Council (TAC) by visiting our new TAC webpage. Here you can find the TAC members, where to view upcoming meeting agendas, access presentations, attend public meetings, watch meeting recordings and participate in discussions on key technical topics. Plus stay tuned on our blog for regular updates from the PyTorch Foundation TAC leadership.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Conference 2024 Recap: On Fire 🔥</title>
      <link href="https://pytorch.org/blog/pytorch-conference-2024-recap/" rel="alternate" type="text/html" title="PyTorch Conference 2024 Recap: On Fire 🔥" />
      <published>2024-10-02T00:00:00-07:00</published>
      <updated>2024-10-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-conference-2024-recap</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-conference-2024-recap/">&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018197476_9fce5b234d_k.jpg&quot; alt=&quot;women dancing with fire&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The 2024 PyTorch Conference in San Francisco gathered nearly 1,500 AI researchers, developers, and enthusiasts. Over two days, the event featured engaging discussions, insightful keynotes, and hands-on sessions focused on artificial intelligence (AI) and advancements in PyTorch, the leading open-source machine learning framework. Attendees delved into the future of generative AI, Large Language Models (LLMs), and the crucial role open-source technology plays in driving AI innovation. Here’s a recap of the key themes, highlights, and major takeaways from this year’s conference.&lt;/p&gt;

&lt;h2 id=&quot;key-themes-of-the-pytorch-conference-2024&quot;&gt;Key Themes of the PyTorch Conference 2024&lt;/h2&gt;

&lt;p&gt;Three core themes emerged throughout the conference:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Generative AI and LLMs&lt;/strong&gt;: Many sessions focused on how PyTorch continues to evolve as a primary framework for Large Language Models and Generative AI applications. From scaling these models to optimizing their performance on various hardware platforms, the conference showcased the ongoing advancements and challenges in LLM architecture.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Democratizing AI Through Open Source&lt;/strong&gt;: One of the recurring themes was the importance of open source tools and communities in shaping the future of AI. PyTorch is committed to inclusivity, ease of use, and accessibility to developers of all levels, with a focus on bringing AI to an even larger global audience.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Distributed and Edge Computing&lt;/strong&gt;: Distributed computing and edge deployment appeared in many discussions, highlighting how PyTorch is being used to drive AI to the edge. The focus on edge accelerators, scalable training, and inference showcased how PyTorch enables the deployment of powerful models across diverse environments, from the cloud to on-device applications.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54017358432_8d9b53a2c8_k.jpg&quot; alt=&quot;panel of people on a conference stage&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;watch-the-sessions-from-pytorch-conference&quot;&gt;Watch the Sessions from PyTorch Conference&lt;/h2&gt;

&lt;p&gt;The PyTorch Conference featured keynote sessions from top AI leaders and interesting lightning talks. You can &lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l&amp;amp;feature=shared&quot;&gt;view all of the conference sessions&lt;/a&gt; on our YouTube channel.&lt;/p&gt;

&lt;div class=&quot;video-container&quot;&gt;
    &lt;iframe src=&quot;https://www.youtube.com/embed/videoseries?si=qoVqnWpWR_LQOSt6&amp;amp;list=PL_lsbAsL_o2B_znuvm-pDtV_cRhpqZb8l&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;style&gt;
.video-container {
    position: relative;
    width: 100%;
    padding-bottom: 56.25%; /* This maintains a 16:9 aspect ratio */
    height: 0;
    overflow: hidden;
}

.video-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
&lt;/style&gt;

&lt;h2 id=&quot;pytorch-conference-startup-showcase&quot;&gt;PyTorch Conference Startup Showcase&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018500933_4df67cbbd4_k.jpg&quot; alt=&quot;man speaking at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;New this year, the Startup Showcase was an exciting addition to the PyTorch Conference. Featuring early-stage founders pitching their AI startups to a panel of top venture capitalists, this event showcased the next generation of AI-driven innovation. The finalists for the inaugural PyTorch Conference Startup Showcase included Remix Inc., Cartesia, OpenBabylon, Remyx AI, A2 Labs, Inc., QuicSnap, Iso AI, CTGT, and Creao.ai, representing some of the most innovative AI/ML startups in the industry. Attendees got a front-row seat to see cutting-edge AI startups in action, while top VCs from the AI industry evaluated the pitches.&lt;/p&gt;

&lt;p&gt;Congratulations to the PyTorch Conference Startup Showcase winner, CTGT!  Deep learning can be opaque and biased, which limits its potential in crucial areas like healthcare and finance. CTGT is changing the game by enhancing data lineage in LLMs and cutting hallucinations. They’re empowering companies to create customized models using 500x less compute.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/xAePG2YVz7c?feature=shared&quot;&gt;View the Startup Showcase&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;mini-summits&quot;&gt;Mini-Summits&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;DL Compiler Mini-Summit&lt;/strong&gt; offered attendees a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2DyFOVyBzDS5scLfUotrG52&amp;amp;feature=shared&quot;&gt;View the DL Compiler Mini-Summit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54036162068_0afdec2ca6_k.jpg&quot; alt=&quot;People watching an event&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Fine-Tuning Mini-Summit&lt;/strong&gt; brought together a thriving community of researchers, developers, practitioners and hobbyists which focuses on topics ranging from memory efficiency, parameter-efficient fine-tuning and quantization to performance at scale and reproducible evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtube.com/playlist?list=PL_lsbAsL_o2D6l1brEg0DuDShep5p33nu&amp;amp;feature=shared&quot;&gt;View the Fine-Tuning Mini-Summit&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;major-takeaways-from-the-pytorch-conference-2024&quot;&gt;Major Takeaways from the PyTorch Conference 2024&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54018555324_daae473637_k.jpg&quot; alt=&quot;Matt giving his keynote&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;LLMs are Here to Stay&lt;/strong&gt;: were a focal point of the event, reaffirming their pivotal role in the future of AI. As these models continue to scale, PyTorch remains the preferred framework for developing, training, and deploying them across various platforms and industries.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Open Source Drives Innovation&lt;/strong&gt;: A key takeaway from the conference was that open-source tools like PyTorch are vital for democratizing AI. This community-driven approach accelerates innovation, enabling researchers and developers globally to collaborate and contribute to faster advancements and more accessible AI technologies.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ethics and Sustainability Matter&lt;/strong&gt;: The focus on ethical AI development was a significant takeaway. Talks on the inclusivity of computer vision models, the environmental impacts of AI infrastructure, and the need for transparent, unbiased AI models highlighted the growing importance of ethical considerations in the future of AI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PyTorch Expands Beyond the Cloud&lt;/strong&gt;: With several sessions dedicated to edge AI and distributed computing, the conference showcased how PyTorch is expanding beyond cloud-based applications into edge devices and diverse computing environments. This shift is crucial as AI advances into areas like autonomous vehicles, mobile applications, and IoT devices.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;thank-you-to-our-sponsors&quot;&gt;Thank You to Our Sponsors&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/54006027240_be489d89a3_k.jpg&quot; alt=&quot;A crowd of people at a conference&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pytorch-conference-2024-recap/sponsors.png&quot; alt=&quot;Sponsor logos&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We would like to thank each of the sponsors that made the PyTorch Conference 2024 possible. These include:&lt;/p&gt;

&lt;h3 id=&quot;diamond-sponsors&quot;&gt;Diamond Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AMD&lt;/li&gt;
  &lt;li&gt;Cloud Native Computing Foundation&lt;/li&gt;
  &lt;li&gt;IBM&lt;/li&gt;
  &lt;li&gt;Intel – PyTorch&lt;/li&gt;
  &lt;li&gt;Lightning.ai&lt;/li&gt;
  &lt;li&gt;Meta – PyTorch&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;platinum-sponsors&quot;&gt;Platinum Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Arm&lt;/li&gt;
  &lt;li&gt;Google&lt;/li&gt;
  &lt;li&gt;Lambda Labs&lt;/li&gt;
  &lt;li&gt;Nvidia&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;silver-sponsors&quot;&gt;Silver Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Anyscale – PyTorch&lt;/li&gt;
  &lt;li&gt;Baseten&lt;/li&gt;
  &lt;li&gt;Chainguard&lt;/li&gt;
  &lt;li&gt;Databricks&lt;/li&gt;
  &lt;li&gt;Fal&lt;/li&gt;
  &lt;li&gt;FuriosaAi&lt;/li&gt;
  &lt;li&gt;HPE&lt;/li&gt;
  &lt;li&gt;Jane Street&lt;/li&gt;
  &lt;li&gt;Microsoft – PyTorch&lt;/li&gt;
  &lt;li&gt;MinIO&lt;/li&gt;
  &lt;li&gt;Outerbounds&lt;/li&gt;
  &lt;li&gt;Together.AI&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bronze-sponsors&quot;&gt;Bronze Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;d-Matrix&lt;/li&gt;
  &lt;li&gt;MemVerge&lt;/li&gt;
  &lt;li&gt;Perforated AI&lt;/li&gt;
  &lt;li&gt;Quansight&lt;/li&gt;
  &lt;li&gt;Rotational Labs&lt;/li&gt;
  &lt;li&gt;ScaleGenAI&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;special-event-sponsors&quot;&gt;Special Event Sponsors:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;PyTorch Flare Party: Hugging Face&lt;/li&gt;
  &lt;li&gt;Startup Showcase: Mayfield&lt;/li&gt;
  &lt;li&gt;Diversity Scholarship: AWS&lt;/li&gt;
  &lt;li&gt;Women and Non-Binary in PyTorch Lunch: Google&lt;/li&gt;
  &lt;li&gt;Happy Hour Reception: Lightning.AI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thank you for your continued support in advancing the PyTorch ecosystem and helping to shape the future of AI!&lt;/p&gt;

&lt;h2 id=&quot;save-the-date&quot;&gt;Save the Date&lt;/h2&gt;

&lt;p&gt;See you next year for the &lt;a href=&quot;https://events.linuxfoundation.org/pytorch-conference-2025/&quot;&gt;PyTorch Conference in San Francisco at the Palace of Fine Arts&lt;/a&gt; from October 22-23, 2025.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">The 2024 PyTorch Conference in San Francisco gathered nearly 1,500 AI researchers, developers, and enthusiasts. Over two days, the event featured engaging discussions, insightful keynotes, and hands-on sessions focused on artificial intelligence (AI) and advancements in PyTorch, the leading open-source machine learning framework. Attendees delved into the future of generative AI, Large Language Models (LLMs), and the crucial role open-source technology plays in driving AI innovation. Here’s a recap of the key themes, highlights, and major takeaways from this year’s conference. Key Themes of the PyTorch Conference 2024 Three core themes emerged throughout the conference: Generative AI and LLMs: Many sessions focused on how PyTorch continues to evolve as a primary framework for Large Language Models and Generative AI applications. From scaling these models to optimizing their performance on various hardware platforms, the conference showcased the ongoing advancements and challenges in LLM architecture. Democratizing AI Through Open Source: One of the recurring themes was the importance of open source tools and communities in shaping the future of AI. PyTorch is committed to inclusivity, ease of use, and accessibility to developers of all levels, with a focus on bringing AI to an even larger global audience. Distributed and Edge Computing: Distributed computing and edge deployment appeared in many discussions, highlighting how PyTorch is being used to drive AI to the edge. The focus on edge accelerators, scalable training, and inference showcased how PyTorch enables the deployment of powerful models across diverse environments, from the cloud to on-device applications. Watch the Sessions from PyTorch Conference The PyTorch Conference featured keynote sessions from top AI leaders and interesting lightning talks. You can view all of the conference sessions on our YouTube channel. PyTorch Conference Startup Showcase New this year, the Startup Showcase was an exciting addition to the PyTorch Conference. Featuring early-stage founders pitching their AI startups to a panel of top venture capitalists, this event showcased the next generation of AI-driven innovation. The finalists for the inaugural PyTorch Conference Startup Showcase included Remix Inc., Cartesia, OpenBabylon, Remyx AI, A2 Labs, Inc., QuicSnap, Iso AI, CTGT, and Creao.ai, representing some of the most innovative AI/ML startups in the industry. Attendees got a front-row seat to see cutting-edge AI startups in action, while top VCs from the AI industry evaluated the pitches. Congratulations to the PyTorch Conference Startup Showcase winner, CTGT! Deep learning can be opaque and biased, which limits its potential in crucial areas like healthcare and finance. CTGT is changing the game by enhancing data lineage in LLMs and cutting hallucinations. They’re empowering companies to create customized models using 500x less compute. View the Startup Showcase Mini-Summits The DL Compiler Mini-Summit offered attendees a deep dive into the advances in deep learning (DL) compilers that are transforming AI workloads. View the DL Compiler Mini-Summit The Fine-Tuning Mini-Summit brought together a thriving community of researchers, developers, practitioners and hobbyists which focuses on topics ranging from memory efficiency, parameter-efficient fine-tuning and quantization to performance at scale and reproducible evaluations. View the Fine-Tuning Mini-Summit Major Takeaways from the PyTorch Conference 2024 LLMs are Here to Stay: were a focal point of the event, reaffirming their pivotal role in the future of AI. As these models continue to scale, PyTorch remains the preferred framework for developing, training, and deploying them across various platforms and industries. Open Source Drives Innovation: A key takeaway from the conference was that open-source tools like PyTorch are vital for democratizing AI. This community-driven approach accelerates innovation, enabling researchers and developers globally to collaborate and contribute to faster advancements and more accessible AI technologies. Ethics and Sustainability Matter: The focus on ethical AI development was a significant takeaway. Talks on the inclusivity of computer vision models, the environmental impacts of AI infrastructure, and the need for transparent, unbiased AI models highlighted the growing importance of ethical considerations in the future of AI. PyTorch Expands Beyond the Cloud: With several sessions dedicated to edge AI and distributed computing, the conference showcased how PyTorch is expanding beyond cloud-based applications into edge devices and diverse computing environments. This shift is crucial as AI advances into areas like autonomous vehicles, mobile applications, and IoT devices. Thank You to Our Sponsors We would like to thank each of the sponsors that made the PyTorch Conference 2024 possible. These include: Diamond Sponsors: AMD Cloud Native Computing Foundation IBM Intel – PyTorch Lightning.ai Meta – PyTorch Platinum Sponsors: Arm Google Lambda Labs Nvidia Silver Sponsors: Anyscale – PyTorch Baseten Chainguard Databricks Fal FuriosaAi HPE Jane Street Microsoft – PyTorch MinIO Outerbounds Together.AI Bronze Sponsors: d-Matrix MemVerge Perforated AI Quansight Rotational Labs ScaleGenAI Special Event Sponsors: PyTorch Flare Party: Hugging Face Startup Showcase: Mayfield Diversity Scholarship: AWS Women and Non-Binary in PyTorch Lunch: Google Happy Hour Reception: Lightning.AI Thank you for your continued support in advancing the PyTorch ecosystem and helping to shape the future of AI! Save the Date See you next year for the PyTorch Conference in San Francisco at the Palace of Fine Arts from October 22-23, 2025.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch Native Architecture Optimization: torchao</title>
      <link href="https://pytorch.org/blog/pytorch-native-architecture-optimization/" rel="alternate" type="text/html" title="PyTorch Native Architecture Optimization: torchao" />
      <published>2024-09-26T00:00:00-07:00</published>
      <updated>2024-09-26T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch-native-architecture-optimization</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-native-architecture-optimization/">&lt;p&gt;We’re happy to officially launch torchao, a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao&lt;/a&gt; is an accessible toolkit of techniques written (mostly) in easy to read PyTorch code spanning both inference and training. This blog will help you pick which techniques matter for your workloads.&lt;/p&gt;

&lt;p&gt;We benchmarked our techniques on popular GenAI models like LLama 3 and Diffusion models and saw minimal drops in accuracy. Unless otherwise noted the baselines are bf16 run on A100 80GB GPU.&lt;/p&gt;

&lt;p&gt;Our topline metrics for llama 3 are&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;97% speedup for Llama 3 8B inference using autoquant with int4 weight only quantization and hqq&lt;/li&gt;
  &lt;li&gt;73% peak VRAM reduction for Llama 3.1 8B inference at 128K context length with a quantized KV cache&lt;/li&gt;
  &lt;li&gt;50% speedup for Llama 3 70B pretraining using float8 training on H100&lt;/li&gt;
  &lt;li&gt;30% peak VRAM reduction for Llama 3 8B using 4 bit quantized optimizers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our topline metrics for diffusion model inference&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;53% speedup using float8 dynamic quantization inference with float8 row-wise scaling on flux1.dev onH100&lt;/li&gt;
  &lt;li&gt;50% reduction in model VRAM for CogVideoX using int8 dynamic quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below we’ll walk through some of the techniques available in torchao you can apply to your models for inference and training.&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/quantization&quot;&gt;Our inference quantization algorithms&lt;/a&gt; work over arbitrary PyTorch models that contain nn.Linear layers. Weight only and dynamic activation quantization for various dtypes and sparse layouts can be chosen using our top level &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; api&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;quantize_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;quantize_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sometimes quantizing a layer can make it slower because of overhead so if you’d rather we just pick how to quantize each layer in a model for you then you can instead run&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torchao&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autoquant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'max-autotune'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quantize_&lt;/code&gt; API has a few different options depending on whether your model is compute bound or memory bound.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.quantization&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;  
    &lt;span class=&quot;c1&quot;&gt;# Memory bound models  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;int4_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int8_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Compute bound models  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;int8_dynamic_activation_int8_semi_sparse_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;int8_dynamic_activation_int8_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
      
    &lt;span class=&quot;c1&quot;&gt;# Device capability 8.9+  
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;float8_weight_only&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
    &lt;span class=&quot;n&quot;&gt;float8_dynamic_activation_float8_weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also have extensive benchmarks on diffusion models in collaboration with the HuggingFace diffusers team in &lt;a href=&quot;https://github.com/sayakpaul/diffusers-torchao&quot;&gt;diffusers-torchao&lt;/a&gt; where we demonstrated 53.88% speedup on Flux.1-Dev and 27.33% speedup on CogVideoX-5b&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_1.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our APIs are composable so we’ve for example composed sparsity and quantization to bring 5% &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity&quot;&gt;speedup for ViT-H inference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But also can do things like quantize weights to int4 and the kv cache to int8 to support &lt;a href=&quot;https://github.com/pytorch/ao/pull/738&quot;&gt;Llama 3.1 8B at the full 128K context length running in under 18.9GB of VRAM&lt;/a&gt;. &lt;br /&gt;
&lt;img src=&quot;/assets/images/Figure_2.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;qat&quot;&gt;QAT&lt;/h2&gt;

&lt;p&gt;Post training quantization, especially at less than 4 bit can suffer from serious accuracy degradations. Using &lt;a href=&quot;https://pytorch.org/blog/quantization-aware-training/&quot;&gt;Quantization Aware Training&lt;/a&gt; (QAT) we’ve managed to recover up to 96% of the accuracy degradation on hellaswag. We’ve integrated this as an end to end recipe in torchtune with a minimal &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/quantization/prototype/qat&quot;&gt;tutorial&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_3.jpg&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;training&quot;&gt;Training&lt;/h1&gt;

&lt;h2 id=&quot;low-precision-compute-and-communications&quot;&gt;Low precision compute and communications&lt;/h2&gt;

&lt;p&gt;torchao provides easy to use e2e workflows for reducing the precision of training compute and distributed communications, starting with float8 for `torch.nn.Linear` layers.Here is a one-liner to convert the compute gemms of your training run to float8:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.float8&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_to_float8_training&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;convert_to_float8_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For an e2e example of how to speed up LLaMa 3 70B pretraining by up to &lt;strong&gt;1.5x&lt;/strong&gt; with float8, see our &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/float8&quot;&gt;README&lt;/a&gt;, and torchtitan’s &lt;a href=&quot;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&quot;&gt;blog&lt;/a&gt; and &lt;a href=&quot;https://github.com/pytorch/torchtitan/blob/main/docs/float8.md&quot;&gt;float8 recipe&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;performance-and-accuracy-of-float8-pretraining-of-llama-3-70b-vs-bfloat16&quot;&gt;Performance and accuracy of float8 pretraining of LLaMa 3 70B, vs bfloat16&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_4.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;
(source: &lt;a href=&quot;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&quot;&gt;https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;We are expanding our training workflows to more dtypes and layouts&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/torchtune/main/tutorials/qlora_finetune.html&quot;&gt;NF4 QLoRA in torchtune&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pytorch/ao/pull/748&quot;&gt;Prototype int8 training support&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/blog/accelerating-neural-network-training/&quot;&gt;Accelerated sparse 2:4 training&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;low-bit-optimizers&quot;&gt;Low bit Optimizers&lt;/h2&gt;

&lt;p&gt;Inspired by Bits and Bytes we’ve also added prototype support for 8 and 4 bit optimizers as a drop in replacement for AdamW.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchao.prototype.low_bit_optim&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW8bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW4bit&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AdamW8bit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Figure_5.png&quot; alt=&quot;&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;integrations&quot;&gt;Integrations&lt;/h1&gt;

&lt;p&gt;We’ve been actively working on making sure torchao works well in some of the most important projects in open source.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Huggingface transformers as an &lt;a href=&quot;https://huggingface.co/docs/transformers/main/quantization/torchao&quot;&gt;inference backend&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/sayakpaul/diffusers-torchao&quot;&gt;In diffusers-torchao&lt;/a&gt; as a reference implementation for accelerating diffusion models&lt;/li&gt;
  &lt;li&gt;In HQQ for &lt;a href=&quot;https://github.com/mobiusml/hqq#faster-inference&quot;&gt;fast 4 bit inference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In &lt;a href=&quot;https://github.com/pytorch/torchtune&quot;&gt;torchtune&lt;/a&gt; for PyTorch native QLoRA and QAT recipes&lt;/li&gt;
  &lt;li&gt;In &lt;a href=&quot;https://github.com/pytorch/torchchat&quot;&gt;torchchat&lt;/a&gt; for post training quantization&lt;/li&gt;
  &lt;li&gt;In SGLang for for &lt;a href=&quot;https://github.com/sgl-project/sglang/pull/1341&quot;&gt;int4 and int8 post training quantization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;If you’re interested in making your models faster and smaller for training or inference, we hope you’ll find torchao useful and easy to integrate.&lt;/p&gt;

&lt;p&gt;pip install torchao&lt;/p&gt;

&lt;p&gt;There are a lot of things we’re excited about next ranging from going lower than 4 bit, performant kernels for high-throughput inference, expanding to more layers, scaling types or granularities, MX hardware support and supporting more hardware backends. If any of the above sounds exciting you can follow our progress at: &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;https://github.com/pytorch/ao&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If you’re interested in working on torchao, we’ve created a &lt;a href=&quot;https://github.com/pytorch/ao/issues/391&quot;&gt;contributors guide&lt;/a&gt;, and if you have any questions we hang out on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#torchao&lt;/code&gt; channel on &lt;a href=&quot;http://discord.gg/gpumode&quot;&gt;discord.gg/gpumode&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We are fortunate to stand on the shoulders of giants and collaborate with some of the best people in open source. Thank you!&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Bits and Bytes for pioneering work in low bit optimizers and QLoRA&lt;/li&gt;
  &lt;li&gt;Answer.ai for their engineering work to get FSDP and QLoRA composing&lt;/li&gt;
  &lt;li&gt;Mobius Labs for the lovely back and forths on quantization algorithms and low bit kernels&lt;/li&gt;
  &lt;li&gt;HuggingFace transformers for their help in battle testing and integrating our work&lt;/li&gt;
  &lt;li&gt;HuggingFace diffusers for our collaboration on extensive benchmarks and best practices&lt;/li&gt;
  &lt;li&gt;torch.compile so we could write our algorithms in pure PyTorch&lt;/li&gt;
  &lt;li&gt;GPU MODE for most of our early contributors&lt;/li&gt;
&lt;/ol&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re happy to officially launch torchao, a PyTorch native library that makes models faster and smaller by leveraging low bit dtypes, quantization and sparsity. torchao is an accessible toolkit of techniques written (mostly) in easy to read PyTorch code spanning both inference and training. This blog will help you pick which techniques matter for your workloads. We benchmarked our techniques on popular GenAI models like LLama 3 and Diffusion models and saw minimal drops in accuracy. Unless otherwise noted the baselines are bf16 run on A100 80GB GPU. Our topline metrics for llama 3 are 97% speedup for Llama 3 8B inference using autoquant with int4 weight only quantization and hqq 73% peak VRAM reduction for Llama 3.1 8B inference at 128K context length with a quantized KV cache 50% speedup for Llama 3 70B pretraining using float8 training on H100 30% peak VRAM reduction for Llama 3 8B using 4 bit quantized optimizers. Our topline metrics for diffusion model inference 53% speedup using float8 dynamic quantization inference with float8 row-wise scaling on flux1.dev onH100 50% reduction in model VRAM for CogVideoX using int8 dynamic quantization Below we’ll walk through some of the techniques available in torchao you can apply to your models for inference and training. Inference Our inference quantization algorithms work over arbitrary PyTorch models that contain nn.Linear layers. Weight only and dynamic activation quantization for various dtypes and sparse layouts can be chosen using our top level quantize_ api from torchao.quantization import ( quantize_, int4_weight_only, ) quantize_(model, int4_weight_only()) Sometimes quantizing a layer can make it slower because of overhead so if you’d rather we just pick how to quantize each layer in a model for you then you can instead run model = torchao.autoquant(torch.compile(model, mode='max-autotune')) quantize_ API has a few different options depending on whether your model is compute bound or memory bound. from torchao.quantization import ( # Memory bound models int4_weight_only, int8_weight_only, # Compute bound models int8_dynamic_activation_int8_semi_sparse_weight, int8_dynamic_activation_int8_weight, # Device capability 8.9+ float8_weight_only, float8_dynamic_activation_float8_weight, ) We also have extensive benchmarks on diffusion models in collaboration with the HuggingFace diffusers team in diffusers-torchao where we demonstrated 53.88% speedup on Flux.1-Dev and 27.33% speedup on CogVideoX-5b Our APIs are composable so we’ve for example composed sparsity and quantization to bring 5% speedup for ViT-H inference But also can do things like quantize weights to int4 and the kv cache to int8 to support Llama 3.1 8B at the full 128K context length running in under 18.9GB of VRAM. QAT Post training quantization, especially at less than 4 bit can suffer from serious accuracy degradations. Using Quantization Aware Training (QAT) we’ve managed to recover up to 96% of the accuracy degradation on hellaswag. We’ve integrated this as an end to end recipe in torchtune with a minimal tutorial Training Low precision compute and communications torchao provides easy to use e2e workflows for reducing the precision of training compute and distributed communications, starting with float8 for `torch.nn.Linear` layers.Here is a one-liner to convert the compute gemms of your training run to float8: from torchao.float8 import convert_to_float8_training convert_to_float8_training(model) For an e2e example of how to speed up LLaMa 3 70B pretraining by up to 1.5x with float8, see our README, and torchtitan’s blog and float8 recipe. Performance and accuracy of float8 pretraining of LLaMa 3 70B, vs bfloat16 (source: https://dev-discuss.pytorch.org/t/enabling-float8-all-gather-in-fsdp2/2359) We are expanding our training workflows to more dtypes and layouts NF4 QLoRA in torchtune Prototype int8 training support Accelerated sparse 2:4 training Low bit Optimizers Inspired by Bits and Bytes we’ve also added prototype support for 8 and 4 bit optimizers as a drop in replacement for AdamW. from torchao.prototype.low_bit_optim import AdamW8bit, AdamW4bit optim = AdamW8bit(model.parameters()) Integrations We’ve been actively working on making sure torchao works well in some of the most important projects in open source. Huggingface transformers as an inference backend In diffusers-torchao as a reference implementation for accelerating diffusion models In HQQ for fast 4 bit inference In torchtune for PyTorch native QLoRA and QAT recipes In torchchat for post training quantization In SGLang for for int4 and int8 post training quantization Conclusion If you’re interested in making your models faster and smaller for training or inference, we hope you’ll find torchao useful and easy to integrate. pip install torchao There are a lot of things we’re excited about next ranging from going lower than 4 bit, performant kernels for high-throughput inference, expanding to more layers, scaling types or granularities, MX hardware support and supporting more hardware backends. If any of the above sounds exciting you can follow our progress at: https://github.com/pytorch/ao If you’re interested in working on torchao, we’ve created a contributors guide, and if you have any questions we hang out on the #torchao channel on discord.gg/gpumode Acknowledgements We are fortunate to stand on the shoulders of giants and collaborate with some of the best people in open source. Thank you! Bits and Bytes for pioneering work in low bit optimizers and QLoRA Answer.ai for their engineering work to get FSDP and QLoRA composing Mobius Labs for the lovely back and forths on quantization algorithms and low bit kernels HuggingFace transformers for their help in battle testing and integrating our work HuggingFace diffusers for our collaboration on extensive benchmarks and best practices torch.compile so we could write our algorithms in pure PyTorch GPU MODE for most of our early contributors</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Challenges and Efforts in PyTorch Multi-Device Integration: Compatibility, Portability, and Integration Efficiencies</title>
      <link href="https://pytorch.org/blog/pt-multidevice-integration/" rel="alternate" type="text/html" title="Challenges and Efforts in PyTorch Multi-Device Integration: Compatibility, Portability, and Integration Efficiencies" />
      <published>2024-09-18T00:00:00-07:00</published>
      <updated>2024-09-18T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pt-multidevice-integration</id>
      <content type="html" xml:base="https://pytorch.org/blog/pt-multidevice-integration/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;As the demand for diverse hardware accelerators grows, the need for a robust and adaptable deep learning framework becomes increasingly critical. While working through this integration, several challenges have surfaced in the PyTorch ecosystem, potentially affecting various hardware vendors. This blog aims to highlight these issues and propose solutions to enhance PyTorch’s adaptability, portability, and resilience across different hardware platforms.&lt;/p&gt;

&lt;h2 id=&quot;improve-users-code-portability-via-accelerator-autoloading&quot;&gt;Improve Users’ Code Portability via Accelerator Autoloading&lt;/h2&gt;

&lt;p&gt;Currently, users face additional work when running their code on different accelerators. One such task is manually importing modules for out-of-tree devices. This requires users to not only understand the different usage patterns between accelerators but also make their code aware of these differences. If you have projects originally running on GPU/CPU and want to migrate to other accelerators, this can lead to significant work and potential frustration.&lt;/p&gt;

&lt;p&gt;Examples of extra import:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Case 1: Use HPU
import torch
import torchvision.models as models
import habana_frameworks.torch # &amp;lt;-- extra import
model = models.resnet50().eval().to(&quot;hpu&quot;)
input = torch.rand(128, 3, 224, 224).to(&quot;hpu&quot;)
output = model(input)

# Case 2: Use torch_npu
import torch
import torch_npu # &amp;lt;-- extra import
print(torch.ones(1, 2, device='npu'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As a high-level machine learning framework, PyTorch’s ability to shield users from device differences is a competitive feature. &lt;strong&gt;Accelerator Autoloading&lt;/strong&gt; allows users to continue using the familiar PyTorch device programming model without explicitly loading or importing device-specific extensions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How does it works?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Utilize Python’s plugin architecture to enable automatic loading of device extensions via entry points in the PyTorch package.&lt;/p&gt;

&lt;p&gt;Python entry points provide a standardized way for Python packages to expose and discover components or plugins within an application. Via definition in accelerator’s package &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; , PyTorch can automatically initialize accelerator modules when calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch&lt;/code&gt; , which gives users consistent experience between different backend devices.&lt;/p&gt;

&lt;p&gt;From device perspective, only need to claim following setup in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setup.py&lt;/code&gt; (as example of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_npu&lt;/code&gt; )&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// setup.py 
entry_points={
 'torch.backends': ['torch_npu = torch_npu:_autoload', ],
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;import torch&lt;/code&gt; is invoked, the accelerator module will be loaded automatically. This provides users with a consistent programming experience across out-of-tree devices, eliminating the need to be aware of differences between CUDA, HPU, and NPU.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Case 1: Use HPU 
import torch 
import torchvision.models as models 
model = models.resnet50().eval().to(&quot;hpu&quot;) 
input = torch.rand(128, 3, 224, 224).to(&quot;hpu&quot;) 
output = model(input) 

# Case 2: Use torch_npu 
import torch 
print(torch.ones(1, 2, device='npu'))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;device-integration-optimization&quot;&gt;Device Integration Optimization&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;What is PrivateUse1?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In PyTorch, the dispatcher is a crucial component of the framework’s backend that manages how operations are routed to the appropriate device-specific implementation. Dispatch keys are an integral part of this system, serving as identifiers that represent various execution contexts—such as the device (CPU, CUDA, XPU), layout (dense, sparse), and autograd functionality. These keys ensure that operations are directed to the correct implementation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PrivateUse1&lt;/strong&gt; is a customizable device dispatch key, similar to CUDA/CPU/XPU, etc.), reserved for out-of-tree devices. It provides developers with a way to extend PyTorch’s functionality without modifying the core framework, allowing for the integration of new devices, hardware accelerators, or other specialized computing environments.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Why do we need PrivateUse1?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Internally, dispatch keys are represented as bit masks, each bit represents whether a certain key is active. This bit mask representation is efficient for quick lookup and combination of keys, but it inherently limits the number of distinct keys (typically to 64 or fewer).&lt;/p&gt;

&lt;p&gt;The current implementation of BackendComponent dispatch keys in PyTorch has encountered a critical bottleneck, which restricts the addition of new backends and, as a result, limits the expansion of the PyTorch ecosystem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multidevice-integration/fg1.png&quot; alt=&quot;bit diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In response to this challenge, a series of optimizations have been applied to the PrivateUse1 mechanism to enhance its capacity.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PrivateUse1 integration mechanism&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;Initially reserved as fallback options, &lt;strong&gt;PrivateUse1&lt;/strong&gt;, along with &lt;strong&gt;PrivateUse2&lt;/strong&gt; and &lt;strong&gt;PrivateUse3&lt;/strong&gt;, were designed to be activated only when existing key resources became scarce.&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;PrivateUse1&lt;/strong&gt; is now being developed to match the robustness and versatility of established keys like CUDA and CPU. Achieving this required a deep integration across critical PyTorch modules. This integration wasn’t just a simple switch—it involved significant updates to core components such as &lt;strong&gt;AMP (Automatic Mixed Precision)&lt;/strong&gt;, &lt;strong&gt;Autograd&lt;/strong&gt;, &lt;strong&gt;Distributed Training&lt;/strong&gt;, &lt;strong&gt;Checkpointing&lt;/strong&gt;, &lt;strong&gt;DataLoader&lt;/strong&gt;, &lt;strong&gt;Optimization&lt;/strong&gt;, and &lt;strong&gt;Quantization,&lt;/strong&gt; etc.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/multidevice-integration/fg2.png&quot; alt=&quot;flow diagram&quot; style=&quot;width:100%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The activation of &lt;strong&gt;PrivateUse1&lt;/strong&gt; was a massive collaborative effort, culminating in over 100 pull requests aimed at making it from a placeholder to a fully operational dispatch key.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;PrivateUse1 UT/CI Quality Assurance&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;While unit tests are essential for ensuring quality during the development of the &lt;strong&gt;PrivateUse1&lt;/strong&gt; mechanism, they are not sufficient on their own to prevent new pull requests from inadvertently affecting existing functionality or compatibility of out-of-tree devices.&lt;/p&gt;

    &lt;p&gt;To mitigate this risk, the community has added the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch_openreg&lt;/code&gt; module to the test suite. This module leverages a CPU backend to simulate interactions with accelerators, creating a controlled environment for rigorous testing. After implemented, this will enable automatic execution of device-generic test cases whenever relevant code is updated, allowing us to quickly detect and address any potential issues affecting the PrivateUse1 integration mechanism.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Comprehensive Documentation&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;By providing comprehensive and easy-to-understand documentation, we aim to lower the barrier to entry for developers and encourage wider adoption of the PrivateUse1 mechanism in the PyTorch ecosystem. This documentation includes:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Step-by-step guides for integrating new backends using PrivateUse1&lt;/li&gt;
      &lt;li&gt;Clear explanations of PrivateUse1’s functionality and benefits&lt;/li&gt;
      &lt;li&gt;Code examples and best practices for efficient implementation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These enhancements aim to improve the robustness and reliability of the PrivateUse1 mechanism, facilitating better integration of new backends and expanding the capabilities of PyTorch.&lt;/p&gt;

&lt;h2 id=&quot;compatibility-between-upstream-and-downstream&quot;&gt;Compatibility Between Upstream and Downstream&lt;/h2&gt;

&lt;h3 id=&quot;device-generic-unit-tests&quot;&gt;Device-Generic Unit Tests&lt;/h3&gt;

&lt;p&gt;Most unit tests in PyTorch focus on CPU and CUDA devices, which limits participation from users with other hardware. To address this, a plan to modify PyTorch’s unit testing framework, enabling better support for non-CUDA devices. This plan includes removing existing device restrictions, implementing dynamic data type loading, and generalizing decorators to accommodate a broader range of devices. Additionally, we aim to enforce the use of universal device code and expand distributed testing to support non-NCCL backends.&lt;/p&gt;

&lt;p&gt;Through these improvements, we hope to significantly increase test coverage and pass rates for non-CUDA devices, integrating them into PyTorch’s continuous integration process. Initial changes have already been implemented, paving the way for new hardware support and creating a reference template for other devices.&lt;/p&gt;

&lt;h3 id=&quot;ensuring-robust-device-integration-through-automated-testing&quot;&gt;Ensuring Robust Device Integration through Automated Testing&lt;/h3&gt;

&lt;p&gt;To uphold the high standards of quality assurance in PyTorch, an independent build repository and daily continuous integration (CI) workflows have been established, focusing on smoke and integration testing.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytorch-integration-tests&lt;/code&gt; repository automates the testing of PyTorch’s device-specific functionalities, ensuring that they operate correctly and efficiently across a variety of hardware platforms(NPUs and other specialized devices). In repository we are trying to make a fully automated system that continuously validates PyTorch’s compatibility with different hardware backends.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Automated Integration Tests&lt;/strong&gt;: Run automated tests across different devices using GitHub Actions. This automation ensures that every change in the codebase is thoroughly tested against multiple hardware platforms, catching potential issues early in the development process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reusable Workflows&lt;/strong&gt;: Workflows in this repository are modular and reusable, which streamlines the testing process. Developers can easily adapt these workflows to new devices or testing scenarios, making the system both flexible and scalable as PyTorch evolves.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Awareness of Out-of-Tree Devices&lt;/strong&gt;: The repository displays the existence and behavior of all out-of-tree devices, keeping the community informed. This approach minimizes the risk of accidentally breaking downstream functionalities and provides fast feedback on changes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Efforts to enhance multi-device integration are pivotal for its adaptability in the evolving deep learning landscape. These initiatives not only benefit current users but also lower entry barriers for new hardware vendors and developers, fostering innovation in AI and machine learning. As PyTorch continues to evolve, its commitment to flexibility, robustness, and inclusivity positions it as a leading framework capable of meeting the diverse needs of the deep learning community.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Zesheng Zong (Huawei), Jiawei Li (Huawei) | Co-authors: Jiong Gong (Intel), Bartosz Sochacki (Intel), Eikan Wang (Intel)</name>
        
        
      </author>

      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Arm Joins the PyTorch Foundation as a Premier Member</title>
      <link href="https://pytorch.org/blog/arm-joins-pytorch/" rel="alternate" type="text/html" title="Arm Joins the PyTorch Foundation as a Premier Member" />
      <published>2024-09-12T00:00:00-07:00</published>
      <updated>2024-09-12T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/arm-joins-pytorch</id>
      <content type="html" xml:base="https://pytorch.org/blog/arm-joins-pytorch/">&lt;p&gt;The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that &lt;a href=&quot;https://www.arm.com/&quot;&gt;Arm&lt;/a&gt; has joined as a premier member.&lt;/p&gt;

&lt;p&gt;Arm designs a high-performance, power-efficient compute platform with unmatched scalability, supporting a vast ecosystem of developers deploying AI at the edge and in the cloud, ranging from the Arm instances offered by all major cloud service providers to smartphones, laptops, software-defined vehicles and more.&lt;/p&gt;

&lt;p&gt;“Our continued investments in software are accelerating development and AI performance for over 20 million software developers, ensuring they can develop for Arm, on Arm,” said Alex Spinelli, VP Developer Technology at Arm. “PyTorch is a pivotal framework in advancing AI research and development. This membership demonstrates our strong commitment to open source - ensuring PyTorch just works on Arm and can leverage seamless acceleration for the most demanding AI models, now and in the future.”&lt;/p&gt;

&lt;p&gt;Last year at the PyTorch Conference, Arm partnered with Apple, Meta and Qualcomm to release &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;ExecuTorch&lt;/a&gt;, an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers.&lt;/p&gt;

&lt;p&gt;“We’re thrilled to welcome Arm to the PyTorch Foundation. As we look to the future of AI and machine learning, the role of specialized silicon and edge devices becomes increasingly crucial. Arm’s expertise in these areas will be invaluable as we work to make PyTorch more efficient and accessible across a wider range of hardware,” said PyTorch Foundation Executive Director Matt White. “This collaboration underscores our commitment to fostering innovation and expanding PyTorch’s capabilities to meet the evolving needs of developers and researchers worldwide.”&lt;/p&gt;

&lt;p&gt;As a premier member, Arm is granted one seat to the PyTorch Foundation Governing Board. The Board sets policy through our bylaws, mission and vision statements, describing the overarching scope of foundation initiatives, technical vision, and direction.&lt;/p&gt;

&lt;p&gt;We’re happy to welcome Alex Spinelli, VP Developer Technology at Arm, to our board. Prior to Arm, Alex was VP of Product for Core Machine Learning at Google, where he led Google’s technology and infrastructure for building, training, and serving machine learning, including the TensorFlow stack.&lt;/p&gt;

&lt;p&gt;To learn more about how you can be a part of the PyTorch Foundation, visit our &lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;website&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;about-pytorch-foundation&quot;&gt;About PyTorch Foundation&lt;/h2&gt;

&lt;p&gt;The PyTorch Foundation is a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem. The PyTorch Foundation is supported by its members and leading contributors to the PyTorch open source project. The Foundation leverages resources provided by members and contributors to enable community discussions and collaboration.&lt;/p&gt;

&lt;h2 id=&quot;about-the-linux-foundation&quot;&gt;About The Linux Foundation&lt;/h2&gt;

&lt;p&gt;The Linux Foundation is the world’s leading home for collaboration on open source software, hardware, standards, and data. Linux Foundation projects are critical to the world’s infrastructure including Linux, Kubernetes, Node.js, ONAP, PyTorch, RISC-V, SPDX, OpenChain, and more. The Linux Foundation focuses on leveraging best practices and addressing the needs of contributors, users, and solution providers to create sustainable models for open collaboration. For more information, please visit us at linuxfoundation.org. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see its trademark usage page. Linux is a registered trademark of Linus Torvalds.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>The PyTorch Foundation</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch Foundation, a neutral home for the deep learning community to collaborate on the open source PyTorch framework and ecosystem, is announcing today that Arm has joined as a premier member.</summary>
      

      
      
    </entry>
  
</feed>



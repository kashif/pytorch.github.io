<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-04-04T18:04:15-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Accelerating MoE model inference with Locality-Aware Kernel Design</title>
      <link href="https://pytorch.org/blog/accelerating-moe-model/" rel="alternate" type="text/html" title="Accelerating MoE model inference with Locality-Aware Kernel Design" />
      <published>2024-04-04T00:00:00-07:00</published>
      <updated>2024-04-04T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-moe-model</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-moe-model/">&lt;h2 id=&quot;10-summary&quot;&gt;1.0 Summary&lt;/h2&gt;

&lt;p&gt;We show that by implementing column-major scheduling to improve data locality, we can accelerate the core Triton GEMM (General Matrix-Matrix Multiply) kernel for MoEs (Mixture of Experts) up to 4x on A100, and up to 4.4x on H100 Nvidia GPUs. This post demonstrates several different work decomposition and scheduling algorithms for MoE GEMMs and shows, at the hardware level, why column-major scheduling produces the highest speedup.&lt;/p&gt;

&lt;p&gt;Repo and code available at: &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/triton/inference/col_major_moe_gemm&quot;&gt;https://github.com/pytorch-labs/applied-ai/tree/main/triton/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-7.png&quot; alt=&quot;Figure 1A. Optimized Fused MoE GEMM Kernel TFLOPs on A100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1A. Optimized Fused MoE GEMM Kernel TFLOPs on &lt;strong&gt;A100&lt;/strong&gt; for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-8.png&quot; alt=&quot;Figure 1B. Optimized Fused MoE GEMM Kernel TFLOPs on H100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto; margin-top: 40px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1B. Optimized Fused MoE GEMM Kernel TFLOPs on &lt;strong&gt;H100&lt;/strong&gt; for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;20-background&quot;&gt;2.0 Background&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/openai/triton&quot;&gt;OpenAI’s Triton&lt;/a&gt; is a hardware-agnostic language and compiler that as our prior &lt;a href=&quot;https://pytorch.org/blog/accelerating-triton/&quot;&gt;blog post&lt;/a&gt; has shown can be used to accelerate quantization workflows. We also showed that in terms of kernel development, much of the same learnings and performance analysis tools from CUDA can be leveraged to provide similar insights into how Triton kernels work under-the-hood and subsequent measures to speedup these kernels in latency sensitive environments. As Triton becomes increasingly adopted in production settings, it is important that developers understand the common tips and tricks to developing performant kernels as well as the generality of these methods to various different architectures and workflows. Thus, this post will explore how we optimized the Triton kernel developed by &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM &lt;/a&gt;for the popular Mixture of Experts (MoE) Mixtral model using classical techniques and how these techniques can be implemented in Triton to achieve performance gain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04088&quot;&gt;Mixtral 8x7B&lt;/a&gt; is a sparse Mixture of Experts Language Model. Unlike the classical dense transformer architecture, each transformer block houses 8 MLP layers where each MLP is an ‘expert’. As a token flows through, a router network selects which 2 of the 8 experts should process that token and the results are then combined. The selected experts for the same token vary at each layer.  As a result, while Mixtral 8x7B has a total of 47B params, during inference only 13B params are active.&lt;/p&gt;

&lt;p&gt;The MoE GEMM (General Matrix-Matrix Multiply) kernel receives a stacked weight matrix containing all the experts, and must subsequently route each token to the TopK (2 for Mixtral) experts by utilizing a mapping array produced by the resultant scores of the router network. In this post, we provide methods to efficiently parallelize this computation during inference time, specifically during autoregression (or decoding stages).&lt;/p&gt;

&lt;h2 id=&quot;30-work-decomposition---splitk&quot;&gt;3.0 Work Decomposition - SplitK&lt;/h2&gt;

&lt;p&gt;We have previously shown that for the matrix problem sizes found in LLM inference, specifically in the context of W4A16 quantized inference, GEMM kernels can be accelerated by applying a &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;SplitK work decomposition&lt;/a&gt;. Thus, we started our MoE acceleration research by implementing SplitK in the &lt;a href=&quot;https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py&quot;&gt;vLLM MoE Kernel&lt;/a&gt;, which produced speedups of approximately 18-20% over the Data Parallel approach.&lt;/p&gt;

&lt;p&gt;This result shows that the SplitK optimization can be used as a part of a more formulaic approach to improving/developing Triton kernels in inference settings. To build intuition about these different work decompositions, let’s consider a simple example for the multiplication of two 4x4 matrices and SplitK=2.&lt;/p&gt;

&lt;p&gt;In the data parallel GEMM kernel shown below, the computation for a single block of the output matrix will be handled by 1 threadblock, TB0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-1.gif&quot; alt=&quot;Figure 2. Data Parallel GEMM&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2. Data Parallel GEMM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In contrast, in the SplitK kernel, the work required to compute 1 block in the output matrix, is “split” or shared amongst 2 thread blocks TB0 and TB1. This provides better load balancing and increased parallelism.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig.gif&quot; alt=&quot;Figure 3. SplitK GEMM&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3. SplitK GEMM&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key idea is that we’ve increased our parallelism from M&lt;em&gt;N to M&lt;/em&gt;N*SplitK. This approach does incur some costs such as adding inter-threadblock communication via atomic operations. However, these costs are minimal compared to the savings of other constrained GPU resources like shared memory and registers. Most importantly, the SplitK strategy provides superior load balancing characteristics for skinny matrices, (as is the case in MoE inference) and is the common matrix profile during decoding and inference.&lt;/p&gt;

&lt;h2 id=&quot;40-gemm-hardware-scheduling---column-major&quot;&gt;4.0 GEMM Hardware Scheduling - Column Major&lt;/h2&gt;

&lt;p&gt;To improve upon the ~20% speedup with SplitK we focused our investigation on the logic that controls the hardware scheduling of the GEMM in Triton Kernels. Our profiling of the vLLM MoE kernel showed a low L2 cache hit rate, thus we investigated three scheduling options - column-major, row-major and grouped launch.  Due to some intrinsic properties of MoE models, such as large expert matrices, and having to dynamically load TopK (2 for Mixtral) matrices during the duration of the kernel, cache reuse/hit rate becomes a bottleneck that this optimization will target.&lt;/p&gt;

&lt;p&gt;For background, in our previous &lt;a href=&quot;https://pytorch.org/blog/accelerating-triton/&quot;&gt;blog&lt;/a&gt;, we touched on the concept of “tile swizzling”, a method to achieve greater L2 cache hit rate. This concept relates to how the software &lt;em&gt;schedules&lt;/em&gt; the GEMM onto the SMs of a GPU. In Triton, this schedule is determined by the pid_m and pid_n calculations. Our key insight is that for skinny matrix multiplications, a column-major ordering ensures optimal reuse of the columns of the weight matrix, B. To illustrate this, let’s take a look at a snippet of what a column major computation of pid_m, and pid_n would look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-6.png&quot; alt=&quot;Figure 4. Column Major ordering in PyTorch&quot; style=&quot;width:100%;display: block; max-width: 500px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4. Column Major ordering in PyTorch&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;From above, we note that with this mapping, we schedule the GEMM such that we calculate the output blocks of C in the following order: C(0, 0), C(1, 0), C(2, 0),… etc. To understand the implications we provide the following illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-5.png&quot; alt=&quot;Activation matrix / Weight matrix&quot; style=&quot;width:100%;display: block; max-width: 500px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-4.png&quot; alt=&quot;L1/L2 Cache&quot; style=&quot;width:100%;display: block; max-width: 300px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-3.png&quot; alt=&quot;C - Output Matrix&quot; style=&quot;width:100%;display: block; max-width: 300px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5. Cache Reuse Pattern for a Column-Major GEMM Schedule&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the above simplified view of a column-major schedule, let’s assume for a GEMM with skinny activation matrix A, that the entire matrix can fit in the GPU cache which is a reasonable assumption to make for the type of problem sizes we encounter in MoE inference. This allows for maximal &lt;em&gt;reuse&lt;/em&gt; of the columns of the weight matrix B, due to the fact that the B column can be re-used for the corresponding output tile calculations, C(0,0), C(1, 0) and C(2, 0). Consider instead, a row-major schedule, C(0,0), C(0,1), C(0, 2) etc. We would have to evict the column of B, and issue multiple load instructions to DRAM to calculate the same amount of output blocks.&lt;/p&gt;

&lt;p&gt;An important design consideration when optimizing kernels is a memory access pattern that results in the least amount of global load instructions. This optimal memory access pattern is achieved with the column-major schedule. The results below showcase the performance of the three schedules we investigated:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-2.png&quot; alt=&quot;Figure 6. Comparison of GEMM Schedules on A100 for varying Batch Sizes M&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 6. Comparison of GEMM Schedules on A100 for varying Batch Sizes M&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The column-major schedule provides up to a 4x speedup over the other patterns, and as we’ll show in the next section, provides an optimal memory access pattern due to greatly improved data locality.&lt;/p&gt;

&lt;h2 id=&quot;50-nsight-compute-analysis---throughput-and-memory-access-pattern&quot;&gt;5.0 Nsight Compute Analysis - Throughput and Memory Access Pattern&lt;/h2&gt;

&lt;p&gt;For performance analysis, we focus on the &lt;strong&gt;M = 2&lt;/strong&gt; case for the H100.  A similar study can be done for the A100 as many of the same observations carry over.  We note the following salient results, that showcase the impact of our optimizations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig-1.png&quot; alt=&quot;Figure 7. H100 Memory Throughput Chart for M = 2.  Note the very large increase in the cache hit rates L1 cache hit rate (+2696%) and L2 cache hit rate (+254%).&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 7. H100 Memory Throughput Chart for M = 2.  Note the very large increase in the cache hit rates L1 cache hit rate (+2696%) and L2 cache hit rate (+254%).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-moe-model/fig.png&quot; alt=&quot;Figure 8. H100 Memory Instruction Statistics M = 2. Note the 49% reduction in global memory loads.&quot; style=&quot;width:100%;margin-top: 40px;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 8. H100 Memory Instruction Statistics M = 2. Note the 49% reduction in global memory loads.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;These statistics show that our optimizations had the intended effect, which can be seen in the reduced cache misses, reduced memory accesses and the resultant 2.7x speedup. More concretely, the trace shows us a 2.54x increase in L2 hit rate (Figure 7), and a  ~50% reduction in DRAM accesses (Figure 8).&lt;/p&gt;

&lt;p&gt;These improvements ultimately yield the reduced latency, with the optimized kernel being 2.7x faster for bs=2 and 4.4x for bs=512.&lt;/p&gt;

&lt;h2 id=&quot;60-future-work&quot;&gt;6.0 Future Work&lt;/h2&gt;

&lt;p&gt;Our kernel was tested in FP16, which showcases the numerics and performance of the column major scheduling for MoE, but most production models are using BFloat16.  We encountered a limitation in Triton such that tl.atomic_add does not support Bfloat16 and hit launch latency concerns which would require cuda graph support for column major production use. In initial testing this translated to a 70% end-to-end speedup but, we encountered some expert mapping inconsistencies in an end to end environment that are not reflected in the test environment, so further work is needed to fully realize these speedups. \&lt;/p&gt;

&lt;p&gt;For future work, we intend to move this into a CUDA kernel which will ensure full BFloat16 support and reduced launch latency relative to Triton, and potentially resolve the expert routing inconsistency. We’ve also previously &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;published work&lt;/a&gt; on enabling GPTQ W4A16 with Triton GEMM kernels, so natural follow-on work would include fusing dequantization into this kernel to allow for a GPTQ quantized inference path.&lt;/p&gt;

&lt;h2 id=&quot;70-reproducibility&quot;&gt;7.0 Reproducibility&lt;/h2&gt;

&lt;p&gt;We have &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai/tree/main/kernels/triton/inference/col_major_moe_gemm&quot;&gt;open sourced&lt;/a&gt; the Triton kernel code along with an easy to run performance benchmark for readers interested in comparing or verifying the performance on their own GPU.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We want to thank Daniel Han, Raghu Ganti, Mudhakar Srivatsa, Bert Maher, Gregory Chanan, Eli Uriegas, and Geeta Chauhan for their review of the presented material and Woo Suk from the vLLM team as we built on his implementation of the Fused MoE kernel.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Antoni Virós Martin, Chih-Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">1.0 Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Maximizing training throughput using PyTorch FSDP</title>
      <link href="https://pytorch.org/blog/maximizing-training/" rel="alternate" type="text/html" title="Maximizing training throughput using PyTorch FSDP" />
      <published>2024-03-13T00:00:00-07:00</published>
      <updated>2024-03-13T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/maximizing-training</id>
      <content type="html" xml:base="https://pytorch.org/blog/maximizing-training/">&lt;p&gt;In this blog, we demonstrate the scalability of FSDP with a pre-training exemplar, a 7B model trained for 2T tokens, and share various techniques we used to achieve a rapid training speed of 3,700 tokens/sec/GPU, or 40B tokens/day on 128 A100 GPUs. This translates to a model FLOPS utilization (MFU) and hardware FLOPS utilization (HFU) of 57%. Additionally, we have observed near linear scaling of FSDP to 512 GPUs, implying that training a 7B model on 512 GPUs to 2T tokens using this method would take just under two weeks.&lt;/p&gt;

&lt;p&gt;IBM researchers trained a Meta Llama 2 7B architecture to 2T tokens, which we will refer to as LlamaT(est). This model demonstrates comparable model quality as Llama 2 on various academic benchmarks. All of the &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;training code&lt;/a&gt;, along with our methodology to achieve this throughput, can be found in this blog. We also share the configuration knobs that work well for the Llama 2 models – 7B, 13B, 34B, and 70B for A100s and H100s.&lt;/p&gt;

&lt;p&gt;In this process, we also propose a _new _selective activation checkpointing mechanism that applies to FSDP which gives us a 10% boost beyond out-of-the box FSDP. We have open sourced the &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;training code base&lt;/a&gt; and an associated scalable data loader as the methodology to achieve this throughput.&lt;/p&gt;

&lt;p&gt;One key benefit of a PyTorch native pathway for training is the ability  to seamlessly train on multiple hardware backends. For example, the recent end-to-end stack for training that was released by AllenAI through OLMo also leverages PyTorch FSDP for training on AMD and NVIDIA GPUs. There are three main components that we leverage from FSDP to achieve our throughput:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;SDPA Flash attention&lt;/a&gt;, that enables fused attention kernels and efficient attention computation&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://engineering.fb.com/2021/07/15/open-source/fsdp/&quot;&gt;Overlap&lt;/a&gt; in computation and communication allows for better utilization of the GPU&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.05198.pdf&quot;&gt;Selective activation checkpointing&lt;/a&gt; enables us to tradeoff between GPU memory and compute&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;IBM has been working closely with Team PyTorch at Meta on &lt;a href=&quot;https://arxiv.org/abs/2304.11277&quot;&gt;PyTorch FSDP&lt;/a&gt; for nearly two years: introducing the &lt;a href=&quot;https://pytorch.org/blog/scaling-pytorch-fsdp-for-training-foundation-models-on-ibm-cloud/&quot;&gt;rate limiter&lt;/a&gt; for achieving better throughput on Ethernet interconnects, &lt;a href=&quot;https://pytorch.org/blog/performant-distributed-checkpointing/&quot;&gt;distributed checkpointing&lt;/a&gt; to improve the checkpoint times by an order of magnitude, and implementing the early version of checkpointing for the hybrid sharding mode of FSDP. Late last year, we used FSDP to train a model end-to-end.&lt;/p&gt;

&lt;h2 id=&quot;training-details&quot;&gt;Training Details&lt;/h2&gt;

&lt;p&gt;The 7B model is trained on 128 A100 GPUs with 400Gbps network connectivity and GPU direct RDMA. We use SDPA FlashAttention v2 for attention computation, and for this model we turned off activation checkpointing that limits the batch size, but provides the highest throughput – batch size is 1 million tokens per batch for 128 GPUs and improves throughput by about 10% when compared to activation checkpointing. With these parameters, we have an almost full overlap in computation and communication. We use the AdamW optimizer in 32-bit with beta1 of 0.9 and beta2 of 0.95, weight decay of 0.1, and a learning rate ending at 3e-5 with a warmup to max learning rate of 3e-4 and a cosine schedule to reduce to 3e-5 over 2T tokens. The training was performed using mixed precision bf16 on an internal dataset. The training stack is using IBM’s &lt;a href=&quot;https://github.com/foundation-model-stack/foundation-model-stack/blob/main/fms/models/llama.py&quot;&gt;Foundation Model Stack&lt;/a&gt; for model architecture and PyTorch nightlies post-2.2 release for FSDP and SDPA. We tried a few different nightlies during the time period of Nov 2023 through Feb 2024 and we observed an improvement in the throughput.&lt;/p&gt;

&lt;h3 id=&quot;selective-activation-checkpointing&quot;&gt;Selective activation checkpointing&lt;/h3&gt;

&lt;p&gt;We jointly implemented a simple and effective mechanism of selective activation checkpointing (AC). In FSDP, the common practice is to checkpoint each transformer block. A simple extension is to checkpoint every _n _blocks and reduce the amount of recomputation, while increasing the memory needed. This is quite effective for the 13B model size, increasing the throughput by 10%. For the 7B model size, we did not need activation checkpointing at all. Future versions of FSDP will provide selective activation checkpointing at an operator level, enabling an optimal compute-memory tradeoff. The code for the above is implemented &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp/blob/main/fms_fsdp/policies/ac_handler.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;throughput-and-mfu-hfu-computation&quot;&gt;Throughput and MFU, HFU computation&lt;/h3&gt;

&lt;p&gt;While we only trained the 7B model to 2T tokens, we performed numerous experiments on the other model sizes to provide the best configuration options. This is summarized in the table below for two types of infrastructure —  an A100 cluster with 128 GPUs and 400Gbps inter-node interconnect, and an H100 cluster with 96 GPUs and 800Gbps inter-node interconnect.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;
&lt;strong&gt;Model size&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Batch size&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Activation checkpoint&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Throughput tokens/sec/GPU (A100 80GB and 400Gbps interconnect)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;MFU % (A100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;HFU % (A100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;Throughput tokens/sec/GPU (H100 80GB and 800Gbps interconnect)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;MFU % (H100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
   &lt;td&gt;
&lt;strong&gt;HFU % (H100 80GB)&lt;/strong&gt;


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
7B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
No


   &lt;/td&gt;
   &lt;td&gt;
3700


   &lt;/td&gt;
   &lt;td&gt;
0.57


   &lt;/td&gt;
   &lt;td&gt;
0.57


   &lt;/td&gt;
   &lt;td&gt;
7500


   &lt;/td&gt;
   &lt;td&gt;
0.37


   &lt;/td&gt;
   &lt;td&gt;
0.37


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
13B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Selective


   &lt;/td&gt;
   &lt;td&gt;
1800


   &lt;/td&gt;
   &lt;td&gt;
0.51


   &lt;/td&gt;
   &lt;td&gt;
0.59


   &lt;/td&gt;
   &lt;td&gt;
3800


   &lt;/td&gt;
   &lt;td&gt;
0.35


   &lt;/td&gt;
   &lt;td&gt;
0.40


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
34B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Yes


   &lt;/td&gt;
   &lt;td&gt;
700


   &lt;/td&gt;
   &lt;td&gt;
0.47


   &lt;/td&gt;
   &lt;td&gt;
0.64


   &lt;/td&gt;
   &lt;td&gt;
1550


   &lt;/td&gt;
   &lt;td&gt;
0.32


   &lt;/td&gt;
   &lt;td&gt;
0.44


   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
70B


   &lt;/td&gt;
   &lt;td&gt;
2


   &lt;/td&gt;
   &lt;td&gt;
Yes


   &lt;/td&gt;
   &lt;td&gt;
370


   &lt;/td&gt;
   &lt;td&gt;
0.50


   &lt;/td&gt;
   &lt;td&gt;
0.67


   &lt;/td&gt;
   &lt;td&gt;
800


   &lt;/td&gt;
   &lt;td&gt;
0.34


   &lt;/td&gt;
   &lt;td&gt;
0.45


   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: Model and Hardware FLOPS utilization of various model sizes on A100 and H100 GPUs&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;HFU numbers are computed using the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/2240018c03744ee34ea14ad53481db934c37e384/torch/utils/flop_counter.py#L336&quot;&gt;PyTorch FLOP counter&lt;/a&gt; and the theoretical bf16 performance of A100 and H100 GPUs, whereas MFU numbers are computed using the methodology outlined in &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;NanoGPT&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/2240018c03744ee34ea14ad53481db934c37e384/torch/utils/flop_counter.py#L336&quot;&gt;PaLM paper&lt;/a&gt;. We also note that the batch sizes we use for the larger models are intentionally kept at 2 per GPU to mimic choices made in training models of 4k sequence length and achieve this up to 512 GPUs without exceeding the 4M tokens popular batch size. Beyond that, we would need tensor parallelism or sequence parallelism.&lt;/p&gt;

&lt;p&gt;We note in the table above that for A100s, that activation recomputation causes the MFU to reduce, while HFU increases! With the introduction of better activation checkpointing schemes, we expect MFU to increase and catch up with HFU. However, we observe that for H100s, both MFU and HFU are relatively low. We analyze the PyTorch profile traces on H100 and observe that there is a 10% gap due to network “peeking” out. In addition, we  hypothesize that the HBM bandwidth of H100s is the cause for the reduced HFU/MFU on H100s and not being able to obtain the 3x improvement (H100s are theoretically 3x faster than A100s - &lt;a href=&quot;https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#tflops-comparison-table&quot;&gt;312 vs 989TFLOPS&lt;/a&gt;, but only have &amp;lt;2x the HBM bandwidth than A100s - &lt;a href=&quot;https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#accelerator-memory-size-and-speed&quot;&gt;2.0 vs 3.35TBps&lt;/a&gt;). We plan to try out other configuration options like Tensor Parallel to improve the knobs for the 70B model on H100s.&lt;/p&gt;

&lt;h3 id=&quot;model-details&quot;&gt;Model details&lt;/h3&gt;

&lt;p&gt;The loss curve for training is shown in the below figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/loss_curve.png&quot; alt=&quot;loss curve for training&quot; style=&quot;width:100%;display: block; max-width: 600px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 1: LlamaT training loss curve&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The 2T checkpoint is converted to Hugging Face format by a script that is provided in the repository and we then use &lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;lm-evaluation-harness&lt;/a&gt; to compute key academic benchmarks and compare that by running it on Llama2-7B. These results are captured in the below table.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Evaluation metric&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Llama2-7B (baseline)&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;LlamaT-7B&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (zero shot)
   &lt;/td&gt;
   &lt;td&gt;0.41
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.43&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;MMLU (5-shot weighted avg)
   &lt;/td&gt;
   &lt;td&gt;0.47
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.50&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Arc challenge
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.46&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.44
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Arc easy
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.74&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.71
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Boolq
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.78&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.76
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Copa
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.87&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.83
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Hellaswag
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.76&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.74
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Openbookqa
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.44&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.42
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Piqa
   &lt;/td&gt;
   &lt;td&gt;0.79
   &lt;/td&gt;
   &lt;td&gt;0.79
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Sciq
   &lt;/td&gt;
   &lt;td&gt;0.91
   &lt;/td&gt;
   &lt;td&gt;0.91
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Winogrande
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.69&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.67
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Truthfulqa
   &lt;/td&gt;
   &lt;td&gt;0.39
   &lt;/td&gt;
   &lt;td&gt;0.39
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;GSM8k (8-shot)
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;0.13&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;0.11
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;em&gt;Table 1: LM eval harness scores&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We observe that the model performs competitively with Llama2 (bolder is better).&lt;/p&gt;

&lt;h3 id=&quot;training-chronicles&quot;&gt;Training chronicles&lt;/h3&gt;

&lt;p&gt;Training was stable with no crashes, though we did observe a few hiccups:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;0-200B tokens&lt;/strong&gt;: We observed a slowdown in the iteration time (time taken to execute one training step). We stopped the job to ensure that the data loader was not causing any slowdowns and the checkpointing was performant and accurate. We did not find any issues. By this time, HSDP checkpointing code was available in PyTorch, and we took this opportunity to make the switch to PyTorch checkpointing code.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;200B tokens-1.9T&lt;/strong&gt;: We did not do any manual intervention in the job in late December. When we came back early January, disk space had exceeded and checkpoints were failing to be written, although the training job continued. The last known checkpoint was 1.5T.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.5T-1.7T&lt;/strong&gt;: We evaluated the 1.5T checkpoint with lm-evaluation-harness and discovered that model has been trained with an extra special token between two documents due to the Hugging Face tokenizer introducing a separator token and our dataloader also appending its own document separator. We modified the dataloader to eliminate the extra special token, and continued training with the modified dataloader from 1.7T token onwards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.7T-2T&lt;/strong&gt;: The loss initially spiked due to the change in the special tokens which was quickly recovered in a few billion tokens. The training finished without any other manual intervention!&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways-and-even-more-speed&quot;&gt;Key takeaways and even more speed&lt;/h3&gt;

&lt;p&gt;We demonstrated how one can use FSDP to train a model to 2T tokens with an excellent performance of 3700 tokens/sec/GPU and that generates a good quality model. As part of this exercise, we open sourced all our code for training and the knobs to achieve this throughput. These knobs can be leveraged by not only large-scale runs, but also smaller scale tuning runs. You can find the code &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;FSDP APIs implement the &lt;a href=&quot;https://pytorch.org/docs/stable/fsdp.html&quot;&gt;ZeRO&lt;/a&gt; algorithms in a PyTorch native manner and allow for tuning and training of large models. In the past, we have seen FSDP proof points (&lt;a href=&quot;https://github.com/tatsu-lab/stanford_alpaca&quot;&gt;Stanford Alpaca&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/blog/ram-efficient-pytorch-fsdp&quot;&gt;Hugging Face&lt;/a&gt;, &lt;a href=&quot;https://github.com/facebookresearch/llama-recipes&quot;&gt;Llama 2 recipes&lt;/a&gt;) on tuning a variety of LLMs (such as Meta Llama 2  7B to 70B Llama) using simple training loops and achieving good throughputs and training times.&lt;/p&gt;

&lt;p&gt;Finally, we note that there are several levers for speeding up training:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Node optimizations that can speedup specific operations (e.g., attention computation using Flash Attention V2)&lt;/li&gt;
  &lt;li&gt;Graph optimizations (e.g., fusing kernels, torch.compile)&lt;/li&gt;
  &lt;li&gt;Overlap in compute-communications&lt;/li&gt;
  &lt;li&gt;Activation recomputation&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have leveraged 1, 3, and a variation of 4 in this blog and are working closely with Team PyTorch at Meta to get torch.compile (2) as well as a more advanced version of 4 with per-operator selective activation recomputation. We plan to share a simple formatting code and example data to ingest into our data loader to enable others to use the code base for training of models.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;There are several teams that have been involved in reaching this proof point and we would like to thank the teams across Meta and IBM. Specifically, we extend our gratitude to the PyTorch distributed team, Facebook Research and Applied AI teams that built the &lt;a href=&quot;https://arxiv.org/abs/2304.11277&quot;&gt;FSDP APIs&lt;/a&gt; and made enhancements based on our feedback. We also wish to thank the data team at IBM Research that curated the data corpus used in this exercise and the infrastructure team at IBM Research (especially, Claudia Misale, Shweta Salaria, and Seetharami Seelam) that optimized NCCL and network configurations. By building and leveraging all of these components, we have successfully demonstrated the LlamaT proof point.&lt;/p&gt;

&lt;p&gt;The selective activation checkpointing was conceptualized at IBM by Linsong Chu, Davis Wertheimer, Mudhakar Srivatsa, and Raghu Ganti and implemented by Less Wright at Meta.&lt;/p&gt;

&lt;p&gt;Special thanks to &lt;a href=&quot;https://www.linkedin.com/in/stasbekman/?originalSubdomain=ca&quot;&gt;Stas Bekman&lt;/a&gt; and &lt;a href=&quot;https://minjiazhang.github.io/&quot;&gt;Minjia Zhang&lt;/a&gt;, who provided extensive feedback and helped improve the blog. Their insights have been invaluable in highlighting key aspects of optimizing the training and exploring further enhancements.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;h3 id=&quot;communication-computation-overlap&quot;&gt;Communication computation overlap&lt;/h3&gt;

&lt;p&gt;Another key aspect of training in a multi-node setting is the ability to overlap communication and computation. In FSDP, there are multiple opportunities for overlapping – during the FSDP unit gathering phase at forward pass as well as the backward pass computation. Overlapping the gather during forward pass while the computation of the previous unit and overlapping backward computation with the next unit gathering and gradient scattering help improve GPU utilization by nearly 2x. We illustrate this on the 400Gbps network interconnect with A100 80GB GPUs. In the case of HSDP, there is no inter-node traffic during the pre-fetch stage for forward pass and the overlap is only for the backward gradient computation phase. Of course, HSDP is feasible only when the model can be sharded within a single node, limiting the size of models to around 30B parameters.&lt;/p&gt;

&lt;p&gt;The below figure shows three steps in FSDP with the communication between nodes at the bottom and the compute stream at the top of the second half of the image. For the 7B model with no activation recomputation, we observe the overlap to be complete. In practice, the overlap percentage possible is 90% since the first block during forward pass and the last block during backward pass are not able to overlap.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/overlap_zoomed_out.png&quot; alt=&quot;three steps in FSDP with the communication between nodes at the bottom and the compute stream at the top of the second half&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A zoomed in view of the above three-step process is shown below for a single step. We can clearly see the granularity of the computation and communication and how they overlap in an interleaved manner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/maximizing-training/overlap_zoomed_in.png&quot; alt=&quot;zoomed in view of the above three-step process&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at IBM and Team PyTorch at Meta</name>
        
        
      </author>

      

      

      
        <summary type="html">In this blog, we demonstrate the scalability of FSDP with a pre-training exemplar, a 7B model trained for 2T tokens, and share various techniques we used to achieve a rapid training speed of 3,700 tokens/sec/GPU, or 40B tokens/day on 128 A100 GPUs. This translates to a model FLOPS utilization (MFU) and hardware FLOPS utilization (HFU) of 57%. Additionally, we have observed near linear scaling of FSDP to 512 GPUs, implying that training a 7B model on 512 GPUs to 2T tokens using this method would take just under two weeks.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2 paper and tutorial @ ASPLOS 2024</title>
      <link href="https://pytorch.org/blog/pytorch-2-paper-tutorial/" rel="alternate" type="text/html" title="PyTorch 2 paper and tutorial @ ASPLOS 2024" />
      <published>2024-02-06T00:00:00-08:00</published>
      <updated>2024-02-06T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch-2-paper-tutorial</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch-2-paper-tutorial/">&lt;p&gt;The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.&lt;/p&gt;

&lt;p&gt;The paper delves into the implementation of torch.compile and highlights the key technologies driving it, including TorchDynamo (graph capture), TorchInductor (backend compiler), and Dynamic Shape support.&lt;/p&gt;

&lt;p&gt;During the ASPLOS conference, we’ll be conducting a tutorial on Saturday, April 27, focusing on the inner workings of PyTorch 2 and how systems researchers can leverage and build upon it. Stay tuned for more details as the event approaches – we look forward to your participation!&lt;/p&gt;

&lt;p&gt;A preview of the paper is attached below:&lt;/p&gt;

&lt;p&gt;Title: &lt;strong&gt;PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.&lt;/strong&gt; &lt;a href=&quot;/assets/pytorch2-2.pdf&quot;&gt;&lt;strong&gt;Full Paper PDF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;abstract&quot;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI’s Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27x inference and 1.41x training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.&lt;/p&gt;

&lt;h3 id=&quot;authors&quot;&gt;Authors&lt;/h3&gt;

&lt;p&gt;Jason Ansel (Meta); Edward Yang (Meta); Horace He (Meta); Natalia Gimelshein (OpenAI); Animesh Jain (Meta); Michael Voznesensky (Meta); Bin Bao (Meta); Peter Bell (Quansight); David Berard (Meta); Evgeni Burovski Quansight; Geeta Chauhan (Meta); Anjali Chourdia (Meta); Will Constable (Meta); Alban Desmaison (Meta); Zachary DeVito (Meta); Elias Ellison (Meta); Will Feng (Meta); Jiong Gong (Intel); Michael Gschwind (Meta); Brian Hirsh (Meta); Sherlock Huang (Meta); Kshiteej Kalambarkar (Quansight); Laurent Kirsch (Meta); Michael Lazos (Meta); Mario Lezcano (Quansight); Yanbo Liang (Meta); Jason Liang (Meta); Yinghai Lu (Meta); CK Luk (Meta); Bert Maher (Meta); Yunjie Pan (University of Michigan); Christian Puhrsch (Meta); Matthias Reso (Meta); Mark Saroufim (Meta); Marcos Yukio Siraichi (Quansight); Helen Suk (Meta); Michael Suo (Meta); Phil Tillet (OpenAI); Eikan Wang (Intel); Xiaodong Wang (Meta); William Wen (Meta); Shunting Zhang (Meta); Xu Zhao (Meta); Keren Zhou (OpenAI &amp;amp; George Mason University); Richard Zou (Meta); Ajit Mathews (Meta); Gregory Chanan (Meta); Peng Wu (Meta); Soumith Chintala (Meta)&lt;/p&gt;

&lt;h3 id=&quot;asplos24---full-day-tutorial-schedule&quot;&gt;ASPLOS’24 - Full Day Tutorial Schedule&lt;/h3&gt;

&lt;p&gt;Full schedule for the ASPLOS’24 PyTorch 2 Tutoral on Saturday, April 27th is available &lt;a href=&quot;https://github.com/pytorch/workshops/tree/master/ASPLOS_2024&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">The PyTorch team is excited to share that our paper on PyTorch 2 has been accepted for presentation at the ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), scheduled to take place from April 27 to May 1, 2024, in San Diego, CA, USA.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">What’s New in PyTorch Documentation</title>
      <link href="https://pytorch.org/blog/new-in-docs/" rel="alternate" type="text/html" title="What's New in PyTorch Documentation" />
      <published>2024-02-01T00:00:00-08:00</published>
      <updated>2024-02-01T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/new-in-docs</id>
      <content type="html" xml:base="https://pytorch.org/blog/new-in-docs/">&lt;p&gt;Greetings to the PyTorch community! Here is a quick update on PyTorch docs.&lt;/p&gt;

&lt;p&gt;In November 2023, we successfully conducted a &lt;a href=&quot;https://pytorch.org/blog/pytorch-docathon-h2-2023-wrap/&quot;&gt;PyTorch Docathon&lt;/a&gt;, a community event where PyTorch community members gathered together to improve PyTorch documentation and tutorials. This event saw a global participation of contributors who dedicated their time and effort to enhance our docs. We extend our sincere gratitude to everyone involved.&lt;/p&gt;

&lt;p&gt;A key accomplishment of the Docathon was the comprehensive work carried out on docstrings. Our community contributors meticulously reviewed and improved the docstrings based on the provided tasks.&lt;/p&gt;

&lt;p&gt;In addition to that, we’ve added three new tutorials that showcase real-world applications of PyTorch. We are particularly proud that two of these tutorials were contributed by PyTorch ecosystem partners.&lt;/p&gt;

&lt;p&gt;Here is the new tutorials for you to explore:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/tiatoolbox_tutorial.html&quot;&gt;Whole Slide Image Classification Using PyTorch and TIAToolbox&lt;/a&gt; —This tutorial demonstrates how to classify Whole Slide Images (WSIs) using PyTorch deep learning models with TIAToolbox, which are images of human tissue samples used by pathologists and researchers to study diseases like cancer at the microscopic level.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/advanced/usb_semisup_learn.html&quot;&gt;Semi-Supervised Learning using USB built upon PyTorch&lt;/a&gt; – This tutorial introduces USB, a flexible and modular semi-supervised learning framework based on PyTorch, demonstrating its ease of use in training a FreeMatch/SoftMatch model on CIFAR-10 using pre-trained ViT and its adaptability to various algorithms and imbalanced datasets.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/torchserve_vertexai_tutorial.html&quot;&gt;Deploying a PyTorch Stable Diffusion model as a Vertex AI Endpoint&lt;/a&gt; – This tutorial provides a step-by-step guide on how to streamline the deployment of a PyTorch Stable Diffusion model (v1.5) using Vertex AI, a fully-managed machine learning platform, by creating a custom TorchServe handler, uploading model artifacts to Google Cloud Storage, creating a Vertex AI model with the model artifacts and a prebuilt PyTorch container image, and finally deploying the model onto an endpoint.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’re planning more community events this year, so stay tuned!&lt;/p&gt;

&lt;p&gt;And finally, we just published new 2.2 PyTorch &lt;a href=&quot;https://pytorch.org/docs/&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/&quot;&gt;tutorials&lt;/a&gt;. Check it out!&lt;/p&gt;

&lt;p&gt;Best regards,&lt;br /&gt;
The PyTorch Team&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Greetings to the PyTorch community! Here is a quick update on PyTorch docs.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">New Library Updates in PyTorch 2.2</title>
      <link href="https://pytorch.org/blog/pytorch2-2-lib-updates/" rel="alternate" type="text/html" title="New Library Updates in PyTorch 2.2" />
      <published>2024-01-30T00:00:00-08:00</published>
      <updated>2024-01-30T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch2-2-lib-updates</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-2-lib-updates/">&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;We are bringing a number of improvements to the current PyTorch libraries, alongside the PyTorch 2.2 release. These updates demonstrate our focus on developing common and extensible APIs across all domains to make it easier for our community to build ecosystem projects on PyTorch.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td colspan=&quot;3&quot; style=&quot;font-weight: 600; text-align: center;&quot;&gt;Latest Stable Library Versions (&lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Full List&lt;/a&gt;)*
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchArrow 0.1.0
   &lt;/td&gt;
   &lt;td&gt;TorchRec 0.6.0
   &lt;/td&gt;
   &lt;td&gt;TorchVision 0.17
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchAudio 2.2.0
   &lt;/td&gt;
   &lt;td&gt;TorchServe 0.9.0
   &lt;/td&gt;
   &lt;td&gt;TorchX 0.7.0
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;TorchData 0.7.1 
   &lt;/td&gt;
   &lt;td&gt;TorchText 0.17.0
   &lt;/td&gt;
   &lt;td&gt;PyTorch on XLA Devices 2.1
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;prior versions&lt;/a&gt; or (unstable) nightlies, click on versions in the top left menu above ‘Search Docs’.&lt;/p&gt;

&lt;h2 id=&quot;torchrl&quot;&gt;TorchRL&lt;/h2&gt;

&lt;h3 id=&quot;feature-torchrls-offline-rl-data-hub&quot;&gt;Feature: TorchRL’s Offline RL Data Hub&lt;/h3&gt;

&lt;p&gt;TorchRL now provides one of the largest dataset hubs for offline RL and imitation learning, and it all comes under a single data format (TED, for TorchRL Episode Data format). This makes it possible to easily swap from different sources in a single training loop. It is also now possible to easily combine datasets of different sources through the ReplayBufferEnsemble class. The data processing is fully customizable. Sources include simulated tasks (Minari, D4RL, VD4RL), robotic datasets (Roboset, OpenX Embodied dataset) and gaming (GenDGRL/ProcGen, Atari/DQN). Check these out in the &lt;a href=&quot;https://pytorch.org/rl/reference/data.html#datasets&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Aside from these changes, our replay buffers can now be dumped on disk using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dumps()&lt;/code&gt; method which will serialize the buffers on disk using the TensorDict API which is faster, safer and more efficient than using torch.save.&lt;/p&gt;

&lt;p&gt;Finally, replay buffers can now be  read and written from separate processes on the same machine without any extra code needed from the user!&lt;/p&gt;

&lt;h3 id=&quot;torchrl2gym-environment-api&quot;&gt;TorchRL2Gym environment API&lt;/h3&gt;

&lt;p&gt;To facilitate TorchRL’s integration in existing code-bases and enjoy all the features of TorchRL’s environment API (execution on device, batched operations, transforms…) we provide a TorchRL-to-gym API that allows users to register any environment they want in gym or gymnasium. This can be used in turn to make TorchRL a universal lib-to-gym converter that works across stateless (eg, dm_control) and stateless (Brax, Jumanji) environments. The feature is thoroughly detailed in the &lt;a href=&quot;https://pytorch.org/rl/reference/generated/torchrl.envs.EnvBase.html#torchrl.envs.EnvBase.register_gym&quot;&gt;doc&lt;/a&gt;. The info_dict reading API has also been improved.&lt;/p&gt;

&lt;h3 id=&quot;environment-speedups&quot;&gt;Environment speedups&lt;/h3&gt;

&lt;p&gt;We added the option of executing environments on a different environment than the one used to deliver data in ParallelEnv. We also speeded up the GymLikeEnv class to a level that now makes it competitive with gym itself.&lt;/p&gt;

&lt;h3 id=&quot;scaling-objectives&quot;&gt;Scaling objectives&lt;/h3&gt;

&lt;p&gt;The most popular objectives for RLHF and training at scale (PPO and A2C) are now compatible with FSDP and DDP models!&lt;/p&gt;

&lt;h2 id=&quot;tensordict&quot;&gt;TensorDict&lt;/h2&gt;

&lt;h3 id=&quot;feature-memorymappedtensor-to-replace-memmaptensor&quot;&gt;Feature: MemoryMappedTensor to replace MemmapTensor&lt;/h3&gt;

&lt;p&gt;We provide a much more efficient mmap backend for TensorDict; MemoryMappedTensor, which directly subclasses torch.Tensor. It comes with a bunch of utils to be constructed, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_tensor&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;empty&lt;/code&gt; and many more. MemoryMappedTensor is now much safer and faster than its counterpart. The library remains fully compatible with the previous class to facilitate transition.&lt;/p&gt;

&lt;p&gt;We also introduce a new set of multithreaded serialization methods that make tensordict serialization highly competitive with torch.save, with serialization and deserialization speeds for LLMs more than &lt;a href=&quot;https://github.com/pytorch/tensordict/pull/592#issuecomment-1850761831&quot;&gt;3x faster than with torch.save&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;feature-non-tensor-data-within-tensordict&quot;&gt;Feature: Non-tensor data within TensorDict&lt;/h3&gt;

&lt;p&gt;It is not possible to carry non-tensor data through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NonTensorData&lt;/code&gt; tensorclass. This makes it possible to build tensordicts with metadata. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memmap&lt;/code&gt;-API is fully compatible with these values, allowing users to seamlessly serialize and deserialize such objects. To store non-tensor data in a tensordict, simply assign it using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__setitem__&lt;/code&gt; method.&lt;/p&gt;

&lt;h3 id=&quot;efficiency-improvements&quot;&gt;Efficiency improvements&lt;/h3&gt;

&lt;p&gt;Several methods runtime have been improved, such as unbind, split, map or even TensorDict instantiation. Check our &lt;a href=&quot;https://pytorch.org/tensordict/dev/bench/&quot;&gt;benchmarks&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;torchrecfbgemm_gpu&quot;&gt;TorchRec/fbgemm_gpu&lt;/h2&gt;

&lt;h3 id=&quot;vbe&quot;&gt;VBE&lt;/h3&gt;

&lt;p&gt;TorchRec now natively supports VBE (variable batched embeddings) within the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EmbeddingBagCollection&lt;/code&gt; module. This allows variable batch size per feature, unlocking sparse input data deduplication, which can greatly speed up embedding lookup and all-to-all time. To enable, simply initialize &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KeyedJaggedTensor &lt;/code&gt;with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stride_per_key_per_rank&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inverse_indices&lt;/code&gt; fields, which specify batch size per feature and inverse indices to reindex the embedding output respectively.&lt;/p&gt;

&lt;p&gt;In addition to the TorchRec library changes, &lt;a href=&quot;https://pytorch.org/FBGEMM/&quot;&gt;fbgemm_gpu&lt;/a&gt; has added the support for variable batch size per feature in TBE. &lt;a href=&quot;https://github.com/pytorch/FBGEMM/pull/1752&quot;&gt;VBE&lt;/a&gt; is enabled on split TBE training for both weighted and unweighted cases. To use VBE, please make sure to use the latest fbgemm_gpu version.&lt;/p&gt;

&lt;h3 id=&quot;embedding-offloading&quot;&gt;Embedding offloading&lt;/h3&gt;

&lt;p&gt;This technique refers to using CUDA UVM to cache ‘hot’ embeddings (i.e. store embedding tables on host memory with cache on HBM memory), and prefetching the cache. Embedding offloading allows running a larger model with fewer GPUs, while maintaining competitive performance. Use the prefetching pipeline (&lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/train_pipeline.py?#L1056&quot;&gt;PrefetchTrainPipelineSparseDist&lt;/a&gt;) and pass in &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/types.py#L457&quot;&gt;per-table cache load factor&lt;/a&gt; and the &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/types.py#L460&quot;&gt;prefetch_pipeline&lt;/a&gt; flag through constraints in the planner to use this feature.&lt;/p&gt;

&lt;p&gt;Fbgemm_gpu has introduced &lt;a href=&quot;https://github.com/pytorch/FBGEMM/pull/1893&quot;&gt;UVM cache pipeline prefetching&lt;/a&gt; in &lt;a href=&quot;https://github.com/pytorch/FBGEMM/releases/tag/v0.5.0&quot;&gt;v0.5.0&lt;/a&gt; for TBE performance speedup. This allows cache-insert to be executed in parallel with TBE forward/backward. To enable this feature, please be sure to use the latest fbgemm_gpu version.&lt;/p&gt;

&lt;h3 id=&quot;trecshardshard_modules&quot;&gt;Trec.shard/shard_modules&lt;/h3&gt;

&lt;p&gt;These APIs replace embedding submodules with its sharded variant. The shard API applies to an individual embedding module while the shard_modules API replaces all embedding modules and won’t touch other non-embedding submodules.&lt;/p&gt;

&lt;p&gt;Embedding sharding follows similar behavior to the prior TorchRec DistributedModuleParallel behavior, except the ShardedModules have been made composable, meaning the modules are backed by &lt;a href=&quot;https://github.com/pytorch/torchrec/blob/main/torchrec/distributed/composable/table_batched_embedding_slice.py#L15&quot;&gt;TableBatchedEmbeddingSlices&lt;/a&gt; which are views into the underlying TBE (including .grad). This means that fused parameters are now returned with named_parameters(), including in DistributedModuleParallel.&lt;/p&gt;

&lt;h2 id=&quot;torchvision&quot;&gt;TorchVision&lt;/h2&gt;

&lt;h3 id=&quot;the-v2-transforms-are-now-stable&quot;&gt;The V2 transforms are now stable!&lt;/h3&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchvision.transforms.v2&lt;/code&gt; namespace was still in BETA stage until now. It is now stable! Whether you’re new to Torchvision transforms, or you’re already experienced with them, we encourage you to start with &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py&quot;&gt;Getting started with transforms v2&lt;/a&gt; in order to learn more about what can be done with the new v2 transforms.&lt;/p&gt;

&lt;p&gt;Browse our &lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#&quot;&gt;main docs&lt;/a&gt; for general information and performance tips. The available transforms and functionals are listed in the &lt;a href=&quot;https://pytorch.org/vision/stable/transforms.html#v2-api-ref&quot;&gt;API reference&lt;/a&gt;. Additional information and tutorials can also be found in our &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/index.html#gallery&quot;&gt;example gallery&lt;/a&gt;, e.g. &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_e2e.html#sphx-glr-auto-examples-transforms-plot-transforms-e2e-py&quot;&gt;Transforms v2: End-to-end object detection/segmentation example&lt;/a&gt; or &lt;a href=&quot;https://pytorch.org/vision/stable/auto_examples/transforms/plot_custom_transforms.html#sphx-glr-auto-examples-transforms-plot-custom-transforms-py&quot;&gt;How to write your own v2 transforms&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;towards-torchcompile-support&quot;&gt;Towards &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; support&lt;/h3&gt;

&lt;p&gt;We are progressively adding support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile()&lt;/code&gt; to torchvision interfaces, reducing graph breaks and allowing dynamic shape.&lt;/p&gt;

&lt;p&gt;The torchvision ops (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nms&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[ps_]roi_align&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[ps_]roi_pool&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;deform_conv_2d&lt;/code&gt;) are now compatible with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; and dynamic shapes.&lt;/p&gt;

&lt;p&gt;On the transforms side, the majority of &lt;a href=&quot;https://github.com/pytorch/vision/blob/main/torchvision/transforms/v2/functional/__init__.py&quot;&gt;low-level kernels&lt;/a&gt; (like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resize_image()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;crop_image()&lt;/code&gt;) should compile properly without graph breaks and with dynamic shapes. We are still addressing the remaining edge-cases, moving up towards full functional support and classes, and you should expect more progress on that front with the next release.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.2: FlashAttention-v2 integration, AOTInductor</title>
      <link href="https://pytorch.org/blog/pytorch2-2/" rel="alternate" type="text/html" title="PyTorch 2.2: FlashAttention-v2 integration, AOTInductor" />
      <published>2024-01-30T00:00:00-08:00</published>
      <updated>2024-01-30T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/pytorch2-2</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-2/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.2 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.2.0&quot;&gt;release note&lt;/a&gt;)!  PyTorch 2.2 offers ~2x performance improvements to &lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;scaled_dot_product_attention&lt;/a&gt;&lt;/em&gt; via &lt;a href=&quot;https://arxiv.org/abs/2307.08691&quot;&gt;FlashAttention-v2&lt;/a&gt; integration, as well as &lt;em&gt;AOTInductor&lt;/em&gt;, a new ahead-of-time compilation and deployment tool built for  non-python server-side deployments.&lt;/p&gt;

&lt;p&gt;This release also includes improved &lt;em&gt;torch.compile&lt;/em&gt; support for Optimizers, a number of new inductor optimizations, and a new logging mechanism called TORCH_LOGS.&lt;/p&gt;

&lt;p&gt;Please note that we are &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/114602&quot;&gt;deprecating macOS x86 support&lt;/a&gt;, and PyTorch 2.2.x will be the last version that supports macOS x64.&lt;/p&gt;

&lt;p&gt;Along with 2.2, we are also releasing a series of updates to the PyTorch domain libraries. More details can be found in the library updates blog.&lt;/p&gt;

&lt;p&gt;This release is composed of 3,628 commits and 521 contributors since PyTorch 2.1. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.2.  More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;p&gt;Summary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;scaled_dot_product_attention&lt;/a&gt;&lt;/em&gt; (SDPA) now supports &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.08691&quot;&gt;FlashAttention-2&lt;/a&gt;&lt;/em&gt;, yielding around 2x speedups compared to previous versions.&lt;/li&gt;
  &lt;li&gt;PyTorch 2.2 introduces a new ahead-of-time extension of &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747&quot;&gt;TorchInductor&lt;/a&gt; called &lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt;AOTInductor&lt;/a&gt;&lt;/em&gt;, designed to compile and deploy PyTorch programs for non-python server-side.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;torch.distributed&lt;/em&gt; supports a new abstraction for initializing and representing ProcessGroups called &lt;em&gt;&lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_device_mesh.html&quot;&gt;device_mesh&lt;/a&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;PyTorch 2.2 ships a standardized, configurable logging mechanism called &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_logs.html&quot;&gt;TORCH_LOGS&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A number of &lt;em&gt;torch.compile&lt;/em&gt; improvements are included in PyTorch 2.2, including improved support for compiling Optimizers and improved TorchInductor fusion and layout optimizations.&lt;/li&gt;
  &lt;li&gt;Please note that we are &lt;a href=&quot;https://github.com/pytorch/pytorch/issues/114602&quot;&gt;deprecating macOS x86 support&lt;/a&gt;, and PyTorch 2.2.x will be the last version that supports macOS x64.&lt;/li&gt;
&lt;/ul&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td style=&quot;width:25%&quot;&gt;
&lt;strong&gt;Stable&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt; 
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.ok7v7pq0igzw&quot;&gt;FlashAttention-2 Integration&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.rk3gf4pgy5m9&quot;&gt;Inductor optimizations&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.3qfc7y6r1dog&quot;&gt;AOTInductor&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.gfep1ccb8bvk&quot;&gt;aarch64 optimizations&lt;/a&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.n2lkw22a8l2m&quot;&gt;TORCH_LOGS&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;em&gt;&lt;a href=&quot;#bookmark=id.h50nybtt0fdm&quot;&gt;device_mesh&lt;/a&gt;&lt;/em&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;a href=&quot;#bookmark=id.1lx0dkeu5zqt&quot;&gt;Optimizer compilation&lt;/a&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-flashattention-2-support-in-torchnnfunctionalscaled_dot_product_attention&quot;&gt;[Beta] FlashAttention-2 support in &lt;em&gt;torch.nn.functional.scaled_dot_product_attention&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;&lt;a href=&quot;https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html&quot;&gt;torch.nn.functional.scaled_dot_product_attention&lt;/a&gt;&lt;/em&gt; (SDPA) now supports FlashAttention-2, yielding around 2x speedups (compared to the previous version) and reaching ~50-73% of theoretical maximum FLOPs/s on A100 GPUs.&lt;/p&gt;

&lt;p&gt;More information is available on FlashAttention-2 in &lt;a href=&quot;https://arxiv.org/abs/2307.08691&quot;&gt;this paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For a tutorial on how to use SDPA please see &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-aotinductor-ahead-of-time-compilation-and-deployment-for-torchexport-ed-programs&quot;&gt;[Beta] AOTInductor: ahead-of-time compilation and deployment for torch.export-ed programs&lt;/h3&gt;

&lt;p&gt;AOTInductor is an extension of &lt;a href=&quot;https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747&quot;&gt;TorchInductor&lt;/a&gt;, designed to process exported PyTorch models, optimize them, and produce shared libraries as well as other relevant artifacts. These compiled artifacts can be deployed in non-Python environments, which are frequently employed for inference on the server-side.  Note that AOTInductor supports the same backends as Inductor, including CUDA, ROCm, and CPU.&lt;/p&gt;

&lt;p&gt;For more information please see the &lt;a href=&quot;https://pytorch.org/docs/main/torch.compiler_aot_inductor.html&quot;&gt;AOTInductor tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-fine-grained-configurable-logging-via-torch_logs&quot;&gt;[Beta] Fine-grained configurable logging via TORCH_LOGS&lt;/h3&gt;

&lt;p&gt;PyTorch now ships a standardized, configurable logging mechanism that can be used to analyze the status of various subsystems such as compilation and distributed operations.&lt;/p&gt;

&lt;p&gt;Logs can be enabled via the TORCH_LOGS environment variable.  For example, to set the log level of TorchDynamo to logging.ERROR and the log level of TorchInductor to logging.DEBUG pass &lt;em&gt;TORCH_LOGS=”-dynamo,+inductor”&lt;/em&gt; to PyTorch.&lt;/p&gt;

&lt;p&gt;For more information, please see the logging &lt;a href=&quot;https://pytorch.org/docs/2.2/logging.html&quot;&gt;documentation&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_logs.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-torchdistributeddevice_mesh&quot;&gt;[Beta] torch.distributed.device_mesh&lt;/h3&gt;

&lt;p&gt;PyTorch 2.2 introduces a new abstraction for representing the ProcessGroups involved in distributed parallelisms called &lt;em&gt;torch.distributed.device_mesh&lt;/em&gt;. This abstraction allows users to represent inter-node and intra-node process groups via an N-dimensional array where, for example, one dimension can data parallelism in FSDP while another could represent tensor parallelism within FSDP.&lt;/p&gt;

&lt;p&gt;For more information, see the &lt;a href=&quot;https://pytorch.org/tutorials/recipes/distributed_device_mesh.html&quot;&gt;device_mesh tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-improvements-to-torchcompile-ing-optimizers&quot;&gt;[Beta] Improvements to &lt;em&gt;torch.compile&lt;/em&gt;-ing Optimizers&lt;/h3&gt;

&lt;p&gt;A number of improvements have been made to torch.compile-ing Optimizers including less overhead and support for cuda graphs.&lt;/p&gt;

&lt;p&gt;More technical details of the improvements are available on &lt;a href=&quot;https://dev-discuss.pytorch.org/t/compiling-the-optimizer-with-pt2/1669&quot;&gt;dev-discuss&lt;/a&gt;, and a recipe for &lt;em&gt;torch.compile&lt;/em&gt;-ing optimizers is available &lt;a href=&quot;https://pytorch.org/tutorials/recipes/compiling_optimizer.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;inductor-performance-optimizations&quot;&gt;Inductor Performance Optimizations&lt;/h3&gt;

&lt;p&gt;A number of performance optimizations have been added to TorchInductor including &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/111437&quot;&gt;horizontal fusion support for torch.concat&lt;/a&gt;, &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/114600&quot;&gt;improved convolution layout optimizations&lt;/a&gt;, and improved &lt;em&gt;scaled_dot_product_attention&lt;/em&gt; &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/109156&quot;&gt;pattern&lt;/a&gt; &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/110001&quot;&gt;matching&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For a complete list of inductor optimizations, please see the &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/v2.2.0&quot;&gt;Release Notes&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;aarch64-performance-optimizations&quot;&gt;aarch64 Performance Optimizations&lt;/h3&gt;

&lt;p&gt;PyTorch 2.2 includes a number of performance enhancements for aarch64 including support for &lt;a href=&quot;https://github.com/pytorch/pytorch/pull/115037/files&quot;&gt;mkldnn weight pre-packing&lt;/a&gt;, improved &lt;a href=&quot;https://github.com/intel/ideep&quot;&gt;ideep&lt;/a&gt; &lt;a href=&quot;https://github.com/intel/ideep/pull/261&quot;&gt;primitive caching&lt;/a&gt;, and improved inference speed via &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/pull/1590&quot;&gt;fixed format kernel improvements&lt;/a&gt; to &lt;a href=&quot;https://github.com/oneapi-src/oneDNN/&quot;&gt;OneDNN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For a complete list of aarch64 optimizations, please see the &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/v2.2.0&quot;&gt;Release Notes&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.2 (release note)! PyTorch 2.2 offers ~2x performance improvements to scaled_dot_product_attention via FlashAttention-v2 integration, as well as AOTInductor, a new ahead-of-time compilation and deployment tool built for non-python server-side deployments.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Generative AI with PyTorch IV: Seamless M4T, fast</title>
      <link href="https://pytorch.org/blog/accelerating-generative-ai-4/" rel="alternate" type="text/html" title="Accelerating Generative AI with PyTorch IV: Seamless M4T, fast" />
      <published>2024-01-23T00:00:00-08:00</published>
      <updated>2024-01-23T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-generative-ai-4</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-generative-ai-4/">&lt;p&gt;This post is the fourth part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. To skip to the code, check out our github (&lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/pull/328&quot;&gt;seamless_communication&lt;/a&gt;, &lt;a href=&quot;https://github.com/facebookresearch/fairseq2/pull/272&quot;&gt;fairseq2&lt;/a&gt;). We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai/&quot;&gt;Segment Anything over 8x&lt;/a&gt; using only pure, native PyTorch. In part two, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-2/&quot;&gt;Llama-7B by almost 10x&lt;/a&gt; using only native PyTorch optimizations. In part three, we showed how to accelerate &lt;a href=&quot;https://pytorch.org/blog/accelerating-generative-ai-3/&quot;&gt;text-to-image diffusion models up to 3x&lt;/a&gt; using only native Pytorch optimizations.&lt;/p&gt;

&lt;p&gt;In this blog, we’ll focus on speeding up FAIR’s Seamless M4T-v2 model resulting in &lt;strong&gt;2x speedup for text decoder module &lt;em&gt;and&lt;/em&gt; 30x for vocoder module, resulting in 2.7x speedup for end-to-end inference&lt;/strong&gt;, with no loss of accuracy by using CUDA Graph and native PyTorch optimization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&quot;&gt;torch.compile&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg1.png&quot; alt=&quot;End to End Inference Speedup&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Seamless M4T is an open-source foundational speech/text translation and transcription technology developed by FAIR. Seamless M4T is a massively multilingual and multimodal machine translation model, with the &lt;a href=&quot;https://github.com/facebookresearch/seamless_communication&quot;&gt;latest version&lt;/a&gt; (Seamless M4T-v2) released on November 30th, 2023. The high-level model architecture of Seamless M4T-v2 is illustrated in Figure 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg2.png&quot; alt=&quot;Model Architecture of Seamless M4T-v2&quot; style=&quot;width:100%;max-width:600px; display:block; margin-left: auto; margin-right: auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; Model Architecture of Seamless M4T-v2.&lt;/p&gt;

&lt;p&gt;Accelerating inference latency is crucial for translation models to improve user experience through faster communication across languages. In particular, batch_size=1 is often used for fast translation where latency matters a lot in applications such as chatbots, speech translation, and live subtitling. Therefore, we conducted the performance analysis on inference with batch_size=1, as shown in Figure 2 to understand the Amdahl’s Law bottleneck. Our results indicate that the text decoder and vocoder are the most time-consuming modules, accounting for 61% and 23% of the inference time, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg3.png&quot; alt=&quot;Text decoder and vocoder are the most time consuming module. Breakdown of inference time by modules for English-Spanish S2ST (Speech-to-Speech-Text) task for batch_size=1 on A100 GPU.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; &lt;span style=&quot;text-decoration:underline;&quot;&gt;Text decoder and vocoder are the most time consuming module&lt;/span&gt;. Breakdown of inference time by modules for English-Spanish S2ST (Speech-to-Speech-Text) task for batch_size=1 on A100 GPU.&lt;/p&gt;

&lt;p&gt;To take a closer look at the performance bottleneck of the text decoder and vocoder, we analyzed GPU traces for the text decoder and vocoder for the 8th sample for the English-Spanish translation example of &lt;a href=&quot;https://huggingface.co/datasets/google/fleurs&quot;&gt;FLEURS&lt;/a&gt; dataset as shown in Figure 3. It revealed that the &lt;strong&gt;text decoder and vocoder are heavily CPU-bound modules.&lt;/strong&gt; We observed a significant gap incurred by CPU overhead that delayed the launch of GPU kernels, resulting in a substantial increase in the execution time for both the modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg4.jpg&quot; alt=&quot;CPU and GPU trace for Text Decoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; CPU and GPU trace for Text Decoder&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg5.jpg&quot; alt=&quot;CPU and GPU trace for Vocoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b)&lt;/strong&gt; CPU and GPU trace for Vocoder&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; &lt;span style=&quot;text-decoration:underline;&quot;&gt;Text Decoder and Vocoder are heavily CPU-bound modules&lt;/span&gt;. CPU and GPU trace for (a) Text Decoder (b) Vocoder for the 8th sample for English-Spanish translation example of &lt;a href=&quot;https://huggingface.co/datasets/google/fleurs&quot;&gt;FLEURS&lt;/a&gt; dataset. The trace is obtained by running inference with batch_size=1 on A100 gpu.&lt;/p&gt;

&lt;p&gt;Based on the real-system performance analysis results that text_decoder and vocoder are heavily CPU bound modules in Seamless M4T-v2, we enabled torch.compile + CUDA Graph to those modules. In this post, we share modifications required to enable torch.compile + CUDA Graph on each module for batch_size=1 inference scenario, discussion on CUDA Graph and next step plans.&lt;/p&gt;

&lt;h2 id=&quot;torchcompile-with-cuda-graph&quot;&gt;Torch.compile with CUDA Graph&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a PyTorch API that allows users to compile PyTorch models into a standalone executable or script which is generally used for optimizing model performance by removing unnecessary overhead.&lt;/p&gt;

&lt;p&gt;CUDA Graph is a feature provided by NVIDIA that allows for the optimization of kernel launches in CUDA applications. It creates an execution graph of CUDA kernels, which can be pre-processed and optimized by the driver before being executed on the GPU. The main advantage of using CUDA Graph is that it reduces the overhead associated with launching individual kernels, as the graph can be launched as a single unit, reducing the number of API calls and data transfers between the host and device. This can lead to significant performance improvements, especially for applications that have a large number of small kernels or repeat the same set of kernels multiple times. If this is something you are interested in learning more about, check out this paper that highlights the important role of data for accelerated computing: &lt;strong&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/5762730&quot;&gt;Where is the data? Why you cannot debate CPU vs. GPU performance without the answer&lt;/a&gt;&lt;/strong&gt; by our own Kim Hazelwood! This is when NVIDIA was heavily investing in general-purpose GPU (GPGPUs) and before deep learning revolutionized the computing industry!&lt;/p&gt;

&lt;p&gt;However, because CUDA Graph operates on 1) fixed memory pointer, 2) fixed shape of tensors, that are recorded at the compile time, we introduced the following improvements for CUDA Graph to be reused across multiple sizes of inputs to &lt;em&gt;prevent CUDA Graph generation for each iteration&lt;/em&gt; and let the data inside CUDA Graph be reused across different runs &lt;em&gt;to share KV Cache for multiple decoding steps&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;text-decoder&quot;&gt;Text Decoder&lt;/h2&gt;

&lt;p&gt;The Text Decoder in Seamless is a decoder from NLLB [&lt;a href=&quot;https://ai.meta.com/research/no-language-left-behind/&quot;&gt;1&lt;/a&gt;] that performs T2TT (Text to Text Translation). Also, this module is a CPU-bound model where gpu execution time is not long enough to hide CPU overhead because of &lt;strong&gt;the nature of auto-regressive generation that requires sequential processing of tokens&lt;/strong&gt;, which limits the amount of parallelism that can be achieved on the GPU. Based on this observation, we enabled torch.compile + CUDA Graph for the text decoders to reduce the dominating  CPU overhead as shown in Figure 4.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg6.png&quot; alt=&quot;CPU and GPU trace for Text Decoder after torch.compile + CUDA Graph are enabled&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; CPU and GPU trace for Text Decoder after torch.compile + CUDA Graph are enabled.&lt;/p&gt;

&lt;h3 id=&quot;1--updating-and-retrieving-kv-cache&quot;&gt;1.  Updating and retrieving KV cache&lt;/h3&gt;

&lt;p&gt;During inference, the text decoder has two computation phases: a prefill phase that consumes the prompt and an incremental generation phase that generates output tokens one by one. Given a high enough batch size or input length, prefill operates on a sufficiently high number of tokens in parallel — GPU performance is the bottleneck and the CPU overheads do not impact performance significantly. On the other hand, incremental token generation is always executed with sequence length 1 and it is often executed with a small batch size (even 1), e.g. for interactive use cases. Thus, incremental generation can be limited by the CPU speed and thus is a good candidate for torch.compile + CUDA Graph.&lt;/p&gt;

&lt;p&gt;However, during the incremental token generation phase, the sequence_length dimension of key and value involved in the attention computation increases by one with each step while the sequence length of query always remains 1. Specifically, key/value are generated by appending the newly computed key/value of sequence length 1 to the key/value stored in the KV cache so far. But as mentioned above, CUDA Graph records all the shapes of tensors during compilation and replay with the recorded shapes. Thus, few modifications have been made to address this issue following the great work &lt;a href=&quot;https://blog.fireworks.ai/speed-python-pick-two-how-cuda-graphs-enable-fast-python-code-for-deep-learning-353bf6241248&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;a) We modify the KV-cache handling to take the indices in which to write new values in a CUDA Tensor (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt;) rather than a Python integer.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg7.png&quot; alt=&quot;Modification to KV cache append and get&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; Modification to KV cache &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;append&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;b) We also modify attention to work with the fixed shape of key and value over the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_length&lt;/code&gt;. We only compute softmax over the sequence positions up to the current decoding step (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt;) . To mask out sequence positions &amp;gt; current decoding step (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos)&lt;/code&gt;, we create a boolean mask tensor (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;) where sequence positions &amp;gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt; are set to False.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg8.png&quot; alt=&quot;Helper function to generate valid_seq_pos and mask&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; Helper function to generate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;valid_seq_pos&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;It’s important to post that these modifications result in an increase in the amount of computation required, as we compute attention over more sequence positions than necessary (up to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_seq_length&lt;/code&gt;). However, despite this drawback, our results demonstrate that torch.compile + CUDA Graph still provide significant performance benefits compared to standard PyTorch code.&lt;/p&gt;

&lt;p&gt;c) As different inference samples have different sequence length, it also generates different shapes of inputs that are to be projected to key and value for the cross attention layers. Thus, we pad the input to have a static shape and generate a padding mask to mask out padded output.&lt;/p&gt;

&lt;h3 id=&quot;2-memory-pointer-management&quot;&gt;2. Memory Pointer Management&lt;/h3&gt;

&lt;p&gt;As CUDA Graph records memory pointers along with the shape of tensors, it is important to make different inference samples to correctly reference the recorded memory pointer (e.g., KV cache) to avoid compiling CUDA Graph for each inference sample. However, some parts of the Seamless codebase made different inference samples to refer to different memory addresses, so we made modifications to improve the memory implications.&lt;/p&gt;

&lt;p&gt;e) Seamless adopts beam search as a text decoding strategy. In the beam search process, we need to perform KV cache reordering for all the attention layers for each incremental decoding step to make sure each selected beam performs with corresponding KV cache as shown in the code snippet below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg8b.png&quot; alt=&quot;KV cache reordering operation for beam search decoding strategy&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; KV cache reordering operation for beam search decoding strategy.&lt;/p&gt;

&lt;p&gt;The above code allocates new memory space and overwrites the original memory pointer for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_k&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cache_v&lt;/code&gt;. Thus we modified KV cache reordering to keep the memory pointer of each cache as was recorded during compilation by using &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html&quot;&gt;copy_&lt;/a&gt; operator.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg9.png&quot; alt=&quot;In-place update for KV cache using copy_ operator&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; In-place update for KV cache using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;copy_&lt;/code&gt; operator&lt;/p&gt;

&lt;p&gt;f) After enabling torch.compile + CUDA Graph to text decoder by modifying the code as mentioned above, the overhead of text decoder shifts to KV cache reordering as shown in Figure 10. KV cache reordering repeatedly calls index_select 96 times (assuming 24 decoder layers where each layer consists of two types of attention layers with cache for key and value).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg10.png&quot; alt=&quot;CPU and GPU trace for Text Decoder after enabling torch.compile + CUDA Graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; CPU and GPU trace for Text Decoder after enabling torch.compile + CUDA Graph.&lt;/p&gt;

&lt;p&gt;As part of accelerating text decoder, we additionally applied torch.compile to KV cache reordering to benefit from fusing kernels as shown in Figure 11. Note that we cannot use CUDA Graph here (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mode='max-autotune'&lt;/code&gt;) here, because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;copy_&lt;/code&gt; operation modifies the inputs which violates the static input requirement of CUDA graph version in torch.compile.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg11.png&quot; alt=&quot;Applying torch.compile to KV Cache reordering&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; Applying torch.compile to KV Cache reordering.&lt;/p&gt;

&lt;p&gt;As a result of enabling torch.compile to KV cache reordering, the gpu kernels that were launched separately (Figure 12(a)) are now fused so there are much fewer gpu kernels to launch (Figure 12(b)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg12.png&quot; alt=&quot;CPU and GPU trace for KV cache reordering before enabling torch.compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a)&lt;/strong&gt; CPU and GPU trace for KV cache reordering &lt;strong&gt;before&lt;/strong&gt; enabling torch.compile&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg13.png&quot; alt=&quot;CPU and GPU trace for KV cache reordering after enabling torch.compile&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b)&lt;/strong&gt; CPU and GPU trace for KV cache reordering &lt;strong&gt;after&lt;/strong&gt; enabling torch.compile&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 12.&lt;/strong&gt; CPU and GPU trace for KV cache reordering (a) before and (b) after enabling torch.compile&lt;/p&gt;

&lt;h2 id=&quot;vocoder&quot;&gt;Vocoder&lt;/h2&gt;

&lt;p&gt;Vocoder in Seamless is a HiFi-GAN unit-vocoder that converts generated units to waveform output where an unit is a representation of speech that combines different aspects such as phonemes and syllables, which can be used to generate sounds that are audible to humans. Vocoder is a relatively simple module that consists of Conv1d and ConvTranspose1d layers and is a CPU bound module as shown in FIgure 3. Based on this observation, we decided to enable torch.compile + CUDA Graph for vocoder to reduce the disproportionally large CPU overhead as shown in Figure 10. But there were several fixes to be made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg14.png&quot; alt=&quot;CPU and GPU trace for Vocoder after torch.compile + CUDA Graph are enabled&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 13.&lt;/strong&gt; CPU and GPU trace for Vocoder after torch.compile + CUDA Graph are enabled.&lt;/p&gt;

&lt;p&gt;a) The input tensor shape of the vocoder is different across different inference samples. But as CUDA Graph records the shape of tensors and replays them, we had to pad the input to the fixed size with zeros. Since vocoder only consists of Conv1d layers, we do not need an additional padding mask, and padding with zeros is sufficient.&lt;/p&gt;

&lt;p&gt;b) Vocoder consists of conv1d layers wrapped with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.nn.utils.weight_norm&lt;/code&gt; (see &lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/models/vocoder/hifigan.py#L37-L112&quot;&gt;here&lt;/a&gt;). However, applying torch.compile directly to Vocoder incurs graph break as below, which leads to suboptimal performance improvement. This graph break happens inside the hook handling part in the PyTorch code of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight_norm&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] Graph break: setattr(UserDefinedObjectVariable) &amp;lt;function Module.__setattr__ at 0x7fac8f483c10&amp;gt; from user code at:
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/vocoder.py&quot;, line 49, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return self.code_generator(x, dur_prediction)  # type: ignore[no-any-return]1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1520, in _call_impl
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return forward_call(*args, **kwargs)
[2023-12-13 04:26:16,822] [1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/codehifigan.py&quot;, line 101, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     return super().forward(x)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/mnt/fsx-home/yejinlee/yejinlee/seamless_communication/src/seamless_communication/models/vocoder/hifigan.py&quot;, line 185, in forward
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     x = self.ups[i](x)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1550, in _call_impl
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     args_result = hook(self, args)
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]   File &quot;/data/home/yejinlee/mambaforge/envs/fairseq2_12.1/lib/python3.8/site-packages/torch/nn/utils/weight_norm.py&quot;, line 65, in __call__
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG]     setattr(module, self.name, self.compute_weight(module))
[1/0_2] torch._dynamo.symbolic_convert.__graph_breaks: [DEBUG] 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since the weights of layers do not change during the inference, we do not need weight normalization. So we simply removed weight normalization for Vocoder as shown in Figure 14, by utilizing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remove_weight_norm&lt;/code&gt; function which is already provided at the Seamless codebase (&lt;a href=&quot;https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/models/vocoder/hifigan.py#L198-L205&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg15.png&quot; alt=&quot;Removing weight_norm for Vocoder&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 14.&lt;/strong&gt; Removing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;weight_norm&lt;/code&gt; for Vocoder&lt;/p&gt;

&lt;h2 id=&quot;performance-evaluation--impact-of-cuda-graph&quot;&gt;Performance Evaluation + Impact of CUDA Graph&lt;/h2&gt;

&lt;p&gt;Figure 15 shows the speedup result when enabling torch.compile(mode=”max-autotune”) + CUDA Graph on the text decoder and vocoder. We achieve &lt;strong&gt;2x speedup for the text decoder and 30x speedup for vocoder, leading to 2.7x faster end-to-end inference time.&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tr&gt;
   &lt;td&gt;

&lt;img alt=&quot;Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&quot; src=&quot;/assets/images/accelerating-generative-ai-4/fg16.png&quot; style=&quot;width:100%;&quot; /&gt;
   &lt;/td&gt;
   &lt;td&gt;

&lt;img alt=&quot;Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&quot; src=&quot;/assets/images/accelerating-generative-ai-4/fg17.png&quot; style=&quot;width:100%;&quot; /&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Figure 15.&lt;/strong&gt; Inference time speedup of text decoder and vocoder of applying torch.compile and torch.compile + CUDA Graph&lt;/p&gt;

&lt;p&gt;We also report the speedups for text decoder and vocoder using torch.compile without CUDA Graph, which is supported by torch.compile’s API (i.e., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile(mode=&quot;max-autotune-no-cudagraphs&quot;)&lt;/code&gt;), to identify the impact of CUDA Graph on the performance. Without CUDA Graph, the speedup for text decoder and vocoder reduces to 1.17x and 18.4x. While still quite significant, it indicates the important role of CUDA Graph. We conclude that Seamless M4T-v2 is exposed to a lot of time launching CUDA kernels, especially when we use small batch size (e.g., 1) where the GPU kernel execution time is not long enough to amortize the GPU kernel launch time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-generative-ai-4/fg1.png&quot; alt=&quot;End-to-end inference speedup of applying torch.compile and CUDA graph incrementally&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 16.&lt;/strong&gt; End-to-end inference speedup of applying torch.compile and CUDA graph incrementally. &lt;strong&gt;a)&lt;/strong&gt; “Inc. Decoding”: Apply torch.compile only to the text decoder &lt;strong&gt;b)&lt;/strong&gt; “Inc. Decoding w/ CUDA Graph”: Apply torch.compile + CUDA Graph to the text decoder &lt;strong&gt;c)&lt;/strong&gt; “+KV Cache Reordering”: Additionally apply torch.compile to KV cache reordering operation upon b) &lt;strong&gt;d)&lt;/strong&gt; “+Vocoder”: Additionally apply torch.compile to the vocoder upon c) &lt;strong&gt;e)&lt;/strong&gt; “+Vocoder w/ CUDA Graph”: Additionally apply torch.compile + CUDA Graph to the vocoder upon d).&lt;/p&gt;

&lt;p&gt;Figure 16 represents the cumulative effect of applying torch.compile with and without CUDA Graph to the modules. The results indicate a significant improvement in the end-to-end inference speedup, demonstrating the effectiveness of these techniques in optimizing the overall latency. As a result, we gain &lt;strong&gt;2.7x&lt;/strong&gt; end-to-end inference speedup for Seamless M4T-v2 with batch_size=1.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;We thank the PyTorch team and Seamless team for their tremendous support with this work.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Yejin Lee, Carole-Jean Wu, Christian Puhrsch, Joel Schlosser, Driss Guessous, Jeffrey Wan, Joe Isaacson, Can Balioglu, Juan Pino</name>
        
        
      </author>

      

      

      
        <summary type="html">This post is the fourth part of a multi-series blog focused on how to accelerate generative AI models with pure, native PyTorch. To skip to the code, check out our github (seamless_communication, fairseq2). We are excited to share a breadth of newly released PyTorch performance features alongside practical examples to see how far we can push PyTorch native performance. In part one, we showed how to accelerate Segment Anything over 8x using only pure, native PyTorch. In part two, we showed how to accelerate Llama-7B by almost 10x using only native PyTorch optimizations. In part three, we showed how to accelerate text-to-image diffusion models up to 3x using only native Pytorch optimizations.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerate PyTorch Models Using Quantization Techniques with Intel Extension for PyTorch</title>
      <link href="https://pytorch.org/blog/accelerate-pytorch-models/" rel="alternate" type="text/html" title="Accelerate PyTorch Models Using Quantization Techniques with Intel Extension for PyTorch" />
      <published>2024-01-18T00:00:00-08:00</published>
      <updated>2024-01-18T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerate-pytorch-models</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerate-pytorch-models/">&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;PyTorch is a Python-based framework for developing deep learning models. It is one of the most popular industry-standard AI frameworks and is used for a wide variety of computer vision and natural language processing applications. PyTorch was developed by Meta and is now part of The Linux Foundation. Intel works with the open source PyTorch project to optimize the PyTorch framework for Intel® hardware. The newest optimizations and features are first released in Intel® Extension for PyTorch before upstreaming them into PyTorch. The Intel extension provides quantization features to deliver good accuracy results for large deep learning models.&lt;/p&gt;

&lt;p&gt;This article introduces quantization, types of quantization, and demonstrates a code sample on how to accelerate PyTorch-based models by applying Intel Extension for PyTorch quantization.&lt;/p&gt;

&lt;h2 id=&quot;what-is-quantization&quot;&gt;What Is Quantization?&lt;/h2&gt;

&lt;p&gt;Quantization is a systematic reduction of the precision of all or several layers within the model. This means a higher-precision type (like single precision floating-point (FP32) that is mostly used in deep learning) is converted into a lower-precision type, such as FP16 (16 bits) or int8 (8 bits).&lt;/p&gt;

&lt;p&gt;This helps to achieve:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lower memory bandwidth&lt;/li&gt;
  &lt;li&gt;Lower storage&lt;/li&gt;
  &lt;li&gt;Higher performance with minimum to zero accuracy loss&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quantization is especially important with large models such as those based on the Transformer architecture (like BERT or GPT).&lt;/p&gt;

&lt;p&gt;There are two types of quantization:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Static: This quantizes the weights and activations of the model, and is used when memory bandwidth and compute savings are important.&lt;/li&gt;
  &lt;li&gt;Dynamic: The weights are quantized ahead of time, but the activations are dynamically quantized during inference.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-perform-static-quantization-and-dynamic-quantization&quot;&gt;How to Perform Static Quantization and Dynamic Quantization&lt;/h2&gt;

&lt;p&gt;The Intel extension extends PyTorch with up-to-date features and optimizations for an extra performance boost on Intel hardware.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/intel/intel-extension-for-pytorch#installation&quot;&gt;Installation Instructions for Intel Extension for PyTorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The extension can be loaded as a Python module or linked as a C++ library. Python users can enable it dynamically by importing &lt;strong&gt;intel_extension_for_pytorch&lt;/strong&gt;. The extension provides built-in quantization to deliver good statistical accuracy for most popular deep learning workloads including convolutional neural networks (CNN), natural language processing (NLP), and recommendation models. The quantization functionality in the Intel extension currently supports post-training quantization.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To quantize the existing FP32 model to an int8 model using static quantization:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare the quantization configuration. For default static quantization configuration, use &lt;strong&gt;ipex.quantization.default_static_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the model for calibration using the &lt;strong&gt;ipex.quantization.prepare&lt;/strong&gt; method.&lt;/li&gt;
  &lt;li&gt;Perform calibration against the dataset. This calibration is specific for static quantization as it needs the representative dataset to determine the optimal quantization parameters, so the user should provide data to the model in batches to calibrate it.&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to int8 using the &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method. This function converts the FP32 model to int8 based on the applied calibration and configuration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;To quantize the existing FP32 model to an int8 model using dynamic quantization, which is similar to static quantization:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Prepare the quantization configuration. For default dynamic quantization configuration, use &lt;strong&gt;ipex.quantization.default_dynamic_qconfig&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Prepare the FP32 model by using the &lt;strong&gt;ipex.quantization.prepare&lt;/strong&gt; method. Provide the parameters, such as FP32 model to quantize, the prepared configuration, example inputs, and information.&lt;/li&gt;
  &lt;li&gt;Convert the model from FP32 to int8 using the &lt;strong&gt;ipex.quantization.convert&lt;/strong&gt; method. The input model is the model prepared in Step 2.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;code-sample&quot;&gt;Code Sample&lt;/h2&gt;

&lt;h3 id=&quot;dataset&quot;&gt;Dataset&lt;/h3&gt;

&lt;p&gt;For static quantization, the model is calibrated with the &lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR-10 dataset&lt;/a&gt;. The CIFAR-10 is a subset of the 80 million &lt;a href=&quot;https://groups.csail.mit.edu/vision/TinyImages/&quot;&gt;tiny images dataset&lt;/a&gt; collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.&lt;/p&gt;

&lt;p&gt;This dataset contains 60,000 images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and track). Every class has exactly 6,000 images. All images are 32 x 32 pixels and are colored. Also, the classes are completely mutually exclusive, which means there is no overlapping between classes.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;code sample&lt;/a&gt; demonstrates how to quantize (using static and dynamic quantization) a ResNet*-50 model using Intel Extension for PyTorch. The following steps are implemented in the code sample:&lt;/p&gt;

&lt;h4 id=&quot;download-and-prepare-the-dataset&quot;&gt;Download and Prepare the Dataset&lt;/h4&gt;

&lt;p&gt;Here, we use the CIFAR-10 dataset available in torchvision.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To make data fit the model:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Transform the data.&lt;/li&gt;
  &lt;li&gt;Change the size of the images from 32 x 32 pixels to 224 x 224 pixels.&lt;/li&gt;
  &lt;li&gt;Convert them to tensors.&lt;/li&gt;
  &lt;li&gt;Normalize them.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare transformations of the dataset as shown:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;transform = torchvision.transforms.Compose([
torchvision.transforms.Resize((224, 224)),
torchvision.transforms.ToTensor(),
torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Initialize the dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_dataset = torchvision.datasets.CIFAR10(root=DATA, train=False, transform=transform, download=Ture)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;prepare-the-data-loader&quot;&gt;Prepare the Data Loader&lt;/h4&gt;

&lt;p&gt;To load a dataset for static quantization calibration in specific size batches, create the loader as shown:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;calibration_data_loader = torch.utils.data.DataLoader(
dataset=test_dataset,
batch_size=128
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;create-the-model&quot;&gt;Create the Model&lt;/h4&gt;

&lt;p&gt;Use the pretrained ResNet-50 model available in the Torchvision library with default weights. The prepared model is FP32.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model_fp32 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;apply-static-quantization&quot;&gt;Apply Static Quantization&lt;/h4&gt;

&lt;p&gt;Create a &lt;strong&gt;staticQuantize&lt;/strong&gt; function that implements the steps described previously.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To perform static quantization, we need:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;FP32 model loaded earlier&lt;/li&gt;
  &lt;li&gt;Example data&lt;/li&gt;
  &lt;li&gt;Calibration dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare the quantization configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config_static = ipex.quantization.default_static_qconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code sample, we are using the default quantization configuration, but you can also define your own. \&lt;/p&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Prepare the model using the declared configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prepared_model_static = prepare(model_fp32,
qconfig_static,
example_inputs=data,
inplace=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;Calibrate the model with the calibration dataset. Feed the model with successive batches of data from the dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for batch_idx, (data, target) in enumerate(calibration_data_loader):
prepared_model_static(data)
if batch_idx % 10 == 0:
print(&quot;Batch %d/%d complete, continue ...&quot; %(batch_idx+1, len(calibration_data_loader)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;5&quot;&gt;
  &lt;li&gt;Convert the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;converted_model_static = convert(prepared_model_static)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;apply-dynamic-quantization&quot;&gt;Apply Dynamic Quantization&lt;/h4&gt;

&lt;p&gt;Create the &lt;strong&gt;dynamicQuantize&lt;/strong&gt; function similar to the &lt;strong&gt;staticQuantize&lt;/strong&gt; function.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;To perform dynamic quantization, we only need:&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;The FP32 model loaded earlier&lt;/li&gt;
  &lt;li&gt;Example data&lt;/li&gt;
&lt;/ul&gt;

&lt;ol start=&quot;2&quot;&gt;
  &lt;li&gt;Prepare the quantization configuration:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;qconfig_dynamic = ipex.quantization.default_dynamic_qconfig
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;3&quot;&gt;
  &lt;li&gt;Prepare the model.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prepared_model_dynamic = prepare(model_fp32,
qconfig_dynamic,
example_inputs=data,
inplace=False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol start=&quot;4&quot;&gt;
  &lt;li&gt;Convert the model from FP32 to int8.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;converted_model_dynamic = convert(prepared_model_dynamic)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this way, two functions are created to take advantage of the optimizations that quantization offers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;DynamicQuantize&lt;/strong&gt; for dynamic quantization of models&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;StaticQuantize&lt;/strong&gt; for static model quantization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;Get started with Intel Extension for PyTorch quantization today and use it to achieve better accuracy results for deep learning workloads. Additionally, &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html?cid=sem&amp;amp;source=sa360&amp;amp;campid=2023_q2_iags_us_iagsoapie_iagsoapiee_awa_text-link_exact_cd_dpd-oneapi-intel_neural_compressor_3500107853_google_div_oos_non-pbm_intel&amp;amp;ad_group=ai_model_compression_exact&amp;amp;intel_term=neural+compressor&amp;amp;sa360id=43700076378213630&amp;amp;gclid=CjwKCAjw-IWkBhBTEiwA2exyO1pBoV7k3j16OANdyEOMVYDUvy4MZK3WQX6zzhymBxz7Pikqq0ndwBoCHvUQAvD_BwE&amp;amp;gclsrc=aw.ds#gs.2t5hw6&quot;&gt;Intel® Neural Compressor&lt;/a&gt; provides &lt;a href=&quot;https://intel.github.io/neural-compressor/latest/docs/source/quantization.html&quot;&gt;quantization&lt;/a&gt; to improve the speed of inference.&lt;/p&gt;

&lt;p&gt;Check out and incorporate Intel’s other &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html&quot;&gt;AI and machine learning framework optimizations&lt;/a&gt; and &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/tools.html&quot;&gt;end-to-end portfolio of tools&lt;/a&gt; into your AI workflow.&lt;/p&gt;

&lt;p&gt;Learn about the unified, open, standards-based &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html&quot;&gt;oneAPI&lt;/a&gt; programming model that forms the foundation of Intel’s &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI Software Portfolio&lt;/a&gt; to help you prepare, build, deploy, and scale your AI solutions.&lt;/p&gt;

&lt;p&gt;For more details about the 4th gen Intel® Xeon® Scalable processors, visit the &lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/platform.html&quot;&gt;Intel® AI platform overview&lt;/a&gt; where you can learn how Intel is empowering developers to run end-to-end AI pipelines on these powerful CPUs.&lt;/p&gt;

&lt;h2 id=&quot;additional-resources&quot;&gt;Additional Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/ai-solution-brief.html&quot;&gt;Accelerate AI Workloads with Intel® Advanced Matrix Extensions (Intel® AMX)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html&quot;&gt;AI and Machine Learning Development Tools and Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/frameworks/overview.html#gs.2t503z&quot;&gt;AI Frameworks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/training/computer-vision.html&quot;&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/artificial-intelligence/hardware.html&quot;&gt;Intel Hardware for AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html?cid=sem&amp;amp;source=sa360&amp;amp;campid=2023_q2_iags_us_iagsoapie_iagsoapiee_awa_text-link_exact_cd_dpd-oneapi-intel_neural_compressor_3500107853_google_div_oos_non-pbm_intel&amp;amp;ad_group=ai_model_compression_exact&amp;amp;intel_term=neural+compressor&amp;amp;sa360id=43700076378213630&amp;amp;gclid=CjwKCAjw-IWkBhBTEiwA2exyO1pBoV7k3j16OANdyEOMVYDUvy4MZK3WQX6zzhymBxz7Pikqq0ndwBoCHvUQAvD_BwE&amp;amp;gclsrc=aw.ds#gs.2t5hw6&quot;&gt;Intel Neural Compressor&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html#gs.h7kofh&quot;&gt;oneAPI Unified Programming Model&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/foundation&quot;&gt;PyTorch Foundation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html&quot;&gt;PyTorch Optimizations from Intel&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/oneapi-src/oneAPI-samples/tree/master/AI-and-Analytics/Features-and-Functionality/IntelPytorch_Quantization&quot;&gt;PyTorch Quantization Code Sample&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://intel.github.io/neural-compressor/latest/docs/source/quantization.html&quot;&gt;Quantization Using Intel Neural Compressor&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Intel</name>
        
        
      </author>

      

      

      
        <summary type="html">Overview</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Triton Dequantization Kernels for GPTQ</title>
      <link href="https://pytorch.org/blog/accelerating-triton/" rel="alternate" type="text/html" title="Accelerating Triton Dequantization Kernels for GPTQ" />
      <published>2024-01-16T00:00:00-08:00</published>
      <updated>2024-01-16T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/accelerating-triton</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-triton/">&lt;h2 id=&quot;tldr&quot;&gt;TL;DR&lt;/h2&gt;

&lt;p&gt;Leveraging a first principles approach, we showcase a step by step process undertaken to accelerate the current Triton GPTQ kernels by 3x (core GPTQ) and 6x (AutoGPTQ).  Example: 275us to 47us on a typical Llama style inference input.  The goal is to provide a helpful template for accelerating any given Triton kernel. We provide a background on Triton and GPTQ quantization and dequantization process, showcase the impact of coalesced memory access to improve shared and global memory throughput, highlight changes made to reduce warp stalling to improve total throughput, and an overview on integrating Triton kernels into PyTorch code.  Longer term, we hope to surpass the existing CUDA native GPTQ kernel with our Triton kernel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg1.png&quot; alt=&quot;Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 1: Performance benchmarking the optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on H100&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg2.png&quot; alt=&quot;Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 2: Performance benchmarking the newly optimized AutoGTPQ kernel vs the current AutoGPTQ kernel on A100&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg3.png&quot; alt=&quot;Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100.&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block; margin-top: 60px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fig 3: Even with these improvements, there remains a gap between our optimized Triton kernel and the CUDA native AutoGTPQ kernel on A100.  More to come…&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;10-introduction-to-triton&quot;&gt;1.0 Introduction to Triton&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;https://openai.com/research/triton&quot;&gt;Triton framework&lt;/a&gt; provides a hardware agnostic way of programming and targeting GPUs, currently supporting both NVIDIA and AMD, with support for additional hardware vendors in progress. Triton is now a mainstay for PyTorch 2.0 as torch.compile decomposes eager PyTorch and re-assembles it into a high percentage of Triton kernels with PyTorch connecting code.&lt;/p&gt;

&lt;p&gt;As Triton becomes more widely adopted, it will be essential that programmers understand how to systematically step through the Triton stack (from the high level Python down to the low-level SASS) to address performance bottlenecks in order to optimize GPU efficiency for algorithms that go beyond torch.compile generated kernels.&lt;/p&gt;

&lt;p&gt;In this post, we will introduce some core concepts of the Triton programming language, how to identify common performance limiters in GPU kernels, and in parallel, tune a quantization kernel used in AutoGPTQ that can be used for high throughput inference applications.&lt;/p&gt;

&lt;h3 id=&quot;intro-to-gptq-quantization-and-dequantization&quot;&gt;Intro to GPTQ Quantization and Dequantization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.17323&quot;&gt;GPTQ&lt;/a&gt;  is a quantization algorithm that is able to compress ultra-large (175B+) LLMs efficiently to int4 bit representation, via approximate second order information (Hessian inverse).  &lt;a href=&quot;https://github.com/PanQiWei/AutoGPTQ&quot;&gt;AutoGPTQ&lt;/a&gt; is a framework built on GPTQ, allowing for rapid dequantization and inference/serving of LLMs that have been quantized with GPTQ.&lt;/p&gt;

&lt;p&gt;As part of the AutoGPTQ stack, they provide a Triton GPTQ kernel to handle the dequantization of a model for inference.&lt;/p&gt;

&lt;p&gt;The basic process for INT quantization is shown below and involves determining the scale and zero point, and then computing the quantized 4bit Weight using the Scale and Zero point:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg4.jpg&quot; alt=&quot;The basic process for INT quantization&quot; style=&quot;width:100%;max-width:400px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We thus store the 4 Bit weights along with the meta information of Scale and ZeroPoint for each group of weights.&lt;/p&gt;

&lt;p&gt;To ‘dequant’ these weights, we do the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg5.png&quot; alt=&quot;To ‘dequant’ these weights&quot; style=&quot;width:100%;max-width:400px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And then proceed to &lt;strong&gt;Matrix Multiply&lt;/strong&gt; the dequantized weights with the dense input feature matrix for this linear layer.&lt;/p&gt;

&lt;h2 id=&quot;20-identify-the-bottlenecks---optimizing-matrix-multiplication&quot;&gt;2.0 Identify the Bottlenecks - Optimizing Matrix Multiplication&lt;/h2&gt;

&lt;p&gt;As it turns out, making a fast matrix multiplication kernel is not trivial. A naively implemented matrix multiply will rarely reach peak throughput performance on highly parallel machines like GPUs. So – we need to tackle our compute and memory subsystems in our GPU in an hierarchical fashion to make sure we are maximally utilizing each resource.&lt;/p&gt;

&lt;p&gt;We start our optimization process, by running the unoptimized Triton Kernel, through the Nvidia Nsight Compute tool and taking a note of some important metrics and warnings:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg6.png&quot; alt=&quot;some important metrics and warnings&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fig xy (todo)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg7.png&quot; alt=&quot;some important metrics and warnings&quot; style=&quot;width:100%;max-width:300px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We notice first that both compute and memory throughput are low, 7.40% and 21.19% respectively (fig xy) . Knowing that for typical inference matrix problem sizes, we are in the memory bound regime, we will attempt to optimize the kernel by applying code changes that target the memory subsystem of our A100 GPU.&lt;/p&gt;

&lt;p&gt;The three topics this post will cover are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;L2 Optimization&lt;/li&gt;
  &lt;li&gt;Vectorized Load&lt;/li&gt;
  &lt;li&gt;Warp Stalling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let’s walk through each topic, make the appropriate changes, and see its corresponding impact on our Triton Kernel. This Triton kernel is a fused dequantization kernel that dequantizes a packed int32 weight (we will refer to this as the B Matrix) Tensor into int4 weights, performs matrix multiplication with the activation tensor (refer to as the A matrix) in FP16 mode, and then  stores the results back to a matrix C.&lt;/p&gt;

&lt;p&gt;The above is referred to as W4A16 quantization. Keep in mind that the process we describe can and should be used for the development of any GPU kernel, as these are common bottlenecks in any unoptimized kernel.&lt;/p&gt;

&lt;h2 id=&quot;30-l2-optimization&quot;&gt;3.0 L2 Optimization&lt;/h2&gt;

&lt;p&gt;This optimization already exists in the AutoGPTQ kernel, but we’d like to dedicate a section to this to help readers better understand how mapping and execution order of thread blocks is handled in Triton.  Thus, we will step through a naive mapping and then a more optimal mapping to see its corresponding impact.&lt;/p&gt;

&lt;p&gt;Let’s build up our kernel naively, starting with a “linear” load from global memory and then compare it to a more optimized “swizzled” load. Linear vs Swizzled determines the execution order of our grid of work on the GPU. Let’s take a look at the hints that the &lt;a href=&quot;https://developer.nvidia.com/nsight-compute&quot;&gt;Nvidia Nsight Compute Tool&lt;/a&gt; provides regarding our kernels shared memory access pattern in the naive case:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg8.jpg&quot; alt=&quot;the hints from the Nvidia Nsight Compute Tool&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To tackle this issue we can use an approach referred to as “tile-swizzling.”  The idea of this method is to launch our thread blocks in a more L2 cache friendly order.&lt;/p&gt;

&lt;p&gt;Let’s take a step back and familiarize ourselves with some Triton semantics and make a simple CUDA analogy to understand the concept better. Triton kernels launch “programs”. These so-called programs map to the concept of a Thread Block in CUDA and it is the basic unit of parallelism in a Triton Kernel. Every program has with it associated a “pid” and all the threads in a program are guaranteed to be executing the same instruction.&lt;/p&gt;

&lt;p&gt;The Triton programs will be distributed onto your SMs in a naive-way if you do a simple linear mapping of “pid” to a 2D grid location of your output matrix C.&lt;/p&gt;

&lt;p&gt;This 2D grid location is determined by pid_m and pid_n in Triton. We would like to exploit data and cache locality in the L2 cache of our GPU, when we distribute our grid of work. To do this in Triton we can make the following changes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg9.png&quot; alt=&quot;To do this in Triton&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The code highlighted in red would be the naive “linear” tile ordering, and the code highlighted in green is the “swizzled” tile ordering. This type of launch promotes a sense of locality. Here is a visual to help understand this better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg10.jpg&quot; alt=&quot;a sense of locality&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After incorporating this change, the profiler no longer complains about uncoalesced memory accesses. Let’s take a look at how our memory throughput has changed:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg11.jpg&quot; alt=&quot;how our memory throughput has changed&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This change was tested on a simple load store kernel.  Looking at the GPU speed of light statistics section in the profiler we also see a 112.07% increase in the memory throughput of the simple load kernel, which is what we were after with this optimization.  Again, this optimization already exists in the AutoGPTQ kernel, but is the boilerplate logic that every Triton Kernel programmer will have to write in the beginning of their kernel, before any of the exciting dequantization or matrix multiply logic. It is thus important to understand that:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;This mapping is not unique&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Triton does not automatically handle this kind of optimization for the programmer, and careful thought must be taken to ensure your kernel is optimally handling shared memory accesses&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These are not obvious for those new to Triton, as much of the shared memory access optimization is handled by the Triton compiler. However, in the cases where these are not handled by the compiler, it is important to be able to understand what tools and methods are available to us to be able to influence memory behavior.&lt;/p&gt;

&lt;h2 id=&quot;40-vectorized-load&quot;&gt;4.0 Vectorized Load&lt;/h2&gt;

&lt;p&gt;Now, back to the original complaints of our unoptimized kernel. We want to optimize the global memory access pattern of our kernel. From the details page of the Nvidia Nsight compute tool, we see the following note, where the profiler is complaining about uncoalesced global memory accesses.&lt;/p&gt;

&lt;p&gt;Let’s dig deeper and take a look at the SASS (Assembly) Code load for an unoptimized memory read:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg12.png&quot; alt=&quot;an unoptimized memory read&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This load operation resulted in 32 global load operations that are 16 bit wide. This is not optimal.&lt;/p&gt;

&lt;p&gt;We would like to do our global memory loads in a vectorized way so that it results in the least amount of load instructions. To combat this we can give the Triton Compiler some help.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg13.png&quot; alt=&quot;code block&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The green highlighted lines above act as a compiler hint. It tells the compiler that these elements are contiguous in memory and that this load operation can be coalesced.&lt;/p&gt;

&lt;p&gt;Let’s see the effect in assembly after adding these lines.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg14.png&quot; alt=&quot;the effect in assembly after adding these lines&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The load is now performed in 4 global load operations that are each 128 bit wide, instead of 32 16 bit global load operations. This means 28 fewer memory fetch instructions, and importantly a coalesced memory access. This can be seen from the fact that a single thread is not accessing consecutive memory addresses, which without the compiler hint, was the behavior.&lt;/p&gt;

&lt;p&gt;The resulting effect is 73x speedup in an isolated load operation, and after incorporating it in the full dequantization kernel we were able to see another 6% speedup. Another step in the right direction!&lt;/p&gt;

&lt;h2 id=&quot;50-warp-stalling&quot;&gt;5.0 Warp Stalling&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg15.png&quot; alt=&quot;performance limiter, warp stalling&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now putting all the changes back into our full dequantization kernel, we see the following performance limiter, warp stalling.&lt;/p&gt;

&lt;p&gt;These warp stalls are mostly caused by ‘Long Scoreboard’ stalls, accounting for 92.63% of the total.&lt;/p&gt;

&lt;p&gt;At a high level, &lt;a href=&quot;https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference&quot;&gt;long scoreboard stalls&lt;/a&gt; happen when a warp requires data that may not be ready yet in order to be in the “issued” state. In other words GPUs are throughput machines, and we need to hide the latency of load instructions with compute instructions. By loading more data and rearranging where the load instructions are in the script we can take care of this problem.&lt;/p&gt;

&lt;p&gt;In an ideal scenario, each warp scheduler would be able to issue 1 instruction every clock cycle. Note - Every SM on an A100 GPU has 4 warp schedulers.&lt;/p&gt;

&lt;p&gt;However – our kernel has bottlenecks and is spending 4.4 cycles in the stall state with the block size that AutoGPTQ Triton kernel deems as optimal given the presets it has.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How do we improve this?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We want to be able to increase our memory throughput so that we can increase the chance that when a warp issues an instruction, we won’t be waiting for loads to be stored in SRAM so that they can be used for computation. We played around with multiple parameters (such as number of pipeline stages, and number of warps) and the one that had the biggest impact was increasing the block size by a factor of 2 in the k dimension.&lt;/p&gt;

&lt;p&gt;These changes yield an immediate impact on both compute and memory throughput.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg16.jpg&quot; alt=&quot;an immediate impact on both compute and memory throughput&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also see the long scoreboard wait time at the step where we shift and scale the quantized weights drop significantly, which is what we identified as the original bottleneck in the source code. While there are still stalls at this line, only 68% of them are caused by long scoreboard stalls, compared to the original 92%. Ideally, we do not observe ANY stalls, so there is still work to be done here, but a reduction in the amount of stalls caused by long scoreboard tells us that our data is at this point ready to be used (in L1TEX) memory by an instruction that a warp wants to execute, at a higher frequency then the original kernel.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg17.png&quot; alt=&quot;1.4x speedup in the execution time of our kernel&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The corresponding impact is a 1.4x speedup in the execution time of our kernel.&lt;/p&gt;

&lt;h2 id=&quot;60-results&quot;&gt;6.0 Results&lt;/h2&gt;

&lt;p&gt;By tackling all these problem areas methodically our resulting kernel is 6x faster on the Nvidia A100 GPU than if you were to use the Triton kernel AutoGPTQ provides out-of-the-box.&lt;/p&gt;

&lt;p&gt;Taking a relevant Llama inference sample data point, the &lt;a href=&quot;https://github.com/foundation-model-stack/foundation-model-stack/tree/triton/triton/kernels&quot;&gt;Triton kernel we’ve developed &lt;/a&gt;takes 47us to perform dequantization and matrix multiplication compared to the 275us taken by the AutoGPTQ kernel for the same matrix size.&lt;/p&gt;

&lt;p&gt;By replicating this step-by-step approach it should be possible to get similar speedups in other kernels, and help build understanding on common GPU bottlenecks and how to tackle them.&lt;/p&gt;

&lt;p&gt;It is important to note that while strides have been made in improving the performance of the AutoGPTQ Triton Kernel, we have still not closed the gap on the current exllamaV2 CUDA kernels found in AutoGPTQ.&lt;/p&gt;

&lt;p&gt;More research is required to understand how we can further optimize this kernel to match equivalent custom CUDA kernel performance.&lt;/p&gt;

&lt;h2 id=&quot;summary-and-future-work&quot;&gt;Summary and Future work&lt;/h2&gt;

&lt;p&gt;Triton extends PyTorch by allowing low level GPU optimizations to be done at a higher level of abstraction than CUDA programming, with the net result that adding optimized Triton kernels can help PyTorch models run faster.&lt;/p&gt;

&lt;p&gt;Our goal in this post was to show an example of accelerating the GPTQ dequant kernel and provide a template workflow for how the accelerations were achieved.&lt;/p&gt;

&lt;p&gt;For future work, SplitK work decomposition for the matrix multiplication is a potential speed up we’ll investigate.&lt;/p&gt;

&lt;h2 id=&quot;integrating-custom-triton-kernels-into-pytorch&quot;&gt;Integrating custom Triton Kernels into PyTorch&lt;/h2&gt;

&lt;p&gt;Given the acceleration shown above, a common question is how to actually use a custom kernel in a given PyTorch codebase.&lt;/p&gt;

&lt;p&gt;A triton kernel will contain at least two parts - the actual Triton kernel code which will be compiled by the Triton compiler:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg18.png&quot; alt=&quot;the actual Triton kernel code which will be compiled by the Triton compiler&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Along with the actual kernel code is a python wrapper, that may or may not subclass the PyTorch autograd class - depending if it’s going to support a backwards pass (i.e. for training purposes or only for inference purposes).&lt;/p&gt;

&lt;p&gt;You simply import the python class into your PyTorch code where you want to use it much like any other Python / PyTorch function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-triton/fg19.png&quot; alt=&quot;import the python class into your PyTorch code&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, simply importing and then using ‘fast_qlinear’ would invoke the underlying Triton kernel with the speed-ups we’ve shown above applied to your PyTorch model.&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Thanks to Jamie Yang and Hao Yu from IBM Research for their technical guidance in the collection of these results.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Less Wright, Adnan Hoque (IBM)</name>
        
        
      </author>

      

      

      
        <summary type="html">TL;DR</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem</title>
      <link href="https://pytorch.org/blog/finetune-llms/" rel="alternate" type="text/html" title="Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem" />
      <published>2024-01-10T00:00:00-08:00</published>
      <updated>2024-01-10T00:00:00-08:00</updated>
      <id>https://pytorch.org/blog/finetune-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/finetune-llms/">&lt;p&gt;We demonstrate how to finetune a 7B parameter model on a typical consumer GPU (NVIDIA T4 16GB) with LoRA and tools from the PyTorch and Hugging Face ecosystem with complete reproducible Google Colab notebook.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Large Language Models (LLMs) have shown impressive capabilities in industrial applications. Often, developers seek to tailor these LLMs for specific use-cases and applications to fine-tune them for better performance. However, LLMs are large by design and require a large number of GPUs to be fine-tuned.&lt;/p&gt;

&lt;p&gt;Let’s focus on a specific example by trying to fine-tune a Llama model on a free-tier Google Colab instance (1x NVIDIA T4 16GB). Llama-2 7B has 7 billion parameters, with a total of 28GB in case the model is loaded in full-precision. Given our GPU memory constraint (16GB), the model cannot even be loaded, much less trained on our GPU. This memory requirement can be divided by two with negligible performance degradation. You can read more about running models in half-precision and mixed precision for training &lt;a href=&quot;https://huggingface.co/docs/transformers/v4.15.0/performance#forward-vs-backward-execution-speed&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;what-makes-our-llama-fine-tuning-expensive&quot;&gt;What makes our Llama fine-tuning expensive?&lt;/h2&gt;

&lt;p&gt;In the case of full fine-tuning with Adam optimizer using a half-precision model and mixed-precision mode, we need to allocate per parameter:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2 bytes for the weight&lt;/li&gt;
  &lt;li&gt;2 bytes for the gradient&lt;/li&gt;
  &lt;li&gt;4 + 8 bytes for the Adam optimizer states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;→ With a total of 16 bytes per trainable parameter, this makes a total of &lt;strong&gt;112GB&lt;/strong&gt; (excluding the intermediate hidden states). Given that the largest GPU available today can have up to 80GB GPU VRAM, it makes fine-tuning challenging and less accessible to everyone. To bridge this gap, Parameter Efficient Fine-Tuning (PEFT) methods are largely adopted today by the community.&lt;/p&gt;

&lt;h2 id=&quot;parameter-efficient-fine-tuning-peft-methods&quot;&gt;Parameter Efficient Fine-Tuning (PEFT) methods&lt;/h2&gt;

&lt;p&gt;PEFT methods aim at drastically reducing the number of trainable parameters of a model while keeping the same performance as full fine-tuning.&lt;/p&gt;

&lt;p&gt;They can be differentiated by their conceptual framework: does the method fine-tune a subset of existing parameters, introduce new parameters, introduce trainable prompts, etc.? We recommend readers to have a look at the paper shared below that extensively compares existing PEFT methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg1.png&quot; alt=&quot;Venn diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Image taken from the paper: &lt;a href=&quot;https://arxiv.org/pdf/2303.15647.pdf&quot;&gt;Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;For this blog post, we will focus on Low-Rank Adaption for Large Language Models (LoRA), as it is one of the most adopted PEFT methods by the community.&lt;/p&gt;

&lt;h2 id=&quot;low-rank-adaptation-for-large-language-models-lora-using--peft&quot;&gt;Low-Rank Adaptation for Large Language Models (LoRA) using 🤗 PEFT&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.09685.pdf&quot;&gt;The LoRA method&lt;/a&gt; by Hu et al. from the Microsoft team came out in 2021, and works by attaching extra trainable parameters into a model(that we will denote by &lt;em&gt;base model&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;To make fine-tuning more efficient, LoRA decomposes a large weight matrix into two smaller, low-rank matrices (called update matrices). These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.&lt;/p&gt;

&lt;p&gt;This approach has several advantages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters.&lt;/li&gt;
  &lt;li&gt;The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them.&lt;/li&gt;
  &lt;li&gt;LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them.&lt;/li&gt;
  &lt;li&gt;The performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models.&lt;/li&gt;
  &lt;li&gt;LoRA does not add any inference latency when adapter weights are merged with the base model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, in Transformer models LoRA is typically applied to attention blocks only. The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined mainly by the rank r and the shape of the original weight matrix.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg2.gif&quot; alt=&quot;Animated diagram that show how LoRA works in practice&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Animated diagram that show how LoRA works in practice - original content adapter from the figure 1 of LoRA &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot;&gt;original paper&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Below is a code snippet showing how to train LoRA model using  Hugging Face PEFT library:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg3.png&quot; alt=&quot;code snippet showing how to train LoRA model using  Hugging Face PEFT library&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-base-model-can-be-in-any-dtype-leveraging-sota-llm-quantization-and-loading-the-base-model-in-4-bit-precision&quot;&gt;The base model can be in any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dtype&lt;/code&gt;: leveraging SOTA LLM quantization and loading the base model in 4-bit precision&lt;/h2&gt;

&lt;p&gt;According to the LoRA formulation, the base model can be compressed in any data type (&lt;em&gt;‘dtype’&lt;/em&gt;) as long as the hidden states from the base model are in the same dtype as the output hidden states from the LoRA matrices.&lt;/p&gt;

&lt;p&gt;Compressing and quantizing large language models has recently become an exciting topic as SOTA models become larger and more difficult to serve and use for end users. Many people in the community proposed various approaches for effectively compressing LLMs with minimal performance degradation.&lt;/p&gt;

&lt;p&gt;This is where the &lt;a href=&quot;https://github.com/TimDettmers/bitsandbytes&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt;&lt;/a&gt; library comes in. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public.&lt;/p&gt;

&lt;h2 id=&quot;qlora-one-of-the-core-contributions-of-bitsandbytes-towards-the-democratization-of-ai&quot;&gt;QLoRA: One of the core contributions of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt; towards the democratization of AI&lt;/h2&gt;

&lt;p&gt;Quantization of LLMs has largely focused on quantization for inference, but the &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot;&gt;QLoRA&lt;/a&gt; (Quantized model weights + Low-Rank Adapters) paper showed the breakthrough utility of using backpropagation through frozen, quantized weights at large model scales.&lt;/p&gt;

&lt;p&gt;With QLoRA we are matching 16-bit fine-tuning performance across all scales and models, while reducing fine-tuning memory footprint by more than 90%— thereby allowing fine-tuning of SOTA models on consumer-grade hardware.&lt;/p&gt;

&lt;p&gt;In this approach, LoRA is pivotal both for purposes of fine-tuning &lt;span style=&quot;text-decoration:underline;&quot;&gt;and&lt;/span&gt; the correction of minimal, residual quantization errors. Due to the significantly reduced size of the quantized model it becomes possible to generously place low-rank adaptors at every network layer, which together still make up just 0.2% of the original model’s weight memory footprint. Through such usage of LoRA, we achieve performance that has been shown to be equivalent to 16-bit full model finetuning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg4.png&quot; alt=&quot;System diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In addition to generous use of LoRA, to achieve high-fidelity fine-tuning of 4-bit models, QLoRA uses 3 further algorithmic tricks:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;4-bit NormalFloat (NF4) quantization, a custom data type exploiting the property of the normal distribution of model weights and distributing an equal number of weights (per block) to each quantization bin—thereby enhancing information density.&lt;/li&gt;
  &lt;li&gt;Double Quantization, quantization of the quantization constants (further savings).&lt;/li&gt;
  &lt;li&gt;Paged Optimizers, preventing memory spikes during gradient checkpointing from causing out-of-memory errors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An interesting aspect is the dequantization of 4-bit weights in the GPU cache, with matrix multiplication performed as a 16-bit floating point operation. In other words, we use a &lt;em&gt;low-precision storage data type&lt;/em&gt; (in our case 4-bit, but in principle interchangeable) and one normal precision &lt;em&gt;computation data type&lt;/em&gt;. This is important because the latter defaults to 32-bit for hardware compatibility and numerical stability reasons, &lt;span style=&quot;text-decoration:underline;&quot;&gt;but should be set to the optimal BFloat16 for newer hardware supporting it&lt;/span&gt; to achieve the best performance.&lt;/p&gt;

&lt;p&gt;To conclude, through combining these refinements to the quantization process and generous use of LoRA, we compress the model by over 90% and retain full model performance without the usual quantization degradation, while also retaining full fine-tuning capabilities with 16-bit LoRA adapters at every layer.&lt;/p&gt;

&lt;h2 id=&quot;using-qlora-in-practice&quot;&gt;Using QLoRA in practice&lt;/h2&gt;

&lt;p&gt;These SOTA quantization methods come packaged in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bitsandbytes&lt;/code&gt; library and are conveniently integrated with HuggingFace 🤗 Transformers. For instance, to use LLM.int8 and QLoRA algorithms, respectively, simply pass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_in_8bit&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_in_4bit&lt;/code&gt; to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;from_pretrained&lt;/code&gt; method.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = &quot;facebook/opt-125m&quot;
# For LLM.int8()
# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)

# For QLoRA
model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can read more about quantization features in this specific section of the documentation: &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/quantization&quot;&gt;https://huggingface.co/docs/transformers/main_classes/quantization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;When using QLoRA with Adam optimizer using a 4-bit base model and mixed-precision mode, we need to allocate per parameter:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;~0.5 bytes for the weight&lt;/li&gt;
  &lt;li&gt;2 bytes for the gradient&lt;/li&gt;
  &lt;li&gt;4 + 8 bytes for the Adam optimizer states&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Giving a total of 14 bytes per trainable parameter times 0.0029 as we end up having only 0.29% trainable parameters with QLoRA, this makes the QLoRA training setup cost around 4.5GB to fit, but requires in practice ~7-10GB to include intermediate hidden states which are always in half-precision (7 GB for a sequence length of 512 and 10GB for a sequence length of 1024) in the Google Colab demo shared in the next section.&lt;/p&gt;

&lt;p&gt;Below is the code snippet showing how to train QLoRA model using Hugging Face PEFT:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg5.png&quot; alt=&quot;code snippet showing how to train QLoRA model using Hugging Face PEFT&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;using-trl-for-llm-training&quot;&gt;Using TRL for LLM training&lt;/h2&gt;

&lt;p&gt;Models such as ChatGPT, GPT-4, and Claude are powerful language models that have been fine-tuned using a method called Reinforcement Learning from Human Feedback (RLHF) to be better aligned with how we expect them to behave and would like to use them. The finetuning goes through 3 steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised Fine-tuning (SFT)&lt;/li&gt;
  &lt;li&gt;Reward / preference modeling (RM)&lt;/li&gt;
  &lt;li&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg6.png&quot; alt=&quot;Process diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;From InstructGPT paper: Ouyang, Long, et al. “Training language models to follow instructions with human feedback.” arXiv preprint arXiv:2203.02155 (2022).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Here, we will only focus on the supervised fine-tuning step. We train the model on the new dataset following a process similar to that of pretraining. The objective is to predict the next token (causal language modeling). Multiple techniques can be applied to make the training more efficient:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Packing&lt;/strong&gt;: Instead of having one text per sample in the batch and then padding to either the longest text or the maximal context of the model, we concatenate a lot of texts with an End-Of-Sentence (EOS) token in between and cut chunks of the context size to fill the batch without any padding. This approach significantly improves training efficiency as each token processed by the model contributes to training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg7.png&quot; alt=&quot;Sample diagram&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Train on completion only&lt;/strong&gt;: We want the model to be able to understand the prompt and generate an answer/. Instead of training the model on the whole input (prompt + answer), the training will be more efficient if we only train the model on completion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can perform supervised fine-tuning with these techniques using SFTTrainer:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=train_dataset,
    dataset_text_field=&quot;text&quot;,
    max_seq_length=1024,
    packing=True,
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since SFTTrainer back-end is powered by 🤗&lt;a href=&quot;https://github.com/huggingface/accelerate&quot;&gt;accelerate&lt;/a&gt;, you can easily adapt the training to your hardware setup in one line of code!&lt;/p&gt;

&lt;p&gt;For example, with you have 2 GPUs, you can perform Distributed Data Parallel training with using the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;accelerate launch --num_processes=2 training_llama_script.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;putting-all-the-pieces-together&quot;&gt;Putting all the pieces together&lt;/h2&gt;

&lt;p&gt;We made a complete reproducible Google Colab notebook that you can check through&lt;a href=&quot;https://colab.research.google.com/drive/1vIjBtePIZwUaHWfjfNHzBjwuXOyU_ugD?usp=sharing&quot;&gt; this link&lt;/a&gt;. We use all the components shared in the sections above and fine-tune a llama-7b model on UltraChat dataset using QLoRA. As it can be observed through the screenshot below, when using a sequence length of 1024 and a batch size od 4,  the memory usage remains very  low (around 10GB).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/finetune-llms/fg8.png&quot; alt=&quot;Memory usage diagram&quot; style=&quot;width:100%;max-width:600px; margin-left: auto; margin-right: auto; display:block;&quot; /&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Younes Belkada, Marc Sun, Titus von Köller, Sourab Mangrulkar, Benjamin Bossan, Lysandre Debut, Steven Liu</name>
        
        
      </author>

      

      

      
        <summary type="html">We demonstrate how to finetune a 7B parameter model on a typical consumer GPU (NVIDIA T4 16GB) with LoRA and tools from the PyTorch and Hugging Face ecosystem with complete reproducible Google Colab notebook.</summary>
      

      
      
    </entry>
  
</feed>



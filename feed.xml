<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="https://pytorch.org/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://pytorch.org/" rel="alternate" type="text/html" />
  <updated>2024-05-15T02:45:51-07:00</updated>
  <id>https://pytorch.org/feed.xml</id>

  
  
  

  
    <title type="html">PyTorch Website</title>
  

  
    <subtitle>Scientific Computing...</subtitle>
  

  
    <author>
        <name>Facebook</name>
      
      
    </author>
  

  
  
  
    <entry>
      <title type="html">Speeding up ViTs using Block Sparsity</title>
      <link href="https://pytorch.org/blog/speeding-up-vits/" rel="alternate" type="text/html" title="Speeding up ViTs using Block Sparsity" />
      <published>2024-05-14T00:00:00-07:00</published>
      <updated>2024-05-14T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/speeding-up-vits</id>
      <content type="html" xml:base="https://pytorch.org/blog/speeding-up-vits/">&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; We show promising results of up to a &lt;strong&gt;1.46x speedup with &amp;lt;2% drop in accuracy&lt;/strong&gt; on float32 Vision Transformers on A100 GPUs by applying block sparsity on MLP module’s weights. This approach can potentially be applied to other types of transformers including large language models. Our implementation and benchmarks to reproduce our results are available at &lt;a href=&quot;https://github.com/pytorch-labs/superblock&quot;&gt;https://github.com/pytorch-labs/superblock&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;PyTorch has landed a lot of improvements to CUDA kernels that implement block sparse matrix multiplications. Recent updates to Pytorch can lead up to &lt;a href=&quot;https://gist.github.com/cpuhrsch/7fec60079cbe2daeff59c0577f933320&quot;&gt;4.8x speedup&lt;/a&gt; on large matrix multiplication shapes with high sparsity levels over dense baselines.&lt;/p&gt;

&lt;p&gt;In this blog, we show the promising results of applying block sparsity on weights of linear layers of MLP (multi-layer perceptron) layers in vision transformers (ViTs) and show end-to-end model speedups on A100 Nvidia GPUs.&lt;/p&gt;

&lt;p&gt;As a recap, block sparsity sparsifies weights in tiles of blocks of predetermined size, rather than sparsifying individual elements. This particular sparsity pattern is interesting because it is amenable to GPU acceleration via fast sparse kernels. For more information about the differences between different sparsity patterns, or about sparsity as a whole, please check out &lt;a href=&quot;https://github.com/pytorch/ao/tree/main/torchao/sparsity&quot;&gt;torchao&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig1.png&quot; alt=&quot;Illustrations of different types of sparsity.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustrations of different types of sparsity.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;p&gt;Our approach can be broken down into two distinct steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Training the model from scratch using block sparse masks subnets.&lt;/li&gt;
  &lt;li&gt;Folding these masks into our weights to accelerate them for inference.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We explain our training and inference steps below&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Starting with an uninitialized Vision Transformer, we apply random trainable masks with a specified block size and sparsity level on the weights of output projection linear layer of attention blocks, the weights of the two linear layers inside the MLP, a.k.a., FFN (feed forward networks), as well as the final linear classification layer. The forward pass during training follows the &lt;a href=&quot;https://arxiv.org/abs/2207.00670&quot;&gt;supermask approach&lt;/a&gt;, as each mask is converted to binary map using a tuned threshold based on sparsity requirements, e.g., if we want 80% sparsity, we will have the threshold automatically tuned to keep top 20% weights. The masks are of a square &amp;lt;block size&amp;gt;x&amp;lt;block size&amp;gt; elements, where &amp;lt;block size&amp;gt; is a hyperparameter. The priority of the weights is dependent on the mask value or score which is trained. We &lt;a href=&quot;https://github.com/pytorch-labs/superblock/blob/7a469210c7bcb846dd8b6bfa848d104312312126/supermask.py#L130&quot;&gt;multiply the binary masks of each layer with the weights&lt;/a&gt; to sparsify the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig2.png&quot; alt=&quot;Illustration of the Supermask sparsification approach&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustration of the &lt;a href=&quot;https://arxiv.org/abs/2207.00670&quot;&gt;Supermask&lt;/a&gt; sparsification approach.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;inference&quot;&gt;Inference&lt;/h3&gt;

&lt;p&gt;After training, the &lt;a href=&quot;https://github.com/pytorch-labs/superblock/blob/7a469210c7bcb846dd8b6bfa848d104312312126/supermask.py#L122-L125&quot;&gt;dense weights can be turned to sparse weights by multiplying with the mask&lt;/a&gt; and stored for inference. At this stage, although the weights have a high percentage of zero values, they are still stored in dense format. We use PyTorch’s &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html&quot;&gt;to_sparse_bsr()&lt;/a&gt; API to to convert the weights to &lt;a href=&quot;https://pytorch.org/docs/stable/sparse.html#sparse-bsr-docs&quot;&gt;Block Sparse Representation&lt;/a&gt; (BSR) format that stores only the non-zero values and the indices of their blocks. This step only needs to be done once and the results can be cached for runtime.&lt;/p&gt;

&lt;p&gt;During runtime, no changes in code are required. We just pass any input tensor to the model, and when the forward() function of the sparsified linear layers are invoked, PyTorch takes care of invoking the optimized matrix multiplication for block sparse weights. This should work for A100 as well as H100 NVIDIA GPUs.&lt;/p&gt;

&lt;h2 id=&quot;results-microbenchmarks&quot;&gt;Results: Microbenchmarks&lt;/h2&gt;

&lt;p&gt;To validate the viability of block sparsity from a performance standpoint, we first ran a series of microbenchmarks using this &lt;a href=&quot;https://github.com/pytorch/ao/blob/73f8efce1e950235f58dc917ee204517ec74bba0/benchmarks/benchmark_gpu_sparsity.py&quot;&gt;simple script&lt;/a&gt;. Using the linear shapes from ViT-b, we compared the speedup of our block sparse kernels across a single linear layer as we varied the sparsity level and block size of the weight matrix.&lt;/p&gt;

&lt;p&gt;We run using PyTorch 2.3.0.dev20240305+cu121 nightly on NVIDIA A100s and report the speedup of each sparsity configuration compared to dense baseline. We observed positive speedups when block size &amp;gt;=32 or sparsity level &amp;gt;= 0.8 for float32, while for bfloat16 we observe smaller speedups and usually for block size 64 and higher sparsities. Hence, for end-to-end speedups on the model, we will focus in this blog on float32 and leave bfloat16 for future work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig3.png&quot; alt=&quot;Micro benchmarking results on linear layers of ViT-b-16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig4.png&quot; alt=&quot;Micro benchmarking results on linear layers of ViT-b-16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Micro benchmarking results on linear layers of ViT-b-16.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;results-vision-transformers&quot;&gt;Results: Vision Transformers&lt;/h2&gt;

&lt;p&gt;Once we confirmed that we were able to show speedups over the linear layers, we focused on showing end-to-end speedups on &lt;a href=&quot;https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html&quot;&gt;ViT_B_16&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We trained this model from scratch on ImageNet dataset using the standard &lt;a href=&quot;https://github.com/pytorch/vision/tree/main/references/classification#vit_b_16&quot;&gt;ViT_B_16 recipe&lt;/a&gt;. We show speedups for sparsifying MLP modules and leave sparsifying weights of input and output projections of attention for future work.&lt;/p&gt;

&lt;p&gt;We looked at wall-clock inference speedup, focusing on batch size 256. We found that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For 90% sparsity we can get 1.24x, 1.37x, 1.65x speedups for block sizes 16, 32, and 64 respectively.&lt;/li&gt;
  &lt;li&gt;To obtain speedup, the minimum sparsity for block sizes 16, 32, and 64 are 0.86, 0.82, and 0.7 respectively. Hence, as expected, the larger the block size, the smaller sparsity we need to obtain speedup.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We note a limitation of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sparse_bsr()&lt;/code&gt; API: that layers need to be multiples of the block size. Since the dimensions of the last FC classification layer in ViT was not a multiple of the block size, they were not converted to BSR representation in our experiments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig5.png&quot; alt=&quot;Speedup on ViT-b-16 with batch size 256 on MLP modules across different batch sparsities and block sizes.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Speedup on ViT-b-16 with batch size 256 on MLP modules across different batch sparsities and block sizes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We also explored the speedup for different batch sizes for 90% sparsity. We observed a speedup over the baseline for batch sizes starting from 16 and upwards. While bigger block sizes have bigger speedups at the largest batch sizes, the smallest possible batch size to obtain &amp;gt;1 speedup is smaller for smaller block sizes.&lt;/p&gt;

&lt;p&gt;We believe on-device hardware can obtain speedups for batch size 1 as they - unlike server GPUs - can be fully utilized at such small batch sizes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig6.png&quot; alt=&quot;Speedup on ViT-b-16 with 90% sparsity on MLP modules across different batch sizes and block sizes.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Speedup on ViT-b-16 with 90% sparsity on MLP modules across different batch sizes and block sizes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Looking at the Top-1 accuracy on ImageNet=blurred test set of the sparsified models for different block sizes and sparsities, we see a few expected results:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;low levels of sparsity (&amp;lt;=70%) have no meaningful regression in accuracy&lt;/li&gt;
  &lt;li&gt;mid levels of sparsity (&amp;gt;=80% to &amp;lt;90%) have limited regression in accuracy&lt;/li&gt;
  &lt;li&gt;high levels of sparsity (&amp;gt;=90%) removes so many weights that accuracy is significantly impacted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More research could be done to improve accuracies of higher sparsities and larger block sizes. We hope that the block sparsity support in PyTorch and the illustrated speedups in this blog will encourage researchers to explore more accurate sparsification approaches.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speeding-up-vits/fig7.png&quot; alt=&quot;Accuracies on training ViT-b-16 on ImageNet-blurred using the SuperMask approach.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Accuracies on training ViT-b-16 on ImageNet-blurred using the SuperMask approach.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h2&gt;

&lt;p&gt;We have shown promising speedups for block sparsifying MLP modules ViT in float32 precision.  There is still more work to be done in order to observe speedups on bfloat16 and we hope to obtain progress on that soon. Possible next steps to further optimize block sparsity on vision transformers and transformers in general:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Perform block sparsity on attention input and output projections.&lt;/li&gt;
  &lt;li&gt;Perform block sparsity during finetuning rather than training from scratch.&lt;/li&gt;
  &lt;li&gt;Perform further optimizations on the matmul kernels for ViT’s linear operator specific shapes (especially for 80% and lower sparsity).&lt;/li&gt;
  &lt;li&gt;Combine with other optimizations such as int8 and torch.compile()&lt;/li&gt;
  &lt;li&gt;Explore other weight sparsification algorithms, e.g., &lt;a href=&quot;https://arxiv.org/abs/2205.14107&quot;&gt;Spartan&lt;/a&gt;, to improve accuracy&lt;/li&gt;
  &lt;li&gt;Explore selecting weights to sparsify (e.g., specific transformer layers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please reach out to &lt;a href=&quot;mailto:melhoushi@meta.com&quot;&gt;melhoushi@meta.com&lt;/a&gt; if you have questions or are interested in contributing to block sparsification!&lt;/p&gt;

&lt;p&gt;Additionally if you’re broadly interested in sparsity please feel free to reach out to &lt;a href=&quot;https://github.com/jcaip&quot;&gt;@jcaip&lt;/a&gt; / &lt;a href=&quot;mailto:jessecai@meta.com&quot;&gt;jessecai@meta.com&lt;/a&gt; and please come check out &lt;a href=&quot;https://github.com/pytorch/ao&quot;&gt;torchao&lt;/a&gt;, a community we’re building for architecture optimization techniques like quantization and sparsity.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>FAIR at Meta: Mostafa Elhoushi, Sensors and Systems at Meta Reality Labs Research: Syed Shakib Sarwar, Aaryan Kothapalli, Mia Kasperek, Barbara De Salvo, PyTorch at Meta: Christian Puhrsch, Jesse Cai, Joe Isaacson, Quantsight: Andrew James, Pearu Peterson, Nikita Vedeneev</name>
        
        
      </author>

      

      

      
        <summary type="html">TLDR: We show promising results of up to a 1.46x speedup with &amp;lt;2% drop in accuracy on float32 Vision Transformers on A100 GPUs by applying block sparsity on MLP module’s weights. This approach can potentially be applied to other types of transformers including large language models. Our implementation and benchmarks to reproduce our results are available at https://github.com/pytorch-labs/superblock.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Enhancing Deep Learning Workflows: PyTorch Ecosystem Tools</title>
      <link href="https://pytorch.org/blog/enhancing-deep-learning/" rel="alternate" type="text/html" title="Enhancing Deep Learning Workflows: PyTorch Ecosystem Tools" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/enhancing-deep-learning</id>
      <content type="html" xml:base="https://pytorch.org/blog/enhancing-deep-learning/">&lt;p&gt;Welcome to the thriving PyTorch ecosystem, where a wealth of tools and libraries await, purpose-built to elevate your experience in deep learning as a developer or researcher. The Ecosystem Tools pages host  many projects from experts spanning academia, industry, application development, and machine learning.&lt;/p&gt;

&lt;p&gt;Initially, PyTorch aimed to establish a thriving community, enabling developers to access each other’s tools, engage in meaningful discussions, and explore the wealth of resources available within the community.&lt;/p&gt;

&lt;p&gt;Today, the PyTorch ecosystem has grown to feature over 100 projects tailored to your needs, providing robust support, enhanced speed, and effortless integration with PyTorch. If your project aligns with our mission, we invite you to &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;submit&lt;/a&gt; it and join this dynamic ecosystem.&lt;/p&gt;

&lt;p&gt;New this month, we’ve moved all of our Ecosystem blogs over to our PyTorch.org website to host a space where our community can show off the latest innovations with our users. Read on to hear about the latest projects in the ecosystem!&lt;/p&gt;

&lt;h2 id=&quot;explore-the-latest-tools-and-frameworks-in-the-ecosystem&quot;&gt;Explore the Latest Tools and Frameworks in the Ecosystem&lt;/h2&gt;

&lt;p&gt;As we continue into 2024, we’re thrilled to showcase an impressive array of ecosystem tools that significantly enrich the PyTorch community. These tools cover a wide range of domains, including pose estimation, profiling, and even quantum computing. Let’s explore each one to witness firsthand how they are reshaping the PyTorch landscape, opening up exciting possibilities for developers.&lt;/p&gt;

&lt;h3 id=&quot;anomalib&quot;&gt;&lt;a href=&quot;https://github.com/openvinotoolkit/anomalib&quot;&gt;Anomalib&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Anomalib is a deep learning library that aims to collect state-of-the-art anomaly detection algorithms for benchmarking on both public and private datasets. Anomalib provides several ready-to-use implementations of anomaly detection algorithms described in the recent literature, as well as a set of tools that facilitate the development and implementation of custom models. The library has a strong focus on image-based anomaly detection, where the goal of the algorithm is to identify anomalous images, or anomalous pixel regions within images in a dataset. Anomalib is constantly updated with the latest algorithms and training/inference extensions.&lt;/p&gt;

&lt;h3 id=&quot;diffusers&quot;&gt;&lt;a href=&quot;https://huggingface.co/docs/diffusers&quot;&gt;Diffusers&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Diffusers is a library within the PyTorch ecosystem that focuses on model interpretability. It offers a suite of tools and techniques to explain the decisions made by deep learning models. With Diffusers, developers can gain insights into model behavior, understand feature importance, and detect potential biases. By making deep learning models more transparent, Diffusers promotes fairness, accountability, and robustness in AI applications.&lt;/p&gt;

&lt;h3 id=&quot;pomegranate&quot;&gt;&lt;a href=&quot;https://pomegranate.readthedocs.io/en/latest/&quot;&gt;Pomegranate&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Pomegranate is a versatile machine learning library that integrates seamlessly with PyTorch. It provides a wide range of probabilistic models and tools for probabilistic modeling tasks. Pomegranate empowers users to build complex models such as hidden Markov models (HMMs), Bayesian networks, and Gaussian mixture models (GMMs). By combining the strengths of PyTorch and Pomegranate, developers can leverage the power of deep learning and probabilistic modeling to tackle various machine learning challenges.&lt;/p&gt;

&lt;h3 id=&quot;pypose&quot;&gt;&lt;a href=&quot;https://pypose.org/&quot;&gt;PyPose&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;PyPose is a PyTorch-based library designed for pose estimation tasks. With PyPose, developers can efficiently train and deploy models for human pose estimation, a fundamental computer vision problem. By leveraging PyTorch’s flexibility and performance, PyPose simplifies the process of building accurate pose estimation models. Its intuitive APIs and pre-trained models make it an excellent choice for researchers and developers exploring human pose estimation applications.&lt;/p&gt;

&lt;h3 id=&quot;pypots&quot;&gt;&lt;a href=&quot;https://github.com/WenjieDu/PyPOTS&quot;&gt;PyPOTS&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;A python toolbox/library for data mining on partially-observed time series with PyTorch, including SOTA models supporting tasks of imputation, classification, clustering, and forecasting on incomplete (irregularly-sampled) multivariate time series with missing values.&lt;/p&gt;

&lt;h3 id=&quot;octoml-profiler&quot;&gt;&lt;a href=&quot;https://github.com/octoml/octoml-profile&quot;&gt;OctoML Profiler&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OctoML Profiler is a performance profiling tool that aids in optimizing PyTorch models. This tool helps developers identify performance bottlenecks and inefficiencies within their deep learning models. By providing insights into memory usage, compute time, and data movement, the OctoML Profiler enables developers to fine-tune their models for improved efficiency. With this valuable feedback, developers can optimize their models for deployment on various hardware platforms.&lt;/p&gt;

&lt;h3 id=&quot;open-compass&quot;&gt;&lt;a href=&quot;https://github.com/open-compass/opencompass&quot;&gt;Open Compass&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;OpenCompass is a one-stop platform for large model evaluation, aiming to provide a fair, open, and reproducible benchmark for large model evaluation. Its main features include: Comprehensive support for models and datasets, efficient distributed evaluation, diversified evaluation paradigms, modular design with high extensibility and experiment management and reporting mechanism.&lt;/p&gt;

&lt;h3 id=&quot;renate&quot;&gt;&lt;a href=&quot;https://renate.readthedocs.io/en/latest/&quot;&gt;Renate&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Renate is a PyTorch-based library for neural architecture search (NAS). It simplifies the process of automatically searching for optimal neural network architectures tailored to specific tasks. Renate leverages techniques like reinforcement learning and evolutionary algorithms to efficiently explore the architecture space. By using Renate, developers can save significant time and resources while discovering highly performant models.&lt;/p&gt;

&lt;h3 id=&quot;roma&quot;&gt;&lt;a href=&quot;https://github.com/naver/roma&quot;&gt;RoMa&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;RoMa is a standalone library to handle rotation representations with PyTorch (rotation matrices, quaternions, rotation vectors, etc). It aims for robustness, ease-of-use, and efficiency.&lt;/p&gt;

&lt;h3 id=&quot;substra&quot;&gt;&lt;a href=&quot;https://github.com/Substra&quot;&gt;Substra&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Substra is an open source federated learning (FL) software. It enables the training and validation of machine learning models on distributed datasets. It provides a flexible Python interface and a web application to run federated learning training at scale. Substra’s main usage is in production environments. It has already been deployed and used by hospitals and biotech companies. Substra can also be used on a single machine to perform FL simulations and debug code.&lt;/p&gt;

&lt;h3 id=&quot;torchquantum&quot;&gt;&lt;a href=&quot;https://hanruiwanghw.wixsite.com/torchquantum&quot;&gt;TorchQuantum&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;TorchQuantum is a powerful library that combines the PyTorch framework with quantum computing concepts. It enables developers to explore quantum machine learning algorithms and build hybrid classical-quantum models. By integrating the principles of quantum computing into PyTorch, TorchQuantum opens up new possibilities for solving complex problems that traditional deep learning approaches may struggle with.&lt;/p&gt;

&lt;h3 id=&quot;tiatoolbox&quot;&gt;&lt;a href=&quot;https://github.com/TissueImageAnalytics/tiatoolbox&quot;&gt;TIAToolbox&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The TIAToolbox (Text-Image-Augmentation Toolbox) is a PyTorch library designed to augment text and image data for deep learning tasks. It offers a comprehensive set of tools for data augmentation, including transformations, noise injection, and image/text synthesis. By applying TIAToolbox, developers can enrich their training datasets, improve model generalization, and enhance the robustness of their deep learning models.&lt;/p&gt;

&lt;h3 id=&quot;torchdistill&quot;&gt;&lt;a href=&quot;https://github.com/yoshitomo-matsubara/torchdistill&quot;&gt;torchdistill&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;torchdistill is a coding-free framework built on PyTorch for reproducible deep learning and knowledge distillation studies. The framework is designed to enable users to design experiments by declarative PyYAML configuration files and supports high-level module abstractions.&lt;/p&gt;

&lt;h3 id=&quot;torchopt&quot;&gt;&lt;a href=&quot;https://torchopt.readthedocs.io/en/latest/#&quot;&gt;TorchOpt&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;TorchOpt is a PyTorch library focused on optimization algorithms for deep learning. It provides a collection of state-of-the-art optimization techniques, such as stochastic gradient descent (SGD) variants, adaptive learning rate methods, and optimization schedules. TorchOpt empowers developers to fine-tune their models efficiently, converge faster, and achieve better performance in various deep learning tasks.&lt;/p&gt;

&lt;h3 id=&quot;usb&quot;&gt;&lt;a href=&quot;https://usb.readthedocs.io/&quot;&gt;USB&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;USB, or Unified Speech-to-Text Benchmark, is a PyTorch-based toolkit for training and evaluating speech recognition models. It provides standardized datasets and evaluation metrics to facilitate fair and accurate comparisons between different speech recognition architectures. By using USB, researchers and developers can benchmark their models against state-of-the-art systems and drive advancements in the field of automatic speech recognition.&lt;/p&gt;

&lt;h3 id=&quot;zeus&quot;&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Zeus is the current state-of-the-art in deep learning energy measurement and optimization. It has monitor components that allow users to measure GPU energy consumption and optimizer components that automatically optimize DNN or GPU knobs based on measurements from the monitor component.&lt;/p&gt;

&lt;h2 id=&quot;be-part-of-our-ecosystem&quot;&gt;Be Part of Our Ecosystem&lt;/h2&gt;

&lt;p&gt;Our  diverse ecosystem tools are instrumental in PyTorch’s success.. They provide essential  support for tasks such as pose estimation, probabilistic modeling, performance profiling, model interpretability, speech recognition, quantum computing, data augmentation, optimization, and neural architecture search.&lt;/p&gt;

&lt;p&gt;Leveraging these tools empowers developers and researchers to accelerate their deep learning workflows and unlock new possibilities in the field of AI.&lt;/p&gt;

&lt;p&gt;Have a tool that would be a good fit for the &lt;a href=&quot;https://pytorch.org/ecosystem/&quot;&gt;PyTorch Ecosystem&lt;/a&gt;? If you can answer the below questions, we’d love for you to &lt;a href=&quot;https://pytorch.org/ecosystem/join&quot;&gt;submit your tool for review&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Does your project complement PyTorch, enhancing user experience, introducing new capabilities, or accelerating training and inference processes?
    &lt;ul&gt;
      &lt;li&gt;Examples could include visualization tools, a kernel library or a framework that sits on top to enable research in a particular area such as NLP.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Is the project ready for broad developer usage?
    &lt;ul&gt;
      &lt;li&gt;For example, is the project stable, will it be maintained, and is there adequate supporting infrastructure, documentation, and technical support to allow a developer to successfully use it?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thank you to all of our contributors and collaborators in our ecosystem! Here’s to a great 2024.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">Welcome to the thriving PyTorch ecosystem, where a wealth of tools and libraries await, purpose-built to elevate your experience in deep learning as a developer or researcher. The Ecosystem Tools pages host many projects from experts spanning academia, industry, application development, and machine learning.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Introducing depyf: mastering torch.compile with ease</title>
      <link href="https://pytorch.org/blog/introducing-depyf/" rel="alternate" type="text/html" title="Introducing depyf: mastering torch.compile with ease" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/introducing-depyf</id>
      <content type="html" xml:base="https://pytorch.org/blog/introducing-depyf/">&lt;p&gt;&lt;img src=&quot;/assets/images/depyf.png&quot; alt=&quot;depyf logo&quot; style=&quot;width:100%;display: block; max-width: 400px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are thrilled to introduce &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt;, a new project to the PyTorch ecosystem designed to help users understand, learn, and adapt to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;!&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is a cornerstone of PyTorch 2.x, offering a straightforward path to accelerate machine learning workflows with just a single line of code for both training and inference. The mere inclusion of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@torch.compile&lt;/code&gt; can&lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt; dramatically enhance the performance of your code&lt;/a&gt;. However, identifying the optimal insertion point for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; is not easy, not to mention the complexity of adjusting various knobs for maximum efficiency.&lt;/p&gt;

&lt;p&gt;The intricacies of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; stack, encompassing Dynamo, AOTAutograd, Inductor, and more, present a &lt;strong&gt;steep learning curve&lt;/strong&gt;. These components, essential for deep learning performance optimization, can be daunting without a solid foundation in the subject.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: For an introductory example of how torch.compile works, please refer to this&lt;a href=&quot;https://depyf.readthedocs.io/en/latest/walk_through.html&quot;&gt; walk-through explanation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-common-tool-torch_compile_debug&quot;&gt;A common tool: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;To demystify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, the common approach involves leveraging the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; environment variable. While it provides more information, deciphering the output remains a formidable task.&lt;/p&gt;

&lt;p&gt;For example, when we have the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And run it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG=1 python test.py&lt;/code&gt; , we will get a directory named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch_compile_debug/run_2024_02_05_23_02_45_552124-pid_9520&lt;/code&gt; , under which there are these files:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── torchdynamo
│   └── debug.log
└── torchinductor
   ├── aot_model___0_debug.log
   ├── aot_model___10_debug.log
   ├── aot_model___11_debug.log
   ├── model__4_inference_10.1
   │   ├── fx_graph_readable.py
   │   ├── fx_graph_runnable.py
   │   ├── fx_graph_transformed.py
   │   ├── ir_post_fusion.txt
   │   ├── ir_pre_fusion.txt
   │   └── output_code.py
   ├── model__5_inference_11.2
   │   ├── fx_graph_readable.py
   │   ├── fx_graph_runnable.py
   │   ├── fx_graph_transformed.py
   │   ├── ir_post_fusion.txt
   │   ├── ir_pre_fusion.txt
   │   └── output_code.py
   └── model___9.0
       ├── fx_graph_readable.py
       ├── fx_graph_runnable.py
       ├── fx_graph_transformed.py
       ├── ir_post_fusion.txt
       ├── ir_pre_fusion.txt
       └── output_code.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The generated files and logs often raise more questions than they answer, leaving developers puzzled over the meaning and relationships within the data. Common puzzles for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model__4_inference_10.1&lt;/code&gt; mean?&lt;/li&gt;
  &lt;li&gt;I have one function but three &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model__xxx.py&lt;/code&gt; in the directory, what is their correspondence?&lt;/li&gt;
  &lt;li&gt;What are those &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LOAD_GLOBAL&lt;/code&gt; stuff in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;debug.log&lt;/code&gt; ?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-better-tool-depyf-comes-to-rescue&quot;&gt;A better tool: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; comes to rescue&lt;/h2&gt;

&lt;p&gt;Let’s see how &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; can help developers to resolve the above challenges. To use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; , simply execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install depyf&lt;/code&gt; or follow the project page&lt;a href=&quot;https://github.com/thuml/depyf&quot;&gt; https://github.com/thuml/depyf&lt;/a&gt; to install the latest version, and then surround the main code within &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.prepare_debug&lt;/code&gt; .&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   import depyf
   with depyf.prepare_debug(&quot;depyf_debug_dir&quot;):
       main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;python test.py&lt;/code&gt; , &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; will produce a directory named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt; (the argument of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepare_debug&lt;/code&gt; function). Under the directory, there would be these files:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;.
├── __compiled_fn_0 AFTER POST GRAD 0.py
├── __compiled_fn_0 Captured Graph 0.py
├── __compiled_fn_0 Forward graph 0.py
├── __compiled_fn_0 kernel 0.py
├── __compiled_fn_3 AFTER POST GRAD 0.py
├── __compiled_fn_3 Captured Graph 0.py
├── __compiled_fn_3 Forward graph 0.py
├── __compiled_fn_3 kernel 0.py
├── __compiled_fn_4 AFTER POST GRAD 0.py
├── __compiled_fn_4 Captured Graph 0.py
├── __compiled_fn_4 Forward graph 0.py
├── __compiled_fn_4 kernel 0.py
├── __transformed_code_0_for_torch_dynamo_resume_in_toy_example_at_8.py
├── __transformed_code_0_for_toy_example.py
├── __transformed_code_1_for_torch_dynamo_resume_in_toy_example_at_8.py
└── full_code_for_toy_example_0.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And there are two obvious benefits:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The long and difficult-to-understand &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdynamo/debug.log&lt;/code&gt; is gone. Its content is cleaned up and shown as human-readable source code, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;full_code_for_xxx.py&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__transformed_code_{n}_for_xxx.py&lt;/code&gt; . It is worth to note, that the most tedious and difficult job of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; is to decompile the bytecode inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torchdynamo/debug.log&lt;/code&gt; into Python source code, freeing developers from intimidating internals of Python.&lt;/li&gt;
  &lt;li&gt;The correspondence between function names and computation graphs are respected. For example, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__transformed_code_0_for_toy_example.py&lt;/code&gt; , we can see a function named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0&lt;/code&gt; , and we will immediately know its corresponding computation graphs are in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0_xxx.py&lt;/code&gt; , because they share the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__compiled_fn_0&lt;/code&gt; prefix name.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Starting with &lt;code&gt;full_code_for_xxx.py&lt;/code&gt; , and following the functions involved, users will have a clear view of what &lt;code&gt;torch.compile&lt;/code&gt; does to their code.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;one-more-thing-step-through-debuggability&quot;&gt;One more thing: step-through debuggability&lt;/h2&gt;

&lt;p&gt;Stepping through code line by line using debuggers is a great way to understand how code works. However, under &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TORCH_COMPILE_DEBUG&lt;/code&gt; , those files are only for users’ information, and cannot be executed with the data users concern.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: By “debug”, we mean the process of inspecting and improving a program, rather than correcting buggy code.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A standout feature of &lt;code&gt;depyf&lt;/code&gt; is its capability to facilitate step-through debugging for &lt;code&gt;torch.compile&lt;/code&gt;&lt;/strong&gt;: all of the files it generates are linked with runtime code objects inside Python interpreter, and we can set breakpoints in these files. The usage is simple, just add one context manager &lt;code&gt;with depyf.debug()&lt;/code&gt; , and it should do the trick:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# test.py
import torch
from torch import _dynamo as torchdynamo
from typing import List

@torch.compile
def toy_example(a, b):
   x = a / (torch.abs(a) + 1)
   if b.sum() &amp;lt; 0:
       b = b * -1
   return x * b

def main():
   for _ in range(100):
       toy_example(torch.randn(10), torch.randn(10))

if __name__ == &quot;__main__&quot;:
   import depyf
   with depyf.prepare_debug(&quot;depyf_debug_dir&quot;):
       main()
   with depyf.debug():
       main()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just one caveat: the workflow of debugging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; deviates from standard debugging workflow. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, many codes are &lt;strong&gt;dynamically&lt;/strong&gt; generated. Therefore, we need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;launch the program&lt;/li&gt;
  &lt;li&gt;when the program exits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.prepare_debug(&quot;depyf_debug_dir&quot;)&lt;/code&gt; , code will be available in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;when the program enters &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;with depyf.debug()&lt;/code&gt; , it will automatically set a breakpoint internally, so that the program is paused.&lt;/li&gt;
  &lt;li&gt;navigate to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf_debug_dir&lt;/code&gt; to set breakpoints.&lt;/li&gt;
  &lt;li&gt;continue to run the code, and debuggers will hit these breakpoints!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/depyf-screenshot.png&quot; alt=&quot;depyf screenshot&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is a screenshot of what it looks like. All code and tensor variables are live, and we can inspect any variable, and step through the code, as in our daily debugging workflow now! The only difference is that we are debugging &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; generated code rather than human-written code.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt; serves as an invaluable tool for accelerating PyTorch code effortlessly. For those looking to delve deeper into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;, whether to leverage its full potential or to integrate custom operations, the learning curve can be very steep though. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; is designed to lower this barrier, offering a user-friendly experience to understand, learn, and adapt to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Do explore &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; and experience its benefits firsthand! The project is open-source and readily available at&lt;a href=&quot;https://github.com/thuml/depyf&quot;&gt; https://github.com/thuml/depyf&lt;/a&gt;. Installation is straightforward via &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install depyf&lt;/code&gt;. We hope &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;depyf&lt;/code&gt; can enhance everyone’s development workflow with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;torch.compile&lt;/code&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kaichao You</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Deep Learning Energy Measurement and Optimization</title>
      <link href="https://pytorch.org/blog/zeus/" rel="alternate" type="text/html" title="Deep Learning Energy Measurement and Optimization" />
      <published>2024-05-11T00:00:00-07:00</published>
      <updated>2024-05-11T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/zeus</id>
      <content type="html" xml:base="https://pytorch.org/blog/zeus/">&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig1.png&quot; alt=&quot;Zeus logo&quot; style=&quot;width:100%;display: block; max-width: 400px; margin-right: auto; margin-left: auto&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is authored by &lt;a href=&quot;https://jaewonchung.me/about&quot;&gt;Jae-Won Chung&lt;/a&gt;, a PhD student at the University of Michigan and the lead of the &lt;a href=&quot;https://ml.energy&quot;&gt;ML.ENERGY Initiative&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Deep learning consumes quite a bit of energy. For instance, training a single 200B LLM on AWS p4d instances consumed around 11.9 GWh (source: &lt;a href=&quot;https://mvdirona.com/jrh/talksandpapers/JamesHamiltonCIDR2024.pdf&quot;&gt;CIDR 2024 keynote&lt;/a&gt;), which is an amount that can single-handedly power more than a thousand &lt;a href=&quot;https://www.eia.gov/tools/faqs/faq.php?id=97&amp;amp;t=3&quot;&gt;average US households&lt;/a&gt; for a year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus&lt;/a&gt; is an open-source toolbox for measuring and optimizing the energy consumption of deep learning workloads. Our goal is to make energy optimization based on accurate measurements as easy as possible for diverse deep learning workloads and setups by offering composable tools with minimal assumptions.&lt;/p&gt;

&lt;p&gt;Zeus largely provides two types of tools:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Programmatic and command line GPU energy &lt;strong&gt;measurement&lt;/strong&gt; tools&lt;/li&gt;
  &lt;li&gt;Several energy &lt;strong&gt;optimization&lt;/strong&gt; tools that find the best ML and/or GPU configurations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Zeus can benefit those who would like to&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;measure and optimize their electricity cost&lt;/li&gt;
  &lt;li&gt;reduce heat dissipation from their GPUs (by lowering power draw)&lt;/li&gt;
  &lt;li&gt;report energy usage from research and development&lt;/li&gt;
  &lt;li&gt;reduce carbon footprint from electricity usage&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;part-1-measuring-energy&quot;&gt;Part 1: Measuring Energy&lt;/h2&gt;

&lt;p&gt;Just like performance optimization, accurate measurement is the basis of effective energy optimization. Popular proxies for estimating power consumption like the maximum power draw of the hardware &lt;a href=&quot;https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/&quot;&gt;can sometimes be vastly off&lt;/a&gt; compared to actual measurement.&lt;/p&gt;

&lt;p&gt;To make energy measurement as easy and transparent as possible, the core utility Zeus offers is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; class. Let’s take a look at the actual snippet:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# All four GPUs are measured simultaneously.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_indices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Measure total time and energy within the window.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Measurement windows can arbitrarily be overlapped.
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;epoch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;epoch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Epoch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_energy&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; J&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end_window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;training&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Entire training: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_energy&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; J&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What you see above is a typical PyTorch training loop which uses four GPUs for data parallel training. Inside, we created an instance of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; and passed in a list of GPU indices to monitor. Then, using the monitor, we can measure the time and energy consumption of arbitrary execution &lt;em&gt;windows&lt;/em&gt; within the training script by pairing calls to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;begin_window&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end_window&lt;/code&gt;. Multiple windows can overlap and nest in arbitrary ways without affecting the measurement of each, as long as their names are different.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; adds very little overhead – typically single digit milliseconds – around the window. This allows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt; to be used in various applications. For instance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/leaderboard&quot;&gt;The ML.ENERGY Leaderboard&lt;/a&gt;: The first open-source benchmark on how much energy LLM text generation consumes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/leaderboard&quot;&gt;The ML.ENERGY Colosseum&lt;/a&gt;: An online service that lets users compare LLM responses side-by-side based on response quality &lt;em&gt;and&lt;/em&gt; energy consumption.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See our &lt;a href=&quot;https://ml.energy/blog/energy/measurement/measuring-gpu-energy-best-practices/&quot;&gt;blog post&lt;/a&gt; for a deeper technical dive into accurate GPU energy measurement.&lt;/p&gt;

&lt;h2 id=&quot;part-2-optimizing-energy&quot;&gt;Part 2: Optimizing Energy&lt;/h2&gt;

&lt;p&gt;Let me introduce you to two of the energy optimizers provided by Zeus.&lt;/p&gt;

&lt;h3 id=&quot;globalpowerlimitoptimizer&quot;&gt;GlobalPowerLimitOptimizer&lt;/h3&gt;

&lt;p&gt;GPUs allow users to configure its maximum power draw, called &lt;em&gt;power limit&lt;/em&gt;. Typically, as you lower the GPU’s power limit from the default maximum, computation may get slightly slower, but you’ll save disproportionately more energy. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; in Zeus automatically finds the optimal GPU power limit globally across all GPUs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# The optimizer measures time and energy through the ZeusMonitor.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gpu_indices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_epoch_begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_step_begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_hat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_step_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;on_epoch_end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In our familiar PyTorch training loop, we have instantiated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; and passed it an instance of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZeusMonitor&lt;/code&gt;, through which the optimizer sees the GPUs. Then, we just need to let the optimizer know about training progress (step and epoch boundaries), and the optimizer will transparently do all the necessary profiling and converge to the optimal power limit.&lt;/p&gt;

&lt;p&gt;If you’re using the HuggingFace &lt;a href=&quot;https://huggingface.co/docs/transformers/main_classes/trainer&quot;&gt;Trainer&lt;/a&gt; or &lt;a href=&quot;https://huggingface.co/docs/trl/main/en/sft_trainer&quot;&gt;SFTTrainer&lt;/a&gt;, integration is even easier:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.monitor&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# ZeusMonitor actually auto-detects CUDA_VISIBLE_DEVICES.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZeusMonitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pl_optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Pass in the optimizer as a Trainer callback. Also works for SFTTrainer.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;...,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;callbacks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pl_optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HFGlobalPowerLimitOptimizer&lt;/code&gt; wraps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalPowerLimitOptimizer&lt;/code&gt; so that it automatically detects step and epoch boundaries. We have example integrations &lt;a href=&quot;https://github.com/ml-energy/zeus/tree/master/examples/huggingface&quot;&gt;here&lt;/a&gt;, including running Gemma 7B supervised fine-tuning with QLoRA.&lt;/p&gt;

&lt;p&gt;Now, we know how to integrate the optimizer, but what is the &lt;em&gt;optimal&lt;/em&gt; power limit? We know different users can have different preferences regarding trading off time and energy, so we allow users to specify an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OptimumSelector&lt;/code&gt; (basically the &lt;a href=&quot;https://en.wikipedia.org/wiki/Strategy_pattern&quot;&gt;Strategy Pattern&lt;/a&gt;) to express their needs.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Built-in strategies for selecting the optimal power limit.
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zeus.optimizer.power_limit&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Energy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;MaxSlowdownConstraint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Minimize energy while tolerating at most 10% slowdown.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GlobalPowerLimitOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;monitor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;MaxSlowdownConstraint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some of the built-in strategies include “Minimize time” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.Time&quot;&gt;Time&lt;/a&gt;, this might still reduce the power limit from the default since some workloads exhibit almost no slowdown even on lower power limits), “Minimize energy” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.Energy&quot;&gt;Energy&lt;/a&gt;), “Somewhere in between” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.ZeusCost&quot;&gt;ZeusCost&lt;/a&gt;), and “Minimize energy given maximum slowdown” (&lt;a href=&quot;https://ml.energy/zeus/reference/optimizer/power_limit/#zeus.optimizer.power_limit.MaxSlowdownConstraint&quot;&gt;MaxSlowdownConstraint&lt;/a&gt;). Users can also create their own optimum selectors as needed.&lt;/p&gt;

&lt;h3 id=&quot;pipelinefrequencyoptimizer&quot;&gt;PipelineFrequencyOptimizer&lt;/h3&gt;

&lt;p&gt;The pipeline frequency optimizer, based on our research paper &lt;a href=&quot;https://ml.energy/zeus/research_overview/perseus&quot;&gt;Perseus&lt;/a&gt;, is our latest work on energy optimization for large model training, like GPT-3. Perseus can reduce the energy consumption of large model training with no or negligible training throughput degradation. We’ll briefly talk about how.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig2.png&quot; alt=&quot;one iteration of training with four stage pipeline parallelism&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The above is a visualization of one iteration of training with four stage &lt;em&gt;pipeline parallelism&lt;/em&gt; running with the 1F1B schedule. Each box is either a forward or a backward computation, and is colored with its power consumption.&lt;/p&gt;

&lt;p&gt;The key observation here is that when models are partitioned into pipeline stages, it’s very difficult to slice them in perfectly equal sizes. This leads to forward/backward boxes of varying widths and therefore computation &lt;em&gt;idle time&lt;/em&gt; between boxes. You would notice that those smaller boxes can run slightly slower than wider boxes and the overall critical path (blue line) will not change at all.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/zeus/fig3.png&quot; alt=&quot;one iteration of training with four stage pipeline parallelism&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s what Perseus automatically does. Based on profiling, it identifies computation boxes that are not on the critical path and figures out the precise amount of slowdown for each box that minimizes energy consumption. When done correctly, computations we slowed down will consume less power &amp;amp; energy, but the overall iteration time of the pipeline does not change.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://ml.energy/zeus/optimize/pipeline_frequency_optimizer/&quot;&gt;our guide&lt;/a&gt; to get started with Perseus!&lt;/p&gt;

&lt;h2 id=&quot;final-words&quot;&gt;Final Words&lt;/h2&gt;

&lt;p&gt;For users who run their own on-premise compute, energy consumption and the resulting electricity bill is not something that can be easily overlooked. On a larger scale, energy consumption is not just about electricity bills, but also about data center power delivery. With thousands of GPUs running in clusters, finding stable, affordable, and sustainable electricity sources to power data centers is becoming &lt;a href=&quot;https://www.cbre.com/insights/reports/north-america-data-center-trends-h1-2023&quot;&gt;increasingly challenging&lt;/a&gt;. Finding ways to reduce energy disproportionately more than slowdown leads to lower average power consumption, which can help with the power delivery challenge.&lt;/p&gt;

&lt;p&gt;With Zeus, we hope to take the first step towards deep learning energy measurement and optimization.&lt;/p&gt;

&lt;p&gt;Wondering where to go from here? Here are a couple helpful links:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy/zeus&quot;&gt;Zeus homepage/documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus&quot;&gt;Zeus GitHub repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ml-energy/zeus/tree/master/examples&quot;&gt;Zeus usage and integration examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml.energy&quot;&gt;ML.ENERGY Initiative&lt;/a&gt; (i.e., the people building Zeus)&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Jae-Won Chung</name>
        
        
      </author>

      

      

      
        <summary type="html"></summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Announcing PyTorch Docathon June, 2024</title>
      <link href="https://pytorch.org/blog/docathon-june-2024/" rel="alternate" type="text/html" title="Announcing PyTorch Docathon June, 2024" />
      <published>2024-05-02T00:00:00-07:00</published>
      <updated>2024-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/docathon-june-2024</id>
      <content type="html" xml:base="https://pytorch.org/blog/docathon-june-2024/">&lt;p&gt;We are thrilled to announce the upcoming PyTorch Docathon in June! The Docathon, akin to a hackathon, is an event dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Documentation is a vital component of any technology. By refining it, we can simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning. See our previous events &lt;a href=&quot;https://pytorch.org/blog/announcing-docathon/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://pytorch.org/blog/announcing-docathon-h2-2023/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;why-participate&quot;&gt;Why Participate&lt;/h2&gt;

&lt;p&gt;The Docathon is an inclusive event designed to be accessible to newcomers, requiring only a basic understanding of Python, PyTorch, and Machine Learning, with some tasks not even requiring these skills. It offers a rewarding experience as participants can see the direct impact of their contributions on the project’s usability and accessibility. The Docathon promotes a collaborative environment, allowing participants to work with other contributors and PyTorch maintainers, fostering the exchange of ideas and networking. It also provides a rich learning experience, offering the opportunity to explore PyTorch modules, update docstrings, and test tutorials.&lt;/p&gt;

&lt;h2 id=&quot;event-details&quot;&gt;Event Details&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;June 4&lt;/strong&gt;: Kick-off&lt;br /&gt;
&lt;strong&gt;June 4-June 16&lt;/strong&gt;: Submissions and Feedback&lt;br /&gt;
&lt;strong&gt;June 17-18&lt;/strong&gt;: Final Reviews&lt;br /&gt;
&lt;strong&gt;June 20&lt;/strong&gt;: Winner Announcements&lt;/p&gt;

&lt;p&gt;Further details for the Docathon will be announced at the Kick-off call on June 4.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://community.linuxfoundation.org/events/details/lfhq-pytorch-foundation-presents-pytorch-docathon-june-4-20th-2024/&quot;&gt;Please register to join this year’s event&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are thrilled to announce the upcoming PyTorch Docathon in June! The Docathon, akin to a hackathon, is an event dedicated to enhancing the quality of the PyTorch documentation with the invaluable assistance of our community. Documentation is a vital component of any technology. By refining it, we can simplify the process for new users to get started with PyTorch, guide them in effectively utilizing its features, and ultimately expedite the transition from research to production in machine learning. See our previous events here and here.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Hitchhiker’s Guide to Speculative Decoding</title>
      <link href="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/" rel="alternate" type="text/html" title="A Hitchhiker’s Guide to Speculative Decoding" />
      <published>2024-05-02T00:00:00-07:00</published>
      <updated>2024-05-02T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/hitchhikers-guide-speculative-decoding</id>
      <content type="html" xml:base="https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/">&lt;p&gt;Speculative decoding is an optimization technique for inference that makes educated guesses about  future tokens while generating the current token, all within a single forward pass. It incorporates a verification mechanism to ensure the correctness of these speculated tokens, thereby guaranteeing that the overall output of speculative decoding is identical to that of vanilla decoding. Optimizing the cost of inference of large language models (LLMs) is arguably one of the most critical factors in reducing the cost of generative AI and increasing its adoption. Towards this goal, various inference optimization techniques are available, including custom kernels, dynamic batching of input requests, and quantization of large models.&lt;/p&gt;

&lt;p&gt;In this blog post, we provide a guide to speculative decoding and demonstrate how it can coexist with other optimizations. We are proud to open source the following, which includes the first speculator for Llama3 models:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Speculator models for &lt;a href=&quot;https://huggingface.co/ibm-fms/llama3-8b-accelerator&quot;&gt;Meta Llama3 8B&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/ibm/granite-7b-lab-accelerator&quot;&gt;IBM Granite 7B lab&lt;/a&gt;, &lt;a href=&quot;https://huggingface.co/ibm-fms/codellama-13b-accelerator&quot;&gt;Meta Llama2 13B&lt;/a&gt;, and &lt;a href=&quot;https://huggingface.co/ibm-fms/codellama-13b-accelerator&quot;&gt;Meta Code Llama2 13B&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/IBM/text-generation-inference/pull/79&quot;&gt;The code for inference via IBM’s fork of HF TGI.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp/pull/35&quot;&gt;The code for training your own speculators and corresponding recipes.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We have deployed these speculators in an internal production-grade environment with thousands of daily users and observed 2x speedup on language models - Llama3 8B, Llama2 13B, and IBM Granite 7B and 3x speedup on IBM’s Granite 20B code models. We provide a detailed explanation of our approach in this &lt;a href=&quot;https://arxiv.org/abs/2404.19124&quot;&gt;technical report&lt;/a&gt; and are planning in-depth analysis in an upcoming ArXiv paper.&lt;/p&gt;

&lt;h2 id=&quot;speculative-decoding-inference&quot;&gt;Speculative decoding: Inference&lt;/h2&gt;

&lt;p&gt;We run IBM TGIS in our internal production environment that has optimizations such as continuous batching, fused kernels, and quantization kernels. To enable speculative decoding in TGIS, we modified the paged attention kernel from &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt;. In what follows, we will describe the key changes to the inference engine to enable speculative decoding.&lt;/p&gt;

&lt;p&gt;Speculative decoding is based on the premise that the model is powerful enough to predict multiple tokens in a single forward pass. However, the current inference servers are optimized to predict only a single token at a time. In our approach, we attach multiple speculative heads (in addition to the usual one) to the LLM to predict &lt;em&gt;N+1-, N+2-, N+3-th …&lt;/em&gt; token. For example, 3 heads will predict 3 additional tokens. Details of the speculator architecture are explained in a later part of this blog. There are two challenges to achieve &lt;em&gt;efficiency&lt;/em&gt; and &lt;em&gt;correctness&lt;/em&gt; during inference - one is to predict without replicating KV-cache and the other is to verify that the predictions match the original model’s outcomes.&lt;/p&gt;

&lt;p&gt;In a typical generation loop, after the prompt is processed in a single forward step, a sequence length of 1 (next token predicted) is fed into the forward pass of the model along with the kv-cache. In a naive speculative decoding implementation, each speculative head would have its own kv-cache, but instead we  modify the paged attention kernel developed in the vLLM project to enable efficient kv-cache maintenance. This  ensures that throughput does not reduce at larger batch sizes. Further, we modify the attention masks to enable verification of the &lt;em&gt;N+1’th&lt;/em&gt; token and thus enable speculative decoding without deviating from the original model’s output. The details of this implementation are captured &lt;a href=&quot;https://github.com/foundation-model-stack/fms-extras&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;We illustrate the speedup obtained with the Meta’s chat versions of Llama2 13B using a simple prompt.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig1.gif&quot; alt=&quot;Visual illustration of the non-speculative generation (left) compared to speculative generation (right)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 2: Visual illustration of the non-speculative generation (left) compared to speculative generation (right)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We deployed the above solution in an internal production environment. The figure below reports two metrics – time to first token (TTFT) and inter-token latency (ITL) with different numbers of concurrent users (which is captured in the numbers on the graph lines). We observe that the speculative decoding version is nearly twice as fast for the Llama2 13B chat model and nearly thrice as fast for the Granite 20B code model compared to the non-speculative version for all batch sizes. We observe similar behavior for the smaller models - IBM’s Granite 7B and Meta Llama3 8B models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig2.png&quot; alt=&quot;Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Llama 13B with number of concurrent users indicated on the graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 3: Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Llama 13B with number of concurrent users indicated on the graph&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig3.png&quot; alt=&quot;Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Granite 20B Code with number of concurrent users indicated on the graph&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Time to first token (TTFT - left) and Inter-token latency (ITL - right) for Granite 20B Code with number of concurrent users indicated on the graph&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;note-on-efficiency&quot;&gt;Note on efficiency&lt;/h3&gt;

&lt;p&gt;We performed numerous experiments to determine the right configuration for speculator training. These are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Speculator architecture&lt;/strong&gt;: The current approach allows for the number of heads to be modified, which maps to the number of tokens that we can look ahead. Increasing the number of heads also increases the amount of extra compute needed and complexity of  training. In practice, for language models, we find 3-4 heads works well in practice, whereas we found that code models can reap benefits from 6-8 heads.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Compute&lt;/strong&gt;: Increasing the number of heads results in increased compute in two dimensions, one is that of increased latency for a single forward pass as well as the compute needed for multiple tokens. If the speculator is not accurate with more heads, it will result in wasted compute increasing the latency and reducing the throughput.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt;: The increased compute is offset by the roundtrips to HBM that need to be done for each forward pass. Note that if we get 3 tokens lookahead correct, we have saved three round trip times on HBM.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We settled on 3-4 heads for the language models and 6-8 heads for the code models and across different model sizes ranging from 7B to 20B, we observed significant latency improvements without throughput loss compared to non-speculative decoding. We begin to observe throughput reduction beyond a batch size of 64, which happens rarely in practice.&lt;/p&gt;

&lt;h2 id=&quot;speculative-decoding-training&quot;&gt;Speculative decoding: Training&lt;/h2&gt;

&lt;p&gt;There are  two broad approaches for speculative decoding, one is to leverage a smaller model (e.g., Llama 7B as a speculator for Llama 70B) and the other is to attach speculator heads (and train them). In our experiments, we find the approach of attaching speculator heads to be more   effective both in model quality and latency gains.&lt;/p&gt;

&lt;h3 id=&quot;speculator-architecture&quot;&gt;Speculator architecture&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10774&quot;&gt;Medusa&lt;/a&gt; made speculative decoding popular; their approach is to add a head to the existing model which is then trained to do speculation. We modify the Medusa architecture by making the “heads” hierarchical, where each head stage predicts a single token and then feeds it to the next head stage. These multi-stage heads are depicted in the below figure. We are exploring ways of minimizing the embeddings table by sharing these across the multiple stages and base model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig4.png&quot; alt=&quot;A simple architecture diagram for a 3-headed multi-stage  speculator. Z is the state from the base model.&quot; style=&quot;width:100%;display:block;max-width:300px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: A simple architecture diagram for a 3-headed multi-stage  speculator. Z is the state from the base model.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;speculator-training&quot;&gt;Speculator training&lt;/h4&gt;

&lt;p&gt;We have a two-phase approach to training a speculator for efficiency reasons. In the first phase, we train on small batches with long sequence lengths (4k tokens) and use the standard causal LM approach for training. In phase 2, we use large batches with short sequence lengths (256 tokens) generated from the base model. In this training phase, we tune the heads to match the output of the base model. Through numerous experiments, we find that a 5:2 ratio of steps for phase 1 vs phase 2 works well. We depict the progress of these phases in the below figure. We use PyTorch FSDP and &lt;a href=&quot;https://github.com/foundation-model-stack/fms-fsdp&quot;&gt;IBM FMS&lt;/a&gt; for the training of speculators.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/hitchhikers-guide-speculative-decoding/fig5.jpg&quot; alt=&quot;Per-head training loss curves for Llama2-13B speculator training, phase 1 and 2&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 5: Per-head training loss curves for Llama2-13B speculator training, phase 1 and 2&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-future-work&quot;&gt;Conclusion and Future Work&lt;/h2&gt;

&lt;p&gt;Through this blog, we are releasing  a new approach for speculative decoding and the following assets:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Models for improving the inter-token latencies for a range of models - Llama3 8B, Llama2 13B, Granite 7B, and CodeLlama 13B&lt;/li&gt;
  &lt;li&gt;Production quality code for inference&lt;/li&gt;
  &lt;li&gt;Recipes for training speculators&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We are working on training speculators for Llama3 70B and Mistral models and invite the community to contribute as well as help improve on our framework. We would also love to work with major open source serving frameworks such as &lt;a href=&quot;https://github.com/vllm-project/vllm&quot;&gt;vLLM&lt;/a&gt; and &lt;a href=&quot;https://github.com/huggingface/text-generation-inference&quot;&gt;TGI&lt;/a&gt; to contribute back our speculative decoding approach to benefit the community.&lt;/p&gt;

&lt;h4 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h4&gt;

&lt;p&gt;There are several teams that helped us get to these latency improvements for inference. We would like to thank the vLLM team for creating the paged attention kernel in a clean and reusable manner. We extend our gratitude to the Team PyTorch at Meta that helped provide feedback on this blog as well as continued efforts on optimal usage of PyTorch. Special thanks to our internal production teams at IBM Research who took this prototype to production and hardened it. A shout out to Stas Bekman for providing insightful comments on the blog resulting in an improved explanation of the tradeoffs between compute, memory, and speculator effectiveness.&lt;/p&gt;

&lt;p&gt;The paged attention kernel was integrated into IBM FMS by Josh Rosenkranz and Antoni Viros i Martin. The speculator architecture and training was done by Davis Wertheimer, Pavithra Ranganathan, and Sahil Suneja. The integration of the modeling code with the inference server was done by Thomas Parnell, Nick Hill, and Prashant Gupta.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Team PyTorch at IBM</name>
        
        
      </author>

      

      

      
        <summary type="html">Speculative decoding is an optimization technique for inference that makes educated guesses about future tokens while generating the current token, all within a single forward pass. It incorporates a verification mechanism to ensure the correctness of these speculated tokens, thereby guaranteeing that the overall output of speculative decoding is identical to that of vanilla decoding. Optimizing the cost of inference of large language models (LLMs) is arguably one of the most critical factors in reducing the cost of generative AI and increasing its adoption. Towards this goal, various inference optimization techniques are available, including custom kernels, dynamic batching of input requests, and quantization of large models.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Accelerating Llama3 FP8 Inference with Triton Kernels</title>
      <link href="https://pytorch.org/blog/accelerating-llama3/" rel="alternate" type="text/html" title="Accelerating Llama3 FP8 Inference with Triton Kernels" />
      <published>2024-05-01T00:00:00-07:00</published>
      <updated>2024-05-01T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/accelerating-llama3</id>
      <content type="html" xml:base="https://pytorch.org/blog/accelerating-llama3/">&lt;h2 id=&quot;10-summary&quot;&gt;1.0 Summary&lt;/h2&gt;

&lt;p&gt;We present an optimized Triton FP8 GEMM (General Matrix-Matrix Multiply) kernel TK-GEMM, which leverages SplitK parallelization. For small batch size inference, TK-GEMM delivers up to &lt;strong&gt;1.94x&lt;/strong&gt; over the base Triton matmul implementation, &lt;strong&gt;1.87x&lt;/strong&gt; speedup over cuBLAS FP8 and &lt;strong&gt;1.71x&lt;/strong&gt; over cuBLAS FP16 for Llama3-70B inference problem sizes on NVIDIA H100 GPUs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig1.png&quot; alt=&quot;TK-GEMM Speedup over PyTorch (calling cuBLAS) for Llama3-70B Attention Layer Matrix Shapes (N=K=8192)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 1.&lt;/strong&gt; TK-GEMM Speedup over PyTorch (calling cuBLAS) for Llama3-70B Attention Layer Matrix Shapes (N=K=8192)&lt;/p&gt;

&lt;p&gt;In this blog, we will cover how we designed an optimized kernel using &lt;a href=&quot;https://github.com/openai/triton&quot;&gt;Triton&lt;/a&gt; for FP8 inference and tuned it for Lama3-70B inference. We will cover FP8 (8-bit floating point), a new datatype supported by Hopper generation GPUs (SM90), the key SM90 features that Triton supports, and how we modified the parallelization to be able to maximize memory throughput for memory-bound (inference) problem sizes.&lt;/p&gt;

&lt;p&gt;We also dedicate a section on CUDA graphs, an important technology that will help materialize kernel level speedups and enable developers who want to use Triton kernels in production settings to get additional performance gain.&lt;/p&gt;

&lt;p&gt;Repo and code available at: &lt;a href=&quot;https://github.com/pytorch-labs/applied-ai&quot;&gt;https://github.com/pytorch-labs/applied-ai&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;20-fp8-datatype&quot;&gt;2.0 FP8 Datatype&lt;/h2&gt;

&lt;p&gt;The FP8 datatype was &lt;a href=&quot;https://arxiv.org/pdf/2209.05433.pdf&quot;&gt;introduced&lt;/a&gt; jointly by Nvidia, Arm and Intel and serves as a successor to 16-bit floating point types.  With half the bit count, it has the potential to provide significant throughput improvements over its predecessors for Transformer networks. The FP8 datatype consists of 2 formats:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E4M3&lt;/strong&gt; (4-bit exponent and 3-bit mantissa).  Able to store +/ 448 and nan.&lt;br /&gt;
&lt;strong&gt;E5M2&lt;/strong&gt; (5-bit exponent and 2-bit mantissa).  Able to store +/- 57,334, nan and inf.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig2.png&quot; alt=&quot;BF16, FP16, FP8 E4M3 and FP8 E5M2&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Above:&lt;/strong&gt; &lt;em&gt;BF16, FP16, FP8 E4M3 and FP8 E5M2.&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;To show precision differences, the closest representation to 0.3952 is shown in each format.&lt;/em&gt;&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html&quot;&gt;Nvidia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We use E4M3 in inference and forward pass training due its higher precision and E5M2 in training backward pass due to its higher dynamic range. Nvidia has designed their H100 FP8 Tensor Core to provide a peak of 3958 TFLOPS, &lt;strong&gt;2x&lt;/strong&gt; the FLOPS of the FP16 Tensor Core.&lt;/p&gt;

&lt;p&gt;We designed our Triton kernel with these hardware innovations in mind and in the rest of the blog we will discuss methods to leverage and verify that these features are indeed being utilized by the Triton compiler.&lt;/p&gt;

&lt;h2 id=&quot;30-triton-hopper-support-and-fp8-tensor-core-instruction&quot;&gt;3.0 Triton Hopper Support and FP8 Tensor Core Instruction&lt;/h2&gt;

&lt;p&gt;The Hopper GPU architecture has added the following &lt;a href=&quot;https://arxiv.org/abs/2402.13499&quot;&gt;new features&lt;/a&gt; that we can expect will accelerate FP8 GEMM.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;TMA (Tensor Memory Accelerator) Hardware Unit&lt;/li&gt;
  &lt;li&gt;WGMMA (Warp Group Matrix Multiply-Accumulate Instruction)&lt;/li&gt;
  &lt;li&gt;Threadblock Clusters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Triton currently takes advantage of one of these features, the &lt;em&gt;wgmma&lt;/em&gt; instruction, whereas PyTorch (calling cuBLAS) leverages all 3 which makes these speedups even more impressive. To fully take advantage of the Hopper FP8 Tensor Core, the wgmma is necessary even though the older mma.sync instruction is still supported.&lt;/p&gt;

&lt;p&gt;The key difference between the mma and wgmma instructions is that instead of 1 CUDA warp being responsible for an output shard, an entire warp group, 4 CUDA warps, &lt;em&gt;asynchronously&lt;/em&gt; contributes to an output shard.&lt;/p&gt;

&lt;p&gt;To see what this instruction looks like in practice, and to verify that our Triton Kernel is indeed utilizing this feature we analyzed the PTX and SASS assembly using &lt;a href=&quot;https://developer.nvidia.com/nsight-compute&quot;&gt;nsight compute&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig3.png&quot; alt=&quot;PTX Assembly&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 2.&lt;/strong&gt; PTX Assembly&lt;/p&gt;

&lt;p&gt;This instruction is further lowered into a QGMMA instruction in SASS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig4.png&quot; alt=&quot;SASS Assembly&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 3.&lt;/strong&gt; SASS Assembly&lt;/p&gt;

&lt;p&gt;Both instructions tell us that we are multiplying two FP8 E4M3 input tensors and accumulating in F32, which confirms that the TK-GEMM Kernel is utilizing the FP8 Tensor Core and the lowering is being done correctly.&lt;/p&gt;

&lt;h2 id=&quot;40-splitk-work-decomposition&quot;&gt;4.0 SplitK Work Decomposition&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig5.png&quot; alt=&quot;TK-GEMM vs Base Triton GEMM TFLOPS for M = 1-64&quot; style=&quot;width:100%;display:block;max-width:600px;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 4.&lt;/strong&gt; TK-GEMM vs Base Triton GEMM TFLOPS for M = 1-64&lt;/p&gt;

&lt;p&gt;The base Triton FP8 GEMM implementation does &lt;a href=&quot;https://github.com/openai/triton/issues/3104&quot;&gt;not perform&lt;/a&gt; well for the small M regime, where for a matrix multiplication of A (&lt;em&gt;MxN&lt;/em&gt;) x B (&lt;em&gt;NxK&lt;/em&gt;), &lt;em&gt;M&lt;/em&gt; &amp;lt; &lt;em&gt;N&lt;/em&gt;, &lt;em&gt;K&lt;/em&gt;. To optimize for this type matrix profile we applied a SplitK work decomposition instead of the Data Parallel decomposition found in the base Triton kernel. This greatly improved latencies for the small M regime.&lt;/p&gt;

&lt;p&gt;For background, SplitK launches additional thread blocks along the k dimension to calculate partial output sums. The partial results from each thread block are then summed using an atomic reduction.  This allows for finer grained work decomposition with resultant performance improvements.  More details on SplitK are available in our &lt;a href=&quot;https://arxiv.org/abs/2402.00025&quot;&gt;arxiv paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;After carefully tuning the other relevant hyperparameters for our kernel such as tile sizes, number of warps and the number of pipeline stages to Llama3-70B problem sizes we were able to produce up to &lt;strong&gt;1.94x&lt;/strong&gt; speedup over the Triton &lt;a href=&quot;https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html&quot;&gt;base implementation&lt;/a&gt;. For a more comprehensive introduction to hyperparameter tuning, see our &lt;a href=&quot;https://pytorch.org/blog/accelerating-moe-model/#30-work-decomposition---splitk&quot;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig6.png&quot; alt=&quot;NCU profiler times for TK-GEMM under varying batch sizes, and compared with PyTorch (calling cuBLAS) FP8 and FP16.&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Above&lt;/strong&gt;: &lt;em&gt;NCU profiler times for TK-GEMM under varying batch sizes, and compared with PyTorch (calling cuBLAS) FP8 and FP16.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that starting at M=32, the cuBLAS FP8 kernel starts to outperform TK-GEMM. For M &amp;gt;= 32, we suspect that hyperparameters we found are not optimal, and thus another set of experiments is required to determine the optimal parameters for the mid-sized M regime.&lt;/p&gt;

&lt;h2 id=&quot;50-cuda-graphs-to-enable-end-to-end-speedup&quot;&gt;5.0 CUDA Graphs to Enable End-to-End Speedup&lt;/h2&gt;

&lt;p&gt;To be able to realize these speedups in an end-to-end setting, we must take into account both the kernel execution time (GPU duration) as well as the wall time (CPU+GPU) duration. Triton kernels, which are handwritten (as opposed to torch compile generated) are known to suffer from high-kernel launch latencies. If we use &lt;a href=&quot;https://pytorch.org/docs/stable/profiler.html&quot;&gt;torch profiler&lt;/a&gt; to trace the TK-GEMM kernel we can see the call stack on the CPU side to pinpoint exactly what is causing the slowdown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig7.png&quot; alt=&quot;CPU Launch Overhead: 2.413ms&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 5.&lt;/strong&gt; CPU Launch Overhead: 2.413ms&lt;/p&gt;

&lt;p&gt;From above, we see that the majority of the wall time of our optimized kernel is dominated by JIT (Just-in-Time) compilation overhead. To combat this we can use CUDA graphs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig8.png&quot; alt=&quot;CUDA Graphs Visualization&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 6.&lt;/strong&gt; CUDA Graphs Visualization&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/&quot;&gt;PyTorch&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The key idea is instead of multiple kernel launches, we instead can create and instantiate a graph (1 time cost) and then submit that instance of the graph for execution. To illustrate this point we simulate a Llama3-70B Attention layer, As shown in the below figure generated using &lt;a href=&quot;https://developer.nvidia.com/nsight-systems&quot;&gt;nsight systems&lt;/a&gt;, the time between each GEMM is &lt;strong&gt;&lt;em&gt;165us&lt;/em&gt;&lt;/strong&gt; compared to the &lt;strong&gt;&lt;em&gt;12us&lt;/em&gt;&lt;/strong&gt; spent on the actual matmul due the CPU kernel launch overhead. This means that &lt;strong&gt;&lt;em&gt;92%&lt;/em&gt;&lt;/strong&gt; of the time of the time in an Attention layer the GPU is idle and not doing any work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig9.png&quot; alt=&quot;Simulated Llama3-70B Attention Layer with TK-GEMM&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 7.&lt;/strong&gt; Simulated Llama3-70B Attention Layer with TK-GEMM&lt;/p&gt;

&lt;p&gt;To show the impact of CUDA graphs, we then created a graph of the TK-GEMM kernel in the toy Attention layer and replayed the graph. Below, we can see that the gaps between kernel executions are reduced to 6.65us.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig10.png&quot; alt=&quot;Simulated Llama3-70B Attention Layer with TK-GEMM and CUDA Graphs&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 8.&lt;/strong&gt; Simulated Llama3-70B Attention Layer with TK-GEMM and CUDA Graphs&lt;/p&gt;

&lt;p&gt;In practice, this optimization would result in a &lt;strong&gt;6.4x&lt;/strong&gt; speedup of a single attention layer in Llama3-70B, over naively using TK-GEMM in a model without CUDA graphs.&lt;/p&gt;

&lt;h2 id=&quot;60-potential-future-optimization-paths&quot;&gt;6.0 Potential Future Optimization Paths&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig11.png&quot; alt=&quot;TMA Hardware Unit&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 9.&lt;/strong&gt; TMA Hardware Unit&lt;br /&gt;
&lt;em&gt;Image Credit: &lt;a href=&quot;https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/&quot;&gt;Nvidia&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The Nvidia H100 features a TMA hardware unit. The dedicated TMA unit frees up registers and threads to do other work, as address generation is completely handled by the TMA. For memory bound problem sizes, this can provide even further gain when Triton enables support for this feature.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig12.png&quot; alt=&quot;Tensor Core Utilization (Arrows Indicate Degrees of Freedom)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 10.&lt;/strong&gt; Tensor Core Utilization (Arrows Indicate Degrees of Freedom)&lt;/p&gt;

&lt;p&gt;To identify how well we are utilizing the Tensor Core, we can analyze the roofline chart. Notice that we are in the memory-bound region as expected for small M. To improve kernel latency we can either increase the arithmetic intensity, which with a fixed problem size can only be achieved through exploiting data locality and other loop &lt;a href=&quot;https://www.codee.com/is-your-algorithm-running-at-peak-performance-the-roofline-model/&quot;&gt;optimizations&lt;/a&gt; or increasing the memory throughput. This requires either a more optimal parallel algorithm specialized for the FP8 datatype as well as the type of problem size characteristics we expect to see in FP8 inference.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/accelerating-llama3/fig13.png&quot; alt=&quot;DRAM Throughput Circled, 1.65TB/s vs Peak 3.35TB/s on H100 (M=16, N=8192, K=8192)&quot; style=&quot;width:100%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Figure 11.&lt;/strong&gt; DRAM Throughput Circled, 1.65TB/s vs Peak 3.35TB/s on H100 (M=16, N=8192, K=8192)&lt;/p&gt;

&lt;p&gt;Lastly, we can see that we are only achieving around &lt;strong&gt;50%&lt;/strong&gt; of peak DRAM throughput on the NVIDIA H100. High performance GEMM kernels typically achieve around &lt;strong&gt;70-80%&lt;/strong&gt; of peak throughput. This means that there is still a lot of room to improve and the techniques mentioned above (loop unrolling, optimized parallelization) are needed for additional gain.&lt;/p&gt;

&lt;h2 id=&quot;70-future-work&quot;&gt;7.0 Future Work&lt;/h2&gt;

&lt;p&gt;For future research, we would like to explore &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main&quot;&gt;CUTLASS&lt;/a&gt; 3.x and &lt;a href=&quot;https://github.com/NVIDIA/cutlass/tree/main/media/docs/cute&quot;&gt;CuTe&lt;/a&gt; to leverage more direct control over Hopper features especially in terms of obtaining direct TMA control and exploring pingpong architectures, which have shown promising results for FP8 GEMM.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Adnan Hoque, Less Wright, Chih Chieh Yang</name>
        
        
      </author>

      

      

      
        <summary type="html">1.0 Summary</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ExecuTorch Alpha: Taking LLMs and AI to the Edge with Our Community and Partners</title>
      <link href="https://pytorch.org/blog/executorch-alpha/" rel="alternate" type="text/html" title="ExecuTorch Alpha: Taking LLMs and AI to the Edge with Our Community and Partners" />
      <published>2024-04-30T00:00:00-07:00</published>
      <updated>2024-04-30T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/executorch-alpha</id>
      <content type="html" xml:base="https://pytorch.org/blog/executorch-alpha/">&lt;p&gt;We are excited to announce the release of &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;ExecuTorch alpha&lt;/a&gt;, focused on deploying large language models (LLMs) and large ML models to the edge, stabilizing the API surface, and improving our installation processes. It has been an exciting few months &lt;a href=&quot;https://pytorch.org/blog/pytorch-edge/&quot;&gt;from our 0.1 (preview) release&lt;/a&gt; in collaboration with our partners at Arm, Apple, and Qualcomm Technologies, Inc.&lt;/p&gt;

&lt;p&gt;In this post we’ll discuss our full support for Meta’s Llama 2, early support for Meta’s Llama 3, broad model support in ExecuTorch, and highlight the important work our partners have done to move us forward.&lt;/p&gt;

&lt;h2 id=&quot;large-language-models-on-mobile&quot;&gt;Large Language Models on Mobile&lt;/h2&gt;

&lt;p&gt;Mobile devices are highly constrained for compute, memory, and power. To bring LLMs to these devices, we heavily leverage quantization and other techniques to pack these models appropriately.&lt;/p&gt;

&lt;p&gt;ExecuTorch alpha supports 4-bit post-training quantization using GPTQ. We’ve provided broad device support on CPU by landing dynamic shape support and new dtypes in XNNPack. We’ve also made significant improvements in export and lowering, reduced memory overhead and improved runtime performance. This enables running Llama 2 7B efficiently on iPhone 15 Pro, iPhone 15 Pro Max, Samsung Galaxy S22, S23, and S24 phones and other edge devices. &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;Early support&lt;/a&gt; for &lt;a href=&quot;https://ai.meta.com/blog/meta-llama-3/&quot;&gt;Llama 3 8B&lt;/a&gt; is also included. We are always improving the token/sec on various edge devices and you can visit GitHub for the &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/models/llama2/README.md&quot;&gt;latest performance numbers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We’re working closely with our partners at Apple, Arm, and Qualcomm Technologies to delegate to GPU and NPU for performance through Core ML, MPS, TOSA, and Qualcomm AI Stack backends respectively.&lt;/p&gt;

&lt;h2 id=&quot;supported-models&quot;&gt;Supported Models&lt;/h2&gt;

&lt;p&gt;We remain committed to supporting an ever-expanding list of models with ExecuTorch. Since preview, we have significantly expanded our tested models across NLP, vision and speech, with full details &lt;a href=&quot;https://github.com/pytorch/executorch/releases/tag/v0.2.0&quot;&gt;in our release notes&lt;/a&gt;. Although support for on-device LLMs is early, we anticipate most traditional models to function seamlessly out of the box, with delegation to XNNPACK, Core ML, MPS, TOSA, and HTP for performance. If you encounter any problems please open &lt;a href=&quot;https://github.com/pytorch/executorch/issues&quot;&gt;a GitHub issue&lt;/a&gt; with us.&lt;/p&gt;

&lt;h2 id=&quot;productivity&quot;&gt;Productivity&lt;/h2&gt;

&lt;p&gt;Deploying performant models tuned for specific platforms often require deep visualization into the on-device runtime data to determine the right changes to make in the original PyTorch model. With ExecuTorch alpha, we provide a powerful SDK with observability throughout the process from model authoring to deployment, including delegate and hardware-level information.&lt;/p&gt;

&lt;p&gt;The ExecuTorch SDK was enhanced to include better debugging and profiling tools. Because ExecuTorch is built on PyTorch, the debugging capabilities include the ability to map from operator nodes back to original Python source code for more efficient anomaly resolution and performance tuning for both delegated and non-delegated model instances. You can learn more about the ExecuTorch SDK &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/examples/sdk/README.md&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;partnerships&quot;&gt;Partnerships&lt;/h2&gt;

&lt;p&gt;ExecuTorch has only been possible because of strong collaborations across Arm, Apple, and  Qualcomm Technologies. The collaboration for the initial launch of ExecuTorch continues as we support LLMs and large AI models on the edge for PyTorch. As we’ve seen with this early work for ExecuTorch alpha, there are unique challenges with these larger models and we’re excited to develop in the open.&lt;/p&gt;

&lt;p&gt;We also want to highlight the great partnership with Google on &lt;a href=&quot;https://github.com/google/XNNPACK&quot;&gt;XNNPACK&lt;/a&gt; for CPU performance. The teams continue to work together upstreaming our changes and across the TensorFlow and PyTorch teams to make sure we can all support generative AI models on the edge with SOTA performance.&lt;/p&gt;

&lt;p&gt;Lastly, our hardware partner MediaTek has been doing work enabling the Llama collection of models with ExecuTorch on their SoCs. We’ll have more to share in the future.&lt;/p&gt;

&lt;h2 id=&quot;alpha-and-production-usage&quot;&gt;Alpha and Production Usage&lt;/h2&gt;

&lt;p&gt;With our alpha release, we have production-tested ExecuTorch. Meta is using ExecuTorch for hand tracking on Meta Quest 3 and a variety of models on Ray-Ban Meta Smart Glasses. In addition, we have begun the rollout of ExecuTorch with Instagram and are integrating with other Meta products. We are excited to see how ExecuTorch can be used for other edge experiences.&lt;/p&gt;

&lt;h2 id=&quot;community&quot;&gt;Community&lt;/h2&gt;

&lt;p&gt;We are excited to see various efforts in the community to adopt or contribute to ExecuTorch. For instance, Unity recently &lt;a href=&quot;https://schedule.gdconf.com/session/unity-developer-summit-drive-better-gameplay-experiences-on-user-devices-with-ai-presented-by-unity/903634&quot;&gt;shared their work&lt;/a&gt; at the Game Developers Conference (&lt;a href=&quot;https://gdconf.com/&quot;&gt;GDC&lt;/a&gt;) on leveraging ExecuTorch and Edge IR to run PyTorch models with their neural network inference library Sentis. Leveraging ExecuTorch’s hackability and extensibility, Unity introduced their own custom backend that serializes ExecuTorch’s Edge Dialect IR into Sentis’ native serialized format enabling developers to begin using PyTorch models easily in their games and apps.&lt;/p&gt;

&lt;p&gt;We’ve been building and innovating with ExecuTorch in the open. Our north star is to empower the community to deploy any ML model on edge devices painlessly and efficiently. Whether you are a hobbyist or this is your day job, we’d love for you to &lt;a href=&quot;https://pytorch.org/executorch/stable/getting-started-setup.html&quot;&gt;jump in to bring your ML models to the edge&lt;/a&gt;. We are looking for your help to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use ExecuTorch to &lt;a href=&quot;https://github.com/pytorch/executorch/blob/main/docs/source/llm/getting-started.md&quot;&gt;run your LLM models locally&lt;/a&gt; on various deployment targets and share your feedback&lt;/li&gt;
  &lt;li&gt;Expand our supported models, including bug reports&lt;/li&gt;
  &lt;li&gt;Expand our quantization schemes&lt;/li&gt;
  &lt;li&gt;Help us build out delegates to GPU and NPU&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To all individual contributors and early adopters of ExecuTorch, a big thank you as well. We can’t wait to have more of you &lt;a href=&quot;https://github.com/pytorch/executorch&quot;&gt;join us&lt;/a&gt;!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of ExecuTorch alpha, focused on deploying large language models (LLMs) and large ML models to the edge, stabilizing the API surface, and improving our installation processes. It has been an exciting few months from our 0.1 (preview) release in collaboration with our partners at Arm, Apple, and Qualcomm Technologies, Inc.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">PyTorch 2.3 Release Blog</title>
      <link href="https://pytorch.org/blog/pytorch2-3/" rel="alternate" type="text/html" title="PyTorch 2.3 Release Blog" />
      <published>2024-04-24T00:00:00-07:00</published>
      <updated>2024-04-24T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/pytorch2-3</id>
      <content type="html" xml:base="https://pytorch.org/blog/pytorch2-3/">&lt;p&gt;We are excited to announce the release of PyTorch® 2.3 (&lt;a href=&quot;https://github.com/pytorch/pytorch/releases/tag/v2.3.0&quot;&gt;release note&lt;/a&gt;)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;

&lt;p&gt;This release is composed of 3393 commits and 426 contributors since PyTorch 2.2. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try these out and report any issues as we improve 2.3. More information about how to get started with the PyTorch 2-series can be found at our &lt;a href=&quot;https://pytorch.org/get-started/pytorch-2.0/&quot;&gt;Getting Started&lt;/a&gt; page.&lt;/p&gt;

&lt;table class=&quot;table table-bordered&quot;&gt;
  &lt;tr&gt;
   &lt;td&gt;&lt;strong&gt;Beta&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Prototype&lt;/strong&gt;
   &lt;/td&gt;
   &lt;td&gt;&lt;strong&gt;Performance Improvements&lt;/strong&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;User-defined Triton kernels in torch.compile
   &lt;/td&gt;
   &lt;td&gt;torch.export adds new API to specify dynamic_shapes
   &lt;/td&gt;
   &lt;td&gt;Weight-Only-Quantization introduced into Inductor CPU backend
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Tensor parallelism within PyTorch Distributed
   &lt;/td&gt;
   &lt;td&gt;Asynchronous checkpoint generation
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td&gt;Support for semi-structured sparsity
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
   &lt;td&gt;
   &lt;/td&gt;
  &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;*To see a full list of public feature submissions click &lt;a href=&quot;https://docs.google.com/spreadsheets/d/1TzGkWuUMF1yTe88adz1dt2mzbIsZLd3PBasy588VWgk/edit?usp=sharing&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;beta-features&quot;&gt;Beta Features&lt;/h2&gt;

&lt;h3 id=&quot;beta-support-for-user-defined-triton-kernels-in-torchcompile&quot;&gt;[Beta] Support for User-defined Triton kernels in &lt;em&gt;torch.compile&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;Allows for PyTorch code that contains triton kernels to be executed natively using torch.compile. This enables users to migrate code containing triton kernels from eager PyTorch to &lt;em&gt;torch.compile&lt;/em&gt; without running into performance regressions or graph breaks. Native support also creates an opportunity for Torch Inductor to precompile the user-defined Triton kernel as well as better organize code around the Triton kernel allowing for further optimizations.&lt;/p&gt;

&lt;p&gt;You can find more information about how to utilize user defined Triton kernels in torch.compile within &lt;a href=&quot;https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-tensor-parallelism-introduces-more-efficient-ways-to-train-llms&quot;&gt;[Beta] Tensor Parallelism introduces more efficient ways to train LLMs&lt;/h3&gt;

&lt;p&gt;The Tensor Parallel API facilitates various tensor manipulations across GPUs/hosts and integrates with FSDP for 2D Parallelism (Tensor parallelism across devices + Data Parallelism across hosts). It also offers a low-level API for constructing higher-level Tensor parallel APIs. This API has been validated to support the training of transformer models with over 100 billion parameters.&lt;/p&gt;

&lt;p&gt;You can find more information on how to utilize this within your workflows within &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/TP_tutorial.html&quot;&gt;this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;beta-semi-structured-sparsity-provides-users-with-a-way-to-take-advantage-of-accelerated-sparse-inference-and-memory-savings&quot;&gt;[Beta] Semi-structured sparsity provides users with a way to take advantage of accelerated sparse inference and memory savings&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;torch.sparse.SparseSemiStructuredTensor&lt;/em&gt; implements semi-structured sparsity as a Tensor subclass, which have observed speedups of up to 1.6 over dense matrix multiplication.&lt;/p&gt;

&lt;p&gt;In particular it adds:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Additional support for quantization composability (mixed dtype, dequant fusion)&lt;/li&gt;
  &lt;li&gt;Updated cuSPARSELt and CUTLASS kernels&lt;/li&gt;
  &lt;li&gt;torch.compile support&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find more information on how to take advantage of semi-structured sparsity &lt;a href=&quot;https://pytorch.org/tutorials/advanced/semi_structured_sparse.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype-features&quot;&gt;Prototype Features&lt;/h2&gt;

&lt;h3 id=&quot;prototype-torchexport-adds-new-api-to-specify-dynamic_shapes&quot;&gt;[PROTOTYPE] &lt;em&gt;torch.export&lt;/em&gt; adds new API to specify &lt;em&gt;dynamic_shapes&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;You can now use &lt;em&gt;torch.export.Dim&lt;/em&gt; to better represent dynamic shapes by enabling developers to specify ranges (min and max values) that can be reused across different input dimensions that are constrained to be equal.&lt;/p&gt;

&lt;p&gt;To learn more about &lt;em&gt;torch.export.Dim&lt;/em&gt; as well as how it can be used to express more interesting relationships (such as linear arithmetic expressions) check out the tutorial &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/torch_export_tutorial.html#constraints-dynamic-shapes&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;prototype-asynchronous-checkpoint-generation&quot;&gt;[PROTOTYPE] Asynchronous checkpoint generation&lt;/h3&gt;

&lt;p&gt;Asynchronous checkpoint generation allows users to continue their training loops while checkpoints are being generated, essentially offloading much of the checkpointing cost.&lt;/p&gt;

&lt;p&gt;You can find out how to utilize this within your own workflows with this &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/release/2.3/torch/distributed/checkpoint/examples/async_checkpointing_example.py&quot;&gt;example&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;performance-improvements&quot;&gt;Performance Improvements&lt;/h2&gt;

&lt;h3 id=&quot;prototype-weight-only-quantization-introduced-into-inductor-cpu-backend&quot;&gt;[PROTOTYPE] Weight-Only-Quantization introduced into Inductor CPU backend&lt;/h3&gt;

&lt;p&gt;PyTorch 2.3 enhances LLM inference performance on torch inductor CPU backend. The project &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast&quot;&gt;gpt-fast&lt;/a&gt; offers a simple and efficient PyTorch native acceleration for transformer text generation with &lt;em&gt;torch.compile&lt;/em&gt;. Prior to 2.3 only CUDA devices were supported and this feature enables the CPU counterpart by providing highly optimized kernels for the int4 and int8 weight only quantization Linear.&lt;/p&gt;

&lt;p&gt;For more information / how to utilize this feature please refer to the &lt;a href=&quot;https://github.com/pytorch-labs/gpt-fast#quantization&quot;&gt;gpt-fast README&lt;/a&gt;.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We are excited to announce the release of PyTorch® 2.3 (release note)! PyTorch 2.3 offers support for user-defined Triton kernels in torch.compile, allowing for users to migrate their own Triton kernels from eager without experiencing performance regressions or graph breaks. Tensor Parallelism improves the experience for training Large Language Models using native PyTorch functions, which has been validated on training runs for 100B parameter models. As well, semi-structured sparsity implements semi-structured sparsity as a Tensor subclass, with observed speedups of up to 1.6 over dense matrix multiplication.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">torchtune: Easily fine-tune LLMs using PyTorch</title>
      <link href="https://pytorch.org/blog/torchtune-fine-tune-llms/" rel="alternate" type="text/html" title="torchtune: Easily fine-tune LLMs using PyTorch" />
      <published>2024-04-16T00:00:00-07:00</published>
      <updated>2024-04-16T00:00:00-07:00</updated>
      <id>https://pytorch.org/blog/torchtune-fine-tune-llms</id>
      <content type="html" xml:base="https://pytorch.org/blog/torchtune-fine-tune-llms/">&lt;p&gt;We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.&lt;/p&gt;

&lt;p&gt;Staying true to PyTorch’s design principles, torchtune provides composable and modular building blocks along with easy-to-extend training recipes to fine-tune popular LLMs on a variety of consumer-grade and professional GPUs.&lt;/p&gt;

&lt;p&gt;torchtune supports the full fine-tuning workflow from start to finish, including&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Downloading and preparing datasets and model checkpoints.&lt;/li&gt;
  &lt;li&gt;Customizing the training with composable building blocks that support different model architectures, parameter-efficient fine-tuning (PEFT) techniques, and more.&lt;/li&gt;
  &lt;li&gt;Logging progress and metrics to gain insight into the training process.&lt;/li&gt;
  &lt;li&gt;Quantizing the model post-tuning.&lt;/li&gt;
  &lt;li&gt;Evaluating the fine-tuned model on popular benchmarks.&lt;/li&gt;
  &lt;li&gt;Running local inference for testing fine-tuned models.&lt;/li&gt;
  &lt;li&gt;Checkpoint compatibility with popular production inference systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To get started, jump right into the &lt;a href=&quot;https://www.github.com/pytorch/torchtune&quot;&gt;code&lt;/a&gt; or walk through our many &lt;a href=&quot;https://pytorch.org/torchtune/main/&quot;&gt;tutorials&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;why-torchtune&quot;&gt;Why torchtune?&lt;/h2&gt;

&lt;p&gt;Over the past year there has been an explosion of interest in open LLMs. Fine-tuning these state of the art models has emerged as a critical technique for adapting them to specific use cases. This adaptation can require extensive customization from dataset and model selection all the way through to quantization, evaluation and inference. Moreover, the size of these models poses a significant challenge when trying to fine-tune them on consumer-level GPUs with limited memory.&lt;/p&gt;

&lt;p&gt;Existing solutions make it hard to add these customizations or optimizations by hiding the necessary pieces behind layers of abstractions. It’s unclear how different components interact with each other and which of these need to be updated to add new functionality. torchtune empowers developers to adapt LLMs to their specific needs and constraints with full control and visibility.&lt;/p&gt;

&lt;h2 id=&quot;torchtunes-design&quot;&gt;torchtune’s Design&lt;/h2&gt;

&lt;p&gt;torchtune was built with the following principles in mind&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Easy extensibility&lt;/strong&gt; - New techniques emerge all the time and everyone’s fine-tuning use case is different. torchtune’s recipes are designed around easily composable components and hackable training loops, with minimal abstraction getting in the way of fine-tuning your fine-tuning. Each &lt;a href=&quot;https://github.com/pytorch/torchtune/tree/main/recipes&quot;&gt;recipe&lt;/a&gt; is self-contained - no trainers or frameworks, and is designed to be easy to read - less than 600 lines of code!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Democratize fine-tuning&lt;/strong&gt; - Users, regardless of their level of expertise, should be able to use torchtune. Clone and modify configs, or get your hands dirty with some code! You also don’t need beefy data center GPUs. Our memory efficient recipes have been tested on machines with a single 24GB gaming GPU.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoperability with the OSS LLM ecosystem&lt;/strong&gt; - The open source LLM ecosystem is absolutely thriving, and torchtune takes advantage of this to provide interoperability with a wide range of offerings. This flexibility puts you firmly in control of how you train and use your fine-tuned models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Over the next year, open LLMs will become even more powerful, with support for more languages (multilingual), more modalities (multimodal) and more tasks. As the complexity of these models increases, we need to pay the same attention to “how” we design our libraries as we do to the features provided or performance of a training run. Flexibility will be key to ensuring the community can maintain the current pace of innovation, and many libraries/tools will need to play well with each other to power the full spectrum of use cases. torchtune is built from the ground up with this future in mind.&lt;/p&gt;

&lt;p&gt;In the true PyTorch spirit, torchtune makes it easy to get started by providing integrations with some of the most popular tools for working with LLMs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://huggingface.co/docs/hub/en/index&quot;&gt;Hugging Face Hub&lt;/a&gt;&lt;/strong&gt; - Hugging Face provides an expansive repository of open source models and datasets for fine-tuning. torchtune seamlessly integrates through the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tune download&lt;/code&gt; CLI command so you can get started right away with fine-tuning your first model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html&quot;&gt;PyTorch FSDP&lt;/a&gt;&lt;/strong&gt; - Scale your training using PyTorch FSDP. It is very common for people to invest in machines with multiple consumer level cards like the 3090/4090 by NVidia. torchtune allows you to take advantage of these setups by providing distributed recipes powered by FSDP.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://wandb.ai/site&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;&lt;/strong&gt; - torchtune uses the Weights &amp;amp; Biases AI platform to log metrics and model checkpoints during training. Track your configs, metrics and models from your fine-tuning runs all in one place!&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/EleutherAI/lm-evaluation-harness&quot;&gt;EleutherAI’s LM Evaluation Harness&lt;/a&gt;&lt;/strong&gt; - Evaluating fine-tuned models is critical to understanding whether fine-tuning is giving you the results you need. torchtune includes a simple evaluation recipe powered by EleutherAI’s LM Evaluation Harness to provide easy access to a comprehensive suite of standard LLM benchmarks. Given the importance of evaluation, we will be working with EleutherAI very closely in the next few months to build an even deeper and more “native” integration.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://pytorch.org/executorch-overview&quot;&gt;ExecuTorch&lt;/a&gt;&lt;/strong&gt; - Models fine-tuned with torchtune can be &lt;a href=&quot;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&quot;&gt;easily exported&lt;/a&gt; to ExecuTorch, enabling efficient inference to be run on a wide variety of mobile and edge devices.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/pytorch-labs/ao&quot;&gt;torchao&lt;/a&gt;&lt;/strong&gt; - Easily and efficiently quantize your fine-tuned models into 4-bit or 8-bit using a simple &lt;a href=&quot;https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py&quot;&gt;post-training recipe&lt;/a&gt; powered by the quantization APIs from torchao.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;This is just the beginning and we’re really excited to put this alpha version in front of a vibrant and energetic community. In the coming weeks, we’ll continue to augment the library with more models, features and fine-tuning techniques. We’d love to hear any feedback, comments or feature requests in the form of GitHub issues on our repository, or on our &lt;a href=&quot;https://discord.com/invite/4Xsdn8Rr9Q&quot;&gt;Discord channel&lt;/a&gt;. As always, we’d love any contributions from this awesome community. Happy Tuning!&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Facebook</name>
        
        
      </author>

      

      

      
        <summary type="html">We’re pleased to announce the alpha release of torchtune, a PyTorch-native library for easily fine-tuning large language models.</summary>
      

      
      
    </entry>
  
</feed>


